{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_doc_classifier.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpqqVrEMxhFcAIV7IeGV9k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fbcc89297f384483ab9caac29e7540dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d81194acfd014d31aa1f799383adf0e5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6cc1a9986c644a0db45fd00fc7004fb8","IPY_MODEL_bbe7d922309f448b87b932d619f02f92"]}},"d81194acfd014d31aa1f799383adf0e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6cc1a9986c644a0db45fd00fc7004fb8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b7608b97e976446da8f62012a5a7f12e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eae93ac41ddc4f20a32325fff01465f1"}},"bbe7d922309f448b87b932d619f02f92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d003098a03b54d6eab824ae995950d25","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 110kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d77d662234fd425aa5922747277878f0"}},"b7608b97e976446da8f62012a5a7f12e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eae93ac41ddc4f20a32325fff01465f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d003098a03b54d6eab824ae995950d25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d77d662234fd425aa5922747277878f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b79f8d303fd44d5bb94983551437df7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1001e1f57a394992b68e465cf1151198","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a46dc06f8896463183b3252ca47ee5dd","IPY_MODEL_6dfa9795a643471a9da7cf8f89485c7e"]}},"1001e1f57a394992b68e465cf1151198":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a46dc06f8896463183b3252ca47ee5dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f2b87d948aef42c382f473e32f7816d4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b97368b8ab25455380808738a8ef9642"}},"6dfa9795a643471a9da7cf8f89485c7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dcf219fba6254f58b61b7a4a348b6034","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 43.8B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca7c9dd5848c4b9882307df93b4a03a9"}},"f2b87d948aef42c382f473e32f7816d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b97368b8ab25455380808738a8ef9642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcf219fba6254f58b61b7a4a348b6034":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca7c9dd5848c4b9882307df93b4a03a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b679963eec7348079ad4d651065fed11":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a36f64cff47d44e1be3584f5cf67d047","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cacb4d0a59ce42579303fce171444787","IPY_MODEL_0172ec2d371f4a50b7f793b679e88597"]}},"a36f64cff47d44e1be3584f5cf67d047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cacb4d0a59ce42579303fce171444787":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ff7a370ed09c4f718b56bc75bcbabce0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_880273c12d624251be09d02cd71434a9"}},"0172ec2d371f4a50b7f793b679e88597":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_66a11af4566f4fe889cb717893ece246","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 2.28MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95125689280b4a99895001f5c4908312"}},"ff7a370ed09c4f718b56bc75bcbabce0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"880273c12d624251be09d02cd71434a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66a11af4566f4fe889cb717893ece246":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"95125689280b4a99895001f5c4908312":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09a994b51b174e1f8c690a842170712d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cba392fab998425e9aca50b79073bac0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_475c115071ed40da84e88bb859607491","IPY_MODEL_aefce81f4a56451fb826e34b1a36aee4"]}},"cba392fab998425e9aca50b79073bac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"475c115071ed40da84e88bb859607491":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_14aa4ebfb9154d018181b3f224aa97ba","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5efa2a6b016403590b4a59d1f0212e1"}},"aefce81f4a56451fb826e34b1a36aee4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_84cbf730f2a54d79ba196191f2b53103","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:09&lt;00:00, 57.1B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d8c771742364e1d86911bbd9a0283d9"}},"14aa4ebfb9154d018181b3f224aa97ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5efa2a6b016403590b4a59d1f0212e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84cbf730f2a54d79ba196191f2b53103":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4d8c771742364e1d86911bbd9a0283d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"860a69609d50404380a59f9749302861":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_22b8fafab0d944f78c88e52749af1009","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3f587c58722746148cac6703adb19de3","IPY_MODEL_232599bf6e8941f6ad1d0e718a7c1808"]}},"22b8fafab0d944f78c88e52749af1009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f587c58722746148cac6703adb19de3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3b6c43b09bbd49cdae78421afc921032","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_adeeddcd598b44d3be94633247400ec4"}},"232599bf6e8941f6ad1d0e718a7c1808":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b8df206b472241dea189f97bf7d03fb9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:09&lt;00:00, 46.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_763057edfcde4206bbc7843c7c35492f"}},"3b6c43b09bbd49cdae78421afc921032":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"adeeddcd598b44d3be94633247400ec4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8df206b472241dea189f97bf7d03fb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"763057edfcde4206bbc7843c7c35492f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5351fb60c5904465865a1e91390c6dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0eff3848993b4626a7168c1d513036d3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1238cf28450e4b4eb6340382d71777a6","IPY_MODEL_7928fb37b6124969a91d0c1352c35a4e"]}},"0eff3848993b4626a7168c1d513036d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1238cf28450e4b4eb6340382d71777a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8da8a6762e4046da9043fbcd295e1e17","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f00f58b8f1e44cb9a95e543823f37979"}},"7928fb37b6124969a91d0c1352c35a4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c42f197591854f4bb2af217114ce1dc5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5/5 [24:40&lt;00:00, 296.11s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f1e1ae14fac4bf996b1770c55215ed6"}},"8da8a6762e4046da9043fbcd295e1e17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f00f58b8f1e44cb9a95e543823f37979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c42f197591854f4bb2af217114ce1dc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5f1e1ae14fac4bf996b1770c55215ed6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58324d0fd078444cbea81c00653a920b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b99d7211fa134762b0db58a7132a972f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_282e63ce97d7485794dd07375a89dcbe","IPY_MODEL_401cfddd0bb74e2fb281ff7ab4dcb484"]}},"b99d7211fa134762b0db58a7132a972f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"282e63ce97d7485794dd07375a89dcbe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2d5a28d9a4034d7fb96795450a93042c","_dom_classes":[],"description":"Epoch 1: 100%","_model_name":"FloatProgressModel","bar_style":"","max":798,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_165c129f44154e9b93a218b62a2a4219"}},"401cfddd0bb74e2fb281ff7ab4dcb484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79e85778ac5b490684b3f5c9cce0d948","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798/798 [04:39&lt;00:00,  2.81it/s, training_loss=0.020]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_76a12774e58e4c5789fc001dc46a43aa"}},"2d5a28d9a4034d7fb96795450a93042c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"165c129f44154e9b93a218b62a2a4219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79e85778ac5b490684b3f5c9cce0d948":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"76a12774e58e4c5789fc001dc46a43aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1ce0364c07648f6baae20c58e187faa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb7b9376dcf142e9b8a81ad8f5c5f6d7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_44aab4bd9b804ae98fd59b8495d195a8","IPY_MODEL_01cb7fff4b9240c39b07091692ca8d7e"]}},"bb7b9376dcf142e9b8a81ad8f5c5f6d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"44aab4bd9b804ae98fd59b8495d195a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5c85b1e83f8b47ef998616a9bacb1faa","_dom_classes":[],"description":"Epoch 2: 100%","_model_name":"FloatProgressModel","bar_style":"","max":798,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4cd0807d6cca43d19c2d059968936df1"}},"01cb7fff4b9240c39b07091692ca8d7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9fbffee5637b4cd3b4ae8812a077e5db","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798/798 [04:42&lt;00:00,  2.83it/s, training_loss=0.007]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8025b2912f8143d8ada4af990970ed60"}},"5c85b1e83f8b47ef998616a9bacb1faa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4cd0807d6cca43d19c2d059968936df1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9fbffee5637b4cd3b4ae8812a077e5db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8025b2912f8143d8ada4af990970ed60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66b324d36ddc444ba97cca59938d9d49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e2db1d3310474163a83315ddb868e4b8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f324f963c2ab4c65988a2849e48ed5a7","IPY_MODEL_d510cb99a8274db48be3ab8d1116b23c"]}},"e2db1d3310474163a83315ddb868e4b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f324f963c2ab4c65988a2849e48ed5a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b3e84146ef884b068e436b967c4b05fb","_dom_classes":[],"description":"Epoch 3: 100%","_model_name":"FloatProgressModel","bar_style":"","max":798,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a06a35cb5bc84c84bd87bc56ef1599df"}},"d510cb99a8274db48be3ab8d1116b23c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7bdd088883324efeabfc631dd0221d15","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798/798 [04:41&lt;00:00,  2.84it/s, training_loss=0.594]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f1402f41c4040d0bc992c34e02e04b1"}},"b3e84146ef884b068e436b967c4b05fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a06a35cb5bc84c84bd87bc56ef1599df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7bdd088883324efeabfc631dd0221d15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2f1402f41c4040d0bc992c34e02e04b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d4072af09144b09a7c8497eb6c2cf1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b6e3db500d0945a582db8c2eaa91afdb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49c6c584d38d4883b2919b8b24ca65af","IPY_MODEL_812eb7dd84d545b688bdc4af49bfe323"]}},"b6e3db500d0945a582db8c2eaa91afdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49c6c584d38d4883b2919b8b24ca65af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_65cd2976246c4f9ab1b4ba97f59aba5f","_dom_classes":[],"description":"Epoch 4: 100%","_model_name":"FloatProgressModel","bar_style":"","max":798,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_227deb1d5c784e1995c5e543834c6ba6"}},"812eb7dd84d545b688bdc4af49bfe323":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42c91327a77841b38f18fe7a17d07b06","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798/798 [04:41&lt;00:00,  2.83it/s, training_loss=0.367]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0bd64f2e3beb45fcac780ad882875fcb"}},"65cd2976246c4f9ab1b4ba97f59aba5f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"227deb1d5c784e1995c5e543834c6ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42c91327a77841b38f18fe7a17d07b06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0bd64f2e3beb45fcac780ad882875fcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b1b58a6327b47efb589608cf7645a72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6e2f8ca4a11a40ff9632f48d35cfa626","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_91cd59fbee5f431d82a8d02f7f38f168","IPY_MODEL_212df4f7cc094772bb95d14f297bd603"]}},"6e2f8ca4a11a40ff9632f48d35cfa626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91cd59fbee5f431d82a8d02f7f38f168":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f377e737f6f54df285422a3644d48c28","_dom_classes":[],"description":"Epoch 5: 100%","_model_name":"FloatProgressModel","bar_style":"","max":798,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":798,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f2be373327b74b088ed1d254b1960e11"}},"212df4f7cc094772bb95d14f297bd603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c131cc706a9457ebd720cea8b56abf1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 798/798 [04:40&lt;00:00,  2.85it/s, training_loss=0.000]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95a19ab6c6704a26a6cc4adb34a3f3fe"}},"f377e737f6f54df285422a3644d48c28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f2be373327b74b088ed1d254b1960e11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c131cc706a9457ebd720cea8b56abf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"95a19ab6c6704a26a6cc4adb34a3f3fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bpclORXA19oi","executionInfo":{"status":"ok","timestamp":1627593864889,"user_tz":240,"elapsed":18234,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"0c06f871-5d44-48d8-95a3-64264d1ca808"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTrMBApT2Ama","executionInfo":{"status":"ok","timestamp":1627593867904,"user_tz":240,"elapsed":960,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"13f43d4e-f123-4360-ddba-a02a600e081f"},"source":["cd /content/drive/Shareddrives/si370/final_delivery/Pilot_Test/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/Shareddrives/si370/final_delivery/Pilot_Test\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oQBl5kOq3ztE"},"source":["Prepare training data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHSJc5CF16go","executionInfo":{"status":"ok","timestamp":1626810123415,"user_tz":240,"elapsed":144,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"ff9444fd-d9d7-410e-a182-1fb32940560f"},"source":["import pandas as pd\n","sasb_data = pd.read_csv(\"Data/training_data/sasb_full_training.csv\")\n","sasb_data.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n","labels = sasb_data.Sector.unique()\n","\n","label_dict = {}\n","for index, label in enumerate(sorted(labels)):\n","    label_dict[label] = index\n","label_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'consumer goods': 0,\n"," 'extractives & minerals processing': 1,\n"," 'financials': 2,\n"," 'food & beverage': 3,\n"," 'health care': 4,\n"," 'infrastructure': 5,\n"," 'renewable resources & alternative energy': 6,\n"," 'resource transformation': 7,\n"," 'services': 8,\n"," 'technology & communications': 9,\n"," 'transportation': 10}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"pkm5oeAs2bgP"},"source":["sasb_data['label'] = sasb_data.Sector.replace(label_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kIui26W34kx"},"source":["Split data into train/evaluation and test sets\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":762},"id":"XJJ1syt93FEq","executionInfo":{"status":"ok","timestamp":1626810426458,"user_tz":240,"elapsed":804,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"9286b327-10bd-48bd-83e7-bcba76254130"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(sasb_data.index.values, \n","                                                  sasb_data.label.values, \n","                                                  test_size=0.05, \n","                                                  random_state=42, \n","                                                  stratify=sasb_data.label.values)\n","\n","sasb_data['data_type'] = ['not_set']*sasb_data.shape[0]\n","\n","sasb_data.loc[X_train, 'data_type'] = 'train'\n","sasb_data.loc[X_test, 'data_type'] = 'test'\n","\n","sasb_data.groupby(['Sector', 'label', 'data_type']).count()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>train_text</th>\n","    </tr>\n","    <tr>\n","      <th>Sector</th>\n","      <th>label</th>\n","      <th>data_type</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">consumer goods</th>\n","      <th rowspan=\"2\" valign=\"top\">0</th>\n","      <th>test</th>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>265</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">extractives &amp; minerals processing</th>\n","      <th rowspan=\"2\" valign=\"top\">1</th>\n","      <th>test</th>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>248</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">financials</th>\n","      <th rowspan=\"2\" valign=\"top\">2</th>\n","      <th>test</th>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>215</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">food &amp; beverage</th>\n","      <th rowspan=\"2\" valign=\"top\">3</th>\n","      <th>test</th>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>151</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">health care</th>\n","      <th rowspan=\"2\" valign=\"top\">4</th>\n","      <th>test</th>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>202</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">infrastructure</th>\n","      <th rowspan=\"2\" valign=\"top\">5</th>\n","      <th>test</th>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>509</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">renewable resources &amp; alternative energy</th>\n","      <th rowspan=\"2\" valign=\"top\">6</th>\n","      <th>test</th>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">resource transformation</th>\n","      <th rowspan=\"2\" valign=\"top\">7</th>\n","      <th>test</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>401</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">services</th>\n","      <th rowspan=\"2\" valign=\"top\">8</th>\n","      <th>test</th>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>127</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">technology &amp; communications</th>\n","      <th rowspan=\"2\" valign=\"top\">9</th>\n","      <th>test</th>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>288</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">transportation</th>\n","      <th rowspan=\"2\" valign=\"top\">10</th>\n","      <th>test</th>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>train</th>\n","      <td>222</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                          train_text\n","Sector                                   label data_type            \n","consumer goods                           0     test               14\n","                                               train             265\n","extractives & minerals processing        1     test               13\n","                                               train             248\n","financials                               2     test               11\n","                                               train             215\n","food & beverage                          3     test                8\n","                                               train             151\n","health care                              4     test               11\n","                                               train             202\n","infrastructure                           5     test               27\n","                                               train             509\n","renewable resources & alternative energy 6     test                2\n","                                               train              32\n","resource transformation                  7     test               21\n","                                               train             401\n","services                                 8     test                7\n","                                               train             127\n","technology & communications              9     test               15\n","                                               train             288\n","transportation                           10    test               12\n","                                               train             222"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"IxY6CePUdZuV"},"source":["sasb_train_eval = sasb_data[sasb_data[\"data_type\"] == \"train\"]\n","sasb_test = sasb_data[sasb_data[\"data_type\"] == \"test\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1gUKHNVd21j","executionInfo":{"status":"ok","timestamp":1626810601040,"user_tz":240,"elapsed":163,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"9654c3f2-0b70-4352-99a8-99f3c2f77970"},"source":["print(len(sasb_train_eval))\n","print(len(sasb_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2660\n","141\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"Jxrgi6LteAk6","executionInfo":{"status":"ok","timestamp":1626810618932,"user_tz":240,"elapsed":198,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"a98bff4c-55a6-44a1-ba0c-6f426c753d62"},"source":["sasb_train_eval"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_text</th>\n","      <th>Sector</th>\n","      <th>label</th>\n","      <th>data_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>development distribution and manufacture of a ...</td>\n","      <td>health care</td>\n","      <td>4</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>the worldwide development manufacture and dist...</td>\n","      <td>health care</td>\n","      <td>4</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>201 847 6800 All references herein to the Comp...</td>\n","      <td>health care</td>\n","      <td>4</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>201 847 6800 All references herein to the Comp...</td>\n","      <td>health care</td>\n","      <td>4</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>201 847 6800 All references in this Form 10 K ...</td>\n","      <td>health care</td>\n","      <td>4</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2796</th>\n","      <td>along with our joint venture partners are the ...</td>\n","      <td>food &amp; beverage</td>\n","      <td>3</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2797</th>\n","      <td>along with our joint venture partners are the ...</td>\n","      <td>food &amp; beverage</td>\n","      <td>3</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2798</th>\n","      <td>in 2014 in Delaware without its subsidiaries 8...</td>\n","      <td>extractives &amp; minerals processing</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2799</th>\n","      <td>in 2014 in Delaware without its subsidiaries q...</td>\n","      <td>extractives &amp; minerals processing</td>\n","      <td>1</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2800</th>\n","      <td>to help make the world safer and more comforta...</td>\n","      <td>resource transformation</td>\n","      <td>7</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2660 rows × 4 columns</p>\n","</div>"],"text/plain":["                                             train_text  ... data_type\n","0     development distribution and manufacture of a ...  ...     train\n","1     the worldwide development manufacture and dist...  ...     train\n","2     201 847 6800 All references herein to the Comp...  ...     train\n","3     201 847 6800 All references herein to the Comp...  ...     train\n","4     201 847 6800 All references in this Form 10 K ...  ...     train\n","...                                                 ...  ...       ...\n","2796  along with our joint venture partners are the ...  ...     train\n","2797  along with our joint venture partners are the ...  ...     train\n","2798  in 2014 in Delaware without its subsidiaries 8...  ...     train\n","2799  in 2014 in Delaware without its subsidiaries q...  ...     train\n","2800  to help make the world safer and more comforta...  ...     train\n","\n","[2660 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"9qrQtoee4jzm"},"source":["Split train/evaluation set into two sets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":966},"id":"mXAfM3cn26Lq","executionInfo":{"status":"ok","timestamp":1626810752824,"user_tz":240,"elapsed":165,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"a6378fc0-9957-4e19-b333-250b6ef647f1"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(sasb_train_eval.index.values, \n","                                                  sasb_train_eval.label.values, \n","                                                  test_size=0.1, \n","                                                  random_state=42, \n","                                                  stratify=sasb_train_eval.label.values)\n","\n","sasb_train_eval['data_type'] = ['not_set']*sasb_train_eval.shape[0]\n","\n","sasb_train_eval.loc[X_train, 'data_type'] = 'train'\n","sasb_train_eval.loc[X_val, 'data_type'] = 'val'\n","\n","sasb_train_eval.groupby(['Sector', 'label', 'data_type']).count()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  isetter(loc, value)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>train_text</th>\n","    </tr>\n","    <tr>\n","      <th>Sector</th>\n","      <th>label</th>\n","      <th>data_type</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">consumer goods</th>\n","      <th rowspan=\"2\" valign=\"top\">0</th>\n","      <th>train</th>\n","      <td>239</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">extractives &amp; minerals processing</th>\n","      <th rowspan=\"2\" valign=\"top\">1</th>\n","      <th>train</th>\n","      <td>223</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">financials</th>\n","      <th rowspan=\"2\" valign=\"top\">2</th>\n","      <th>train</th>\n","      <td>193</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">food &amp; beverage</th>\n","      <th rowspan=\"2\" valign=\"top\">3</th>\n","      <th>train</th>\n","      <td>136</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">health care</th>\n","      <th rowspan=\"2\" valign=\"top\">4</th>\n","      <th>train</th>\n","      <td>182</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">infrastructure</th>\n","      <th rowspan=\"2\" valign=\"top\">5</th>\n","      <th>train</th>\n","      <td>458</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">renewable resources &amp; alternative energy</th>\n","      <th rowspan=\"2\" valign=\"top\">6</th>\n","      <th>train</th>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">resource transformation</th>\n","      <th rowspan=\"2\" valign=\"top\">7</th>\n","      <th>train</th>\n","      <td>361</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">services</th>\n","      <th rowspan=\"2\" valign=\"top\">8</th>\n","      <th>train</th>\n","      <td>114</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">technology &amp; communications</th>\n","      <th rowspan=\"2\" valign=\"top\">9</th>\n","      <th>train</th>\n","      <td>259</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">transportation</th>\n","      <th rowspan=\"2\" valign=\"top\">10</th>\n","      <th>train</th>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>val</th>\n","      <td>22</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                          train_text\n","Sector                                   label data_type            \n","consumer goods                           0     train             239\n","                                               val                26\n","extractives & minerals processing        1     train             223\n","                                               val                25\n","financials                               2     train             193\n","                                               val                22\n","food & beverage                          3     train             136\n","                                               val                15\n","health care                              4     train             182\n","                                               val                20\n","infrastructure                           5     train             458\n","                                               val                51\n","renewable resources & alternative energy 6     train              29\n","                                               val                 3\n","resource transformation                  7     train             361\n","                                               val                40\n","services                                 8     train             114\n","                                               val                13\n","technology & communications              9     train             259\n","                                               val                29\n","transportation                           10    train             200\n","                                               val                22"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"cClsR11U4JGy"},"source":["Tokenize data and prepare dataset for training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeEvjrH94RDt","executionInfo":{"status":"ok","timestamp":1626810781409,"user_tz":240,"elapsed":9976,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"8c32a5a7-59f2-4f87-eaf2-bc2c152ef2b1"},"source":["!pip install transformers\n","import torch\n","\n","from tqdm.notebook import tqdm\n","\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","\n","from transformers import BertForSequenceClassification"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["fbcc89297f384483ab9caac29e7540dd","d81194acfd014d31aa1f799383adf0e5","6cc1a9986c644a0db45fd00fc7004fb8","bbe7d922309f448b87b932d619f02f92","b7608b97e976446da8f62012a5a7f12e","eae93ac41ddc4f20a32325fff01465f1","d003098a03b54d6eab824ae995950d25","d77d662234fd425aa5922747277878f0","6b79f8d303fd44d5bb94983551437df7","1001e1f57a394992b68e465cf1151198","a46dc06f8896463183b3252ca47ee5dd","6dfa9795a643471a9da7cf8f89485c7e","f2b87d948aef42c382f473e32f7816d4","b97368b8ab25455380808738a8ef9642","dcf219fba6254f58b61b7a4a348b6034","ca7c9dd5848c4b9882307df93b4a03a9","b679963eec7348079ad4d651065fed11","a36f64cff47d44e1be3584f5cf67d047","cacb4d0a59ce42579303fce171444787","0172ec2d371f4a50b7f793b679e88597","ff7a370ed09c4f718b56bc75bcbabce0","880273c12d624251be09d02cd71434a9","66a11af4566f4fe889cb717893ece246","95125689280b4a99895001f5c4908312"]},"id":"XMFSk2pA3pHw","executionInfo":{"status":"ok","timestamp":1626811093967,"user_tz":240,"elapsed":27524,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"16085837-5637-4b23-82ff-78b2cc9c73d1"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n","                                          do_lower_case=True)\n","                                          \n","encoded_data_train = tokenizer.batch_encode_plus(\n","    sasb_train_eval[sasb_train_eval.data_type=='train'].train_text.values, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    padding=True, \n","    max_length=384, \n","    return_tensors='pt'\n",")\n","\n","encoded_data_val = tokenizer.batch_encode_plus(\n","    sasb_train_eval[sasb_train_eval.data_type=='val'].train_text.values, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    padding=True, \n","    max_length=384, \n","    return_tensors='pt'\n",")\n","\n","encoded_data_test = tokenizer.batch_encode_plus(\n","    sasb_test.train_text.values, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    padding=True, \n","    max_length=384, \n","    return_tensors='pt'\n",")\n","\n","\n","input_ids_train = encoded_data_train['input_ids']\n","attention_masks_train = encoded_data_train['attention_mask']\n","labels_train = torch.tensor(sasb_train_eval[sasb_train_eval.data_type=='train'].label.values)\n","\n","input_ids_val = encoded_data_val['input_ids']\n","attention_masks_val = encoded_data_val['attention_mask']\n","labels_val = torch.tensor(sasb_train_eval[sasb_train_eval.data_type=='val'].label.values)\n","\n","input_ids_test = encoded_data_test['input_ids']\n","attention_masks_test = encoded_data_test['attention_mask']\n","labels_test = torch.tensor(sasb_test.label.values)\n","\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbcc89297f384483ab9caac29e7540dd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b79f8d303fd44d5bb94983551437df7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b679963eec7348079ad4d651065fed11","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j3L5ER8x5qZZ"},"source":["BERT pre-trained model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["09a994b51b174e1f8c690a842170712d","cba392fab998425e9aca50b79073bac0","475c115071ed40da84e88bb859607491","aefce81f4a56451fb826e34b1a36aee4","14aa4ebfb9154d018181b3f224aa97ba","e5efa2a6b016403590b4a59d1f0212e1","84cbf730f2a54d79ba196191f2b53103","4d8c771742364e1d86911bbd9a0283d9","860a69609d50404380a59f9749302861","22b8fafab0d944f78c88e52749af1009","3f587c58722746148cac6703adb19de3","232599bf6e8941f6ad1d0e718a7c1808","3b6c43b09bbd49cdae78421afc921032","adeeddcd598b44d3be94633247400ec4","b8df206b472241dea189f97bf7d03fb9","763057edfcde4206bbc7843c7c35492f"]},"id":"N-u1vsD83pOu","executionInfo":{"status":"ok","timestamp":1626811121052,"user_tz":240,"elapsed":11989,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"e0739327-48c0-48c3-988f-997669dae838"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09a994b51b174e1f8c690a842170712d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"860a69609d50404380a59f9749302861","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HV4OOiE56u18"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 3\n","\n","dataloader_train = DataLoader(dataset_train, \n","                              sampler=RandomSampler(dataset_train), \n","                              batch_size=batch_size)\n","\n","dataloader_validation = DataLoader(dataset_val, \n","                                   sampler=SequentialSampler(dataset_val), \n","                                   batch_size=batch_size)\n","\n","dataloader_test = DataLoader(dataset_test, \n","                                   sampler=SequentialSampler(dataset_test), \n","                                   batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlWVj4YO7OAw"},"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr=1e-5, \n","                  eps=1e-8)\n","                  \n","epochs = 5\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(dataloader_train)*epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sLClTe68MDD"},"source":["from sklearn.metrics import f1_score\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","    \n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asxWtrOA_01p"},"source":["import random\n","import numpy as np\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zeAVNmyQ_4wT","executionInfo":{"status":"ok","timestamp":1626811155627,"user_tz":240,"elapsed":11906,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"1b4edd0f-6865-479a-b0ea-13b87dc4ece1"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","print(device)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5351fb60c5904465865a1e91390c6dc5","0eff3848993b4626a7168c1d513036d3","1238cf28450e4b4eb6340382d71777a6","7928fb37b6124969a91d0c1352c35a4e","8da8a6762e4046da9043fbcd295e1e17","f00f58b8f1e44cb9a95e543823f37979","c42f197591854f4bb2af217114ce1dc5","5f1e1ae14fac4bf996b1770c55215ed6","58324d0fd078444cbea81c00653a920b","b99d7211fa134762b0db58a7132a972f","282e63ce97d7485794dd07375a89dcbe","401cfddd0bb74e2fb281ff7ab4dcb484","2d5a28d9a4034d7fb96795450a93042c","165c129f44154e9b93a218b62a2a4219","79e85778ac5b490684b3f5c9cce0d948","76a12774e58e4c5789fc001dc46a43aa","a1ce0364c07648f6baae20c58e187faa","bb7b9376dcf142e9b8a81ad8f5c5f6d7","44aab4bd9b804ae98fd59b8495d195a8","01cb7fff4b9240c39b07091692ca8d7e","5c85b1e83f8b47ef998616a9bacb1faa","4cd0807d6cca43d19c2d059968936df1","9fbffee5637b4cd3b4ae8812a077e5db","8025b2912f8143d8ada4af990970ed60","66b324d36ddc444ba97cca59938d9d49","e2db1d3310474163a83315ddb868e4b8","f324f963c2ab4c65988a2849e48ed5a7","d510cb99a8274db48be3ab8d1116b23c","b3e84146ef884b068e436b967c4b05fb","a06a35cb5bc84c84bd87bc56ef1599df","7bdd088883324efeabfc631dd0221d15","2f1402f41c4040d0bc992c34e02e04b1","9d4072af09144b09a7c8497eb6c2cf1b","b6e3db500d0945a582db8c2eaa91afdb","49c6c584d38d4883b2919b8b24ca65af","812eb7dd84d545b688bdc4af49bfe323","65cd2976246c4f9ab1b4ba97f59aba5f","227deb1d5c784e1995c5e543834c6ba6","42c91327a77841b38f18fe7a17d07b06","0bd64f2e3beb45fcac780ad882875fcb","3b1b58a6327b47efb589608cf7645a72","6e2f8ca4a11a40ff9632f48d35cfa626","91cd59fbee5f431d82a8d02f7f38f168","212df4f7cc094772bb95d14f297bd603","f377e737f6f54df285422a3644d48c28","f2be373327b74b088ed1d254b1960e11","8c131cc706a9457ebd720cea8b56abf1","95a19ab6c6704a26a6cc4adb34a3f3fe"]},"id":"z4JXtu9Z99_p","executionInfo":{"status":"ok","timestamp":1626812681837,"user_tz":240,"elapsed":1480732,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"98ee932b-63c6-4d4e-f1cf-ddf84be76348"},"source":["def evaluate(dataloader_val):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","    \n","    for batch in dataloader_val:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        print(\"During training one batch, the loss of the output is:\", loss)\n","        print(\"Loss.item:\", loss.item())\n","        logits = outputs[1]\n","        print(\"During training one batch, the logits of the output is:\", logits)\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_val) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals\n","    \n","for epoch in tqdm(range(1, epochs+1)):\n","    \n","    model.train()\n","    \n","    loss_train_total = 0\n","\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","\n","        model.zero_grad()\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }       \n","\n","        outputs = model(**inputs)\n","        \n","        loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Performs a single optimization step (parameter update).\n","        optimizer.step()\n","        # You call scheduler.step() every batch, right after optimizer.step(), to update the learning rate.\n","        scheduler.step()\n","        \n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","         \n","        \n","    torch.save(model.state_dict(), f'second_finetuned_BERT_epoch_{epoch}.model')\n","        \n","    tqdm.write(f'\\nEpoch {epoch}')\n","    \n","    loss_train_avg = loss_train_total/len(dataloader_train)            \n","    tqdm.write(f'Training loss: {loss_train_avg}')\n","    \n","    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n","    val_f1 = f1_score_func(predictions, true_vals)\n","    tqdm.write(f'Validation loss: {val_loss}')\n","    tqdm.write(f'F1 Score (Weighted): {val_f1}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5351fb60c5904465865a1e91390c6dc5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58324d0fd078444cbea81c00653a920b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=798.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 1\n","\rTraining loss: 1.0508883071171684\n","During training one batch, the loss of the output is: tensor(0.0663, device='cuda:0')\n","Loss.item: 0.06630364805459976\n","During training one batch, the logits of the output is: tensor([[-0.6865, -0.8005,  0.1129, -0.1187,  4.8091, -1.2620, -0.7044,  0.3684,\n","         -1.1163, -0.0608,  0.0755],\n","        [-0.5453, -0.8886, -0.0203, -0.7657, -0.0732, -0.2819, -0.0276, -0.4421,\n","         -0.2821,  5.3303, -0.9420],\n","        [-0.6616, -0.4183,  4.2310, -0.4848,  0.6458, -0.3895, -0.6974, -0.8123,\n","         -0.7116, -0.2900,  0.1634]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0978, device='cuda:0')\n","Loss.item: 0.09782391041517258\n","During training one batch, the logits of the output is: tensor([[-0.6639, -0.4180,  4.2361, -0.4892,  0.6438, -0.3837, -0.6977, -0.8172,\n","         -0.7074, -0.2841,  0.1546],\n","        [ 0.2844, -0.2100, -0.4576,  4.3961, -0.1006, -0.8701, -0.3782, -0.0558,\n","         -0.8827, -0.4972,  0.1545],\n","        [ 0.2361, -0.1768, -0.5463,  4.3671, -0.1739, -0.8244, -0.3715,  0.0367,\n","         -0.8305, -0.4868,  0.0678]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0692, device='cuda:0')\n","Loss.item: 0.06922125071287155\n","During training one batch, the logits of the output is: tensor([[ 0.2168, -0.2002, -0.4744,  4.3821, -0.1046, -0.8179, -0.3579, -0.0788,\n","         -0.8312, -0.4950,  0.1594],\n","        [ 4.6568, -1.0961, -0.3261,  0.1022, -0.7349, -0.8298, -0.3997,  0.1972,\n","          0.0975, -0.5226, -1.5367],\n","        [-0.9631, -0.6046, -0.2936, -0.1572, -0.3209, -0.3265, -0.3445,  0.3642,\n","         -0.3158, -1.4483,  4.9166]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.1426, device='cuda:0')\n","Loss.item: 1.1425590515136719\n","During training one batch, the logits of the output is: tensor([[ 1.1213e+00, -2.7113e-01,  7.0176e-01, -9.4738e-01, -1.2799e-01,\n","         -8.7536e-01, -1.3124e-01,  1.3253e+00,  1.1170e+00, -1.4580e+00,\n","         -4.8888e-01],\n","        [ 2.9291e-01, -4.1912e-02, -4.6279e-01,  4.3190e+00, -3.1971e-01,\n","         -7.5730e-01, -3.8225e-01,  2.8639e-03, -8.6507e-01, -5.3557e-01,\n","         -2.7969e-02],\n","        [ 4.6463e+00, -1.0573e+00, -1.0163e-01,  3.8512e-01, -7.0964e-01,\n","         -8.1171e-01, -4.6647e-01, -1.3402e-01, -2.5795e-01, -4.7948e-01,\n","         -1.5561e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.1319, device='cuda:0')\n","Loss.item: 1.1319202184677124\n","During training one batch, the logits of the output is: tensor([[-0.3975,  0.1664, -1.2258, -0.9031, -0.1014, -0.9690, -0.9775,  4.8672,\n","         -0.8166, -0.4293, -0.3333],\n","        [-0.6787, -0.8181,  0.1926,  0.0523,  4.8358, -1.1912, -0.6450,  0.0744,\n","         -1.0673, -0.1046,  0.1529],\n","        [-0.6828, -0.3629,  3.9377, -0.4707,  0.7978, -0.5928, -0.7352, -0.6109,\n","         -0.7697, -0.3265,  0.4973]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1045, device='cuda:0')\n","Loss.item: 0.10450541973114014\n","During training one batch, the logits of the output is: tensor([[-0.5222, -1.0999,  1.7637, -0.4678,  3.8419, -0.6417, -0.6614, -0.5759,\n","         -0.8514, -0.1737, -0.0983],\n","        [-0.9561, -0.6252, -0.2192, -0.0111, -0.3725, -0.3132, -0.2801,  0.1047,\n","         -0.4146, -1.4330,  4.9116],\n","        [-0.2769,  0.3990, -1.2274, -0.7749, -0.4598, -0.9521, -0.7139,  4.8125,\n","         -0.6246, -0.7328, -0.3722]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.4088, device='cuda:0')\n","Loss.item: 0.40876078605651855\n","During training one batch, the logits of the output is: tensor([[ 0.0477,  0.0621, -1.2649, -0.7851, -0.3355, -1.1515, -0.7988,  4.8682,\n","         -0.6120, -0.4470, -0.4353],\n","        [ 0.6123,  0.5415, -0.4464, -0.3694, -1.4377,  0.4895,  0.2323, -0.7912,\n","          2.2788, -0.3691, -0.0187],\n","        [ 0.5526, -0.2537, -0.6223, -0.8063, -1.4849,  1.4190,  0.5582, -0.7927,\n","          2.9651, -0.1045, -0.5169]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1684, device='cuda:0')\n","Loss.item: 0.1683889627456665\n","During training one batch, the logits of the output is: tensor([[ 0.7261, -0.2823, -0.6360, -0.6746, -1.4637,  1.0325,  0.5739, -0.7738,\n","          2.9742, -0.1034, -0.4220],\n","        [ 0.0179,  0.2251, -1.1486, -0.7559, -0.4561, -1.0863, -0.7536,  4.8283,\n","         -0.7574, -0.8455, -0.2884],\n","        [-0.9554, -0.1744, -0.4458, -1.2132, -1.1550,  5.4447, -0.6289, -0.9940,\n","         -0.3348, -0.7888, -0.8405]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0550, device='cuda:0')\n","Loss.item: 0.05498102307319641\n","During training one batch, the logits of the output is: tensor([[-0.4486,  0.2581, -1.1681, -0.9805, -0.4479, -0.7709, -0.8098,  4.8379,\n","         -0.6640, -0.5511, -0.3631],\n","        [ 0.2169, -0.2127, -1.3202, -0.7296, -0.3836, -1.1642, -0.7120,  4.6781,\n","         -0.5958, -0.7222, -0.0364],\n","        [ 0.5598, -0.3494, -1.3626, -0.6973, -0.2857, -1.3421, -0.6966,  4.6308,\n","         -0.4656, -0.5261, -0.4604]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.7755, device='cuda:0')\n","Loss.item: 0.7754928469657898\n","During training one batch, the logits of the output is: tensor([[-1.2033, -0.1235, -0.1200, -0.3651, -0.4947, -0.3073, -0.3223,  0.5305,\n","         -0.5736, -1.5231,  4.6041],\n","        [ 0.2813,  0.9285, -0.9799, -0.9850, -1.6217,  0.0760,  1.2584,  0.2952,\n","          1.8590, -0.2451, -0.8541],\n","        [-0.7758,  2.9792, -1.2707, -1.1305, -1.5902,  1.1234, -0.4470,  2.3081,\n","         -0.4064, -1.1276, -1.3132]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0383, device='cuda:0')\n","Loss.item: 0.03826139494776726\n","During training one batch, the logits of the output is: tensor([[-0.1685, -0.3261, -1.3066, -1.2112, -0.1057, -1.0859, -0.9475,  4.6293,\n","         -0.7021,  1.0212, -0.7111],\n","        [-1.0626,  0.1702, -0.3373, -1.2181, -1.1850,  5.4537, -0.6537, -0.9869,\n","         -0.3938, -0.9215, -0.9093],\n","        [-0.8669, -0.1466, -0.4001, -1.1517, -1.1811,  5.4368, -0.6357, -1.0531,\n","         -0.2177, -1.0544, -0.7672]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0577, device='cuda:0')\n","Loss.item: 0.05774020031094551\n","During training one batch, the logits of the output is: tensor([[-1.0185, -0.1019, -0.4219, -1.1650, -1.1591,  5.4429, -0.6660, -1.0760,\n","         -0.3321, -1.1069, -0.5265],\n","        [-1.0127, -0.4587, -0.4411, -0.2501, -0.4618, -0.0444, -0.4077,  0.4275,\n","         -0.3456, -1.5387,  4.7886],\n","        [ 0.2381, -0.0968, -0.4671,  4.3761, -0.2193, -0.8433, -0.3685,  0.0092,\n","         -0.8434, -0.5181,  0.0748]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1115, device='cuda:0')\n","Loss.item: 0.11145936697721481\n","During training one batch, the logits of the output is: tensor([[ 0.0581, -0.5003,  4.4950, -0.3458,  0.1571, -0.2692, -0.4521, -1.1593,\n","         -0.3006, -0.1325, -0.5710],\n","        [-0.1419, -0.4210,  4.4922, -0.4430,  0.1916, -0.3392, -0.4702, -1.0212,\n","         -0.2721, -0.2454, -0.4101],\n","        [ 0.5781, -0.9402, -0.4470, -0.0550, -0.6298, -0.5266, -0.7416,  1.3601,\n","         -0.3130, -1.8847,  3.9630]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0543, device='cuda:0')\n","Loss.item: 0.05428232625126839\n","During training one batch, the logits of the output is: tensor([[-0.2504, -0.0161, -1.1367, -0.9700, -0.2926, -1.0541, -0.8941,  4.9147,\n","         -0.7199, -0.2061, -0.3657],\n","        [-0.2776,  0.0693, -1.1962, -0.9327, -0.1402, -1.0937, -0.9082,  4.9323,\n","         -0.6610, -0.3190, -0.3671],\n","        [-0.2076, -0.5333,  4.5079, -0.4707,  0.2970, -0.3890, -0.5257, -1.0185,\n","         -0.2527, -0.2600, -0.3069]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2023, device='cuda:0')\n","Loss.item: 0.20234882831573486\n","During training one batch, the logits of the output is: tensor([[ 0.9989, -0.5596, -0.3258, -0.7232, -1.4852,  0.7611,  0.5813, -0.6767,\n","          2.9081,  0.0431, -0.5402],\n","        [-0.0324,  0.0416, -1.2688, -0.9840, -0.6563, -0.8531, -0.8824,  4.7884,\n","         -0.5516, -0.6663, -0.2166],\n","        [ 0.3834, -0.3547, -0.3582,  4.3909,  0.1945, -0.9542, -0.4285, -0.1501,\n","         -0.9752, -0.4499,  0.1358]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0766, device='cuda:0')\n","Loss.item: 0.07656693458557129\n","During training one batch, the logits of the output is: tensor([[ 0.4629, -0.3974, -0.4401,  4.3862,  0.2172, -1.0299, -0.4388, -0.0184,\n","         -0.9575, -0.5252,  0.1162],\n","        [ 4.6296, -1.1203, -0.3749,  0.1966, -0.9686, -0.9411, -0.4298,  0.4842,\n","          0.0581, -0.6997, -1.3278],\n","        [ 4.6774, -1.0854, -0.3683,  0.4634, -0.7054, -1.0900, -0.4955,  0.4717,\n","         -0.3017, -0.4582, -1.5535]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0350, device='cuda:0')\n","Loss.item: 0.03496715798974037\n","During training one batch, the logits of the output is: tensor([[-7.8833e-02, -6.1416e-01, -3.3619e-01, -9.0754e-01, -6.6800e-01,\n","         -3.2115e-01,  1.2210e-01, -4.1465e-01,  5.2385e-01,  5.1305e+00,\n","         -1.8612e+00],\n","        [-5.8106e-01, -6.6498e-01, -3.1305e-01, -6.6097e-01, -1.2248e-01,\n","         -4.4421e-01, -3.4815e-02,  1.5168e-01, -5.1221e-01,  5.3660e+00,\n","         -1.2467e+00],\n","        [-5.9141e-01, -6.7020e-01, -2.1327e-01, -6.3709e-01, -1.0411e-01,\n","         -4.2346e-01, -3.6348e-03, -5.3809e-02, -4.8125e-01,  5.4051e+00,\n","         -1.2016e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.5228, device='cuda:0')\n","Loss.item: 0.5228312611579895\n","During training one batch, the logits of the output is: tensor([[ 0.3675, -0.5032, -1.4741, -0.5737,  0.0536, -1.2241, -0.7905,  4.4768,\n","         -0.6610, -0.4475, -0.5296],\n","        [ 0.2443,  0.7303, -0.8627, -0.9148, -1.6605,  0.0672,  1.4090, -0.0579,\n","          1.6691, -0.2035, -0.6608],\n","        [ 4.6213, -1.0636, -0.3191,  0.3228, -0.8240, -1.0770, -0.3528,  0.3886,\n","         -0.0416, -0.5639, -1.6918]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0788, device='cuda:0')\n","Loss.item: 0.07878443598747253\n","During training one batch, the logits of the output is: tensor([[ 0.6473, -0.4190, -0.2948,  4.3173,  0.2403, -1.0653, -0.4846, -0.0882,\n","         -1.1176, -0.5161,  0.0604],\n","        [-0.2973,  0.7686, -1.1963, -0.8811, -0.5212, -0.9889, -0.7646,  4.7677,\n","         -0.5856, -0.5872, -0.6036],\n","        [ 4.5327, -0.9800, -0.2781,  0.2711, -0.9534, -1.0632, -0.3551,  0.4784,\n","         -0.0233, -0.6899, -1.6431]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1197, device='cuda:0')\n","Loss.item: 0.1197102889418602\n","During training one batch, the logits of the output is: tensor([[ 1.2444, -0.5475, -0.3493,  4.1960,  0.0634, -1.1142, -0.4921, -0.0965,\n","         -1.0872, -0.5789, -0.0358],\n","        [ 1.4425, -0.5875, -0.4391,  4.0892,  0.0488, -1.1455, -0.5357,  0.0165,\n","         -1.0995, -0.6683, -0.0653],\n","        [-0.6165, -0.8314,  0.3367,  0.1476,  4.6914, -1.2069, -0.7139,  0.0957,\n","         -1.2839, -0.2483,  0.1278]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0388, device='cuda:0')\n","Loss.item: 0.03884391114115715\n","During training one batch, the logits of the output is: tensor([[-0.5147, -0.7732, -0.2996, -0.7133, -0.0255, -0.5244, -0.0796,  0.1887,\n","         -0.3801,  5.4472, -1.2360],\n","        [-0.8881,  0.2329, -0.4881, -1.4013, -1.5864,  5.3506, -0.6030, -0.9580,\n","          0.0738, -0.8786, -0.8995],\n","        [-0.6886,  4.6499, -0.6304, -0.5107, -1.0952, -0.2993, -0.2507,  0.5202,\n","         -0.5849, -0.8184, -0.9244]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0462, device='cuda:0')\n","Loss.item: 0.046196918934583664\n","During training one batch, the logits of the output is: tensor([[-1.0822, -0.4095, -0.1489, -0.2047, -0.4033, -0.4450, -0.2197,  0.2459,\n","         -0.4961, -1.4214,  4.7755],\n","        [-0.7038, -0.7789,  0.1493,  0.0711,  4.7958, -1.1692, -0.7210,  0.1797,\n","         -1.1742, -0.1525,  0.1072],\n","        [-0.8986, -0.0607, -0.5404, -1.2458, -1.2925,  5.4314, -0.6530, -0.8814,\n","         -0.1608, -0.7386, -1.1103]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.5220, device='cuda:0')\n","Loss.item: 0.5220351219177246\n","During training one batch, the logits of the output is: tensor([[ 3.0126e-01,  9.1041e-01, -9.2586e-01, -8.9181e-01, -1.6436e+00,\n","          1.2914e-03,  1.3737e+00,  1.9177e-01,  1.6433e+00, -1.5639e-01,\n","         -9.8915e-01],\n","        [-5.6592e-02, -8.4243e-02, -1.1950e+00, -9.9018e-01, -4.9427e-01,\n","         -8.4138e-01, -9.3730e-01,  4.9381e+00, -6.5253e-01, -4.7783e-01,\n","         -2.5416e-01],\n","        [-2.0674e-01, -4.0868e-02, -1.2002e+00, -1.0232e+00, -4.6106e-01,\n","         -8.0446e-01, -9.1254e-01,  4.9144e+00, -6.2013e-01, -3.2536e-01,\n","         -3.6995e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0547, device='cuda:0')\n","Loss.item: 0.05469030514359474\n","During training one batch, the logits of the output is: tensor([[-0.0575,  0.1080, -0.9933, -0.8181, -0.3243, -1.0975, -0.8014,  4.8292,\n","         -0.6982, -0.7958, -0.2081],\n","        [ 0.0740,  0.6646, -1.2083, -0.7961, -0.8872, -0.8717, -0.5588,  4.5612,\n","         -0.3800, -1.0806, -0.7192],\n","        [-0.1412,  0.3708, -1.1023, -0.8392, -0.4589, -0.9986, -0.7727,  4.8240,\n","         -0.6544, -0.7432, -0.3989]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0580, device='cuda:0')\n","Loss.item: 0.057965587824583054\n","During training one batch, the logits of the output is: tensor([[ 0.0286,  0.3969, -1.1973, -0.8654, -0.5800, -0.9272, -0.7023,  4.7440,\n","         -0.5067, -0.7951, -0.6185],\n","        [ 0.6960, -0.2988, -1.1310, -0.7372, -0.3731, -1.2378, -0.7494,  4.7362,\n","         -0.4639, -0.6752, -0.7035],\n","        [ 1.0056, -0.4956, -1.1335, -0.7826, -0.4758, -1.2709, -0.7587,  4.6853,\n","         -0.4421, -0.4649, -0.8917]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0717, device='cuda:0')\n","Loss.item: 0.07171884924173355\n","During training one batch, the logits of the output is: tensor([[ 0.7566, -0.3337, -1.1798, -0.8226, -0.3987, -1.1618, -0.7643,  4.7132,\n","         -0.3990, -0.6146, -0.8707],\n","        [-0.2294, -0.4266,  4.4817, -0.4223,  0.2587, -0.3056, -0.5393, -1.0980,\n","         -0.3146, -0.2650, -0.3270],\n","        [-0.2134, -0.4671,  4.4979, -0.4791,  0.2855, -0.3557, -0.5406, -1.0936,\n","         -0.2701, -0.2510, -0.3067]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1682, device='cuda:0')\n","Loss.item: 0.1682407706975937\n","During training one batch, the logits of the output is: tensor([[-0.4876, -0.9439, -0.0159, -0.6670,  0.0949, -0.4189, -0.0239, -0.4165,\n","         -0.3150,  5.3164, -0.9954],\n","        [-1.2845,  0.8940, -0.4451, -0.5851, -0.6365, -0.3411, -0.3649,  0.8157,\n","         -0.5975, -1.4639,  4.1100],\n","        [-1.2808,  1.6205, -0.5786, -0.7774, -0.8143, -0.4236, -0.4356,  1.3098,\n","         -0.5976, -1.5383,  3.4537]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0803, device='cuda:0')\n","Loss.item: 0.08033718913793564\n","During training one batch, the logits of the output is: tensor([[-0.8254,  4.6141, -0.6384, -0.4786, -0.9204, -0.5975, -0.2800,  1.0108,\n","         -0.6270, -0.9173, -0.6546],\n","        [ 4.4878, -1.2475, -0.7234,  0.3137, -0.8728, -1.0433, -0.6564,  1.5774,\n","         -0.2452, -0.8141, -1.3705],\n","        [ 4.6142, -1.0685,  0.1076,  0.2228, -0.7017, -0.7587, -0.5325, -0.0829,\n","         -0.1691, -0.5562, -1.5284]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0674, device='cuda:0')\n","Loss.item: 0.0674140676856041\n","During training one batch, the logits of the output is: tensor([[ 4.5200, -0.9562,  0.0443,  0.7037, -0.8102, -0.7324, -0.4076, -0.2873,\n","         -0.0804, -0.7382, -1.6016],\n","        [ 4.6863, -1.0368, -0.1286,  0.7484, -0.7116, -0.9864, -0.4668, -0.0499,\n","         -0.3199, -0.6745, -1.4666],\n","        [ 4.7004, -1.0514, -0.1424,  0.3053, -0.7200, -0.8915, -0.4648,  0.0429,\n","         -0.1647, -0.5092, -1.5413]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0466, device='cuda:0')\n","Loss.item: 0.04661784693598747\n","During training one batch, the logits of the output is: tensor([[-0.8780,  0.0945, -0.6740, -1.3420, -1.4365,  5.4241, -0.6569, -0.6888,\n","         -0.1216, -0.7671, -1.0487],\n","        [-0.6346, -0.8406,  0.0856, -0.0303,  4.8254, -1.2323, -0.6468,  0.3489,\n","         -1.0633, -0.1679,  0.0550],\n","        [-0.6297, -0.8365,  0.0889, -0.0564,  4.8249, -1.2286, -0.6417,  0.3742,\n","         -1.0687, -0.1771,  0.0523]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.6150, device='cuda:0')\n","Loss.item: 1.6149848699569702\n","During training one batch, the logits of the output is: tensor([[ 0.2744, -0.3867, -1.1807, -0.9162, -0.2138, -1.1699, -0.9267,  4.8919,\n","         -0.6619, -0.1899, -0.5014],\n","        [-0.6116, -0.4012,  4.2735, -0.5331,  0.5789, -0.3559, -0.7163, -0.8217,\n","         -0.6879, -0.3388,  0.1894],\n","        [-0.5560, -0.7691, -0.0094, -0.2813,  4.7030, -1.4160, -0.7288,  0.7844,\n","         -1.0750, -0.1865,  0.0347]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0495, device='cuda:0')\n","Loss.item: 0.04953538253903389\n","During training one batch, the logits of the output is: tensor([[ 0.0224, -0.0733, -1.1148, -0.8750, -0.3526, -1.0460, -0.8808,  4.9047,\n","         -0.7360, -0.6309, -0.2779],\n","        [ 0.1037, -0.1669, -1.1278, -0.8182, -0.3476, -1.0770, -0.8307,  4.8995,\n","         -0.6974, -0.6690, -0.2489],\n","        [-0.7599,  4.6513, -0.6341, -0.4182, -1.0130, -0.3480, -0.2975,  0.6124,\n","         -0.6713, -0.8574, -0.8296]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0568, device='cuda:0')\n","Loss.item: 0.05676485225558281\n","During training one batch, the logits of the output is: tensor([[-1.3617,  4.1035, -0.7442, -0.5373, -0.8427,  0.8608, -0.4669,  0.7477,\n","         -0.9632, -1.0726, -0.4816],\n","        [-0.7385, -0.2792, -0.3889, -1.1523, -1.1972,  5.4326, -0.6502, -1.0511,\n","         -0.0654, -0.9841, -0.9086],\n","        [-0.8294, -0.2141, -0.4048, -1.1814, -1.1837,  5.4609, -0.6662, -1.0477,\n","         -0.1307, -0.9610, -0.8732]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0789, device='cuda:0')\n","Loss.item: 0.07893440127372742\n","During training one batch, the logits of the output is: tensor([[-0.8107, -0.2810, -0.4125, -1.1752, -1.1757,  5.4429, -0.6737, -1.0241,\n","         -0.1697, -0.9063, -0.9127],\n","        [-1.0923, -0.2653, -0.3685, -1.1573, -1.1267,  5.4695, -0.7225, -1.1317,\n","         -0.3143, -0.5722, -0.8112],\n","        [-0.2149, -0.8758,  3.7972, -0.4912,  0.2671, -0.8682, -0.5851, -0.8098,\n","         -0.5475, -0.0819,  1.2573]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1636, device='cuda:0')\n","Loss.item: 0.16356529295444489\n","During training one batch, the logits of the output is: tensor([[-0.3030, -0.6569,  4.2471, -0.5955,  0.4565, -0.7159, -0.6167, -0.8831,\n","         -0.3345, -0.3500,  0.4396],\n","        [ 0.1767, -1.1485,  3.4127, -0.5486,  0.2646, -1.0438, -0.6312, -0.5944,\n","         -0.5263,  0.3983,  1.1418],\n","        [-0.4457, -0.5103,  4.2893, -0.5103,  0.6141, -0.4544, -0.7084, -0.7991,\n","         -0.6499, -0.3081,  0.0701]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0978, device='cuda:0')\n","Loss.item: 0.09780355542898178\n","During training one batch, the logits of the output is: tensor([[-0.4325, -0.5642,  4.3495, -0.5433,  0.6213, -0.3846, -0.7120, -0.8720,\n","         -0.6308, -0.2035, -0.0387],\n","        [-0.4809, -0.5410,  4.3217, -0.5209,  0.6376, -0.3836, -0.7331, -0.8429,\n","         -0.6770, -0.2331,  0.0169],\n","        [-0.5249, -0.4777,  4.2561, -0.4887,  0.6557, -0.4386, -0.7131, -0.7799,\n","         -0.7067, -0.2357,  0.1107]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0495, device='cuda:0')\n","Loss.item: 0.049500931054353714\n","During training one batch, the logits of the output is: tensor([[-0.9764, -0.6126, -0.1421, -0.0388, -0.2957, -0.3774, -0.2630,  0.1481,\n","         -0.4179, -1.4804,  4.9202],\n","        [-0.9615, -0.6150, -0.2246, -0.0587, -0.3184, -0.3697, -0.2530,  0.1755,\n","         -0.3848, -1.4913,  4.9339],\n","        [-0.9749, -0.6413, -0.1868,  0.0277, -0.2656, -0.4075, -0.2477,  0.1138,\n","         -0.4132, -1.4107,  4.9192]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0579, device='cuda:0')\n","Loss.item: 0.057900238782167435\n","During training one batch, the logits of the output is: tensor([[-0.9858, -0.6289, -0.1675,  0.0116, -0.2563, -0.3886, -0.2392,  0.0933,\n","         -0.4207, -1.4199,  4.9198],\n","        [-0.7281,  4.6909, -0.6724, -0.4484, -0.8946, -0.5909, -0.3089,  0.7150,\n","         -0.6054, -0.8087, -0.6748],\n","        [-0.7045,  4.6665, -0.6091, -0.4686, -0.9624, -0.5575, -0.3061,  0.7235,\n","         -0.6240, -0.8280, -0.7143]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0654, device='cuda:0')\n","Loss.item: 0.06544338166713715\n","During training one batch, the logits of the output is: tensor([[-0.6890,  1.2270, -1.1448, -0.7542, -0.4592, -0.8071, -0.8610,  4.5221,\n","         -0.9234, -1.0405, -0.1828],\n","        [ 0.0599, -0.1382, -1.0742, -0.8934, -0.0856, -1.2314, -0.8947,  4.8788,\n","         -0.7120, -0.1577, -0.5487],\n","        [ 4.5521, -1.0459, -0.3590, -0.0455, -0.8423, -0.8028, -0.4551,  0.3791,\n","          0.0051, -0.5701, -1.5605]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0231, device='cuda:0')\n","Loss.item: 0.023068664595484734\n","During training one batch, the logits of the output is: tensor([[-1.0451,  0.0470, -0.2873, -1.3095, -1.4054,  5.4599, -0.6854, -1.0673,\n","         -0.1320, -0.7677, -0.8694],\n","        [-0.9774, -0.0116,  0.1074, -1.3700, -1.4390,  5.3746, -0.6825, -1.1240,\n","          0.0152, -0.8520, -0.9151],\n","        [-1.0111, -0.0510, -0.4232, -1.2309, -1.3256,  5.4808, -0.6747, -1.1149,\n","         -0.1139, -0.6997, -0.9275]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.5326, device='cuda:0')\n","Loss.item: 0.5325732827186584\n","During training one batch, the logits of the output is: tensor([[-1.3508,  1.0800, -0.7145, -1.1666, -1.3550,  5.1670, -0.6881, -0.8435,\n","         -0.4779, -0.9565, -0.8955],\n","        [-1.2282,  3.6352, -0.5362, -1.0726, -1.6389,  2.5177, -0.5179,  0.0289,\n","         -0.4542, -1.3080, -0.9955],\n","        [-0.2212, -0.5301,  4.5075, -0.4604,  0.3402, -0.4030, -0.5125, -1.0113,\n","         -0.3144, -0.1617, -0.3322]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0661, device='cuda:0')\n","Loss.item: 0.06606047600507736\n","During training one batch, the logits of the output is: tensor([[ 4.6311, -1.2388, -0.5578,  0.0356, -0.7748, -0.9700, -0.5662,  0.8890,\n","         -0.1513, -0.8739, -1.2243],\n","        [ 4.6449, -1.2345, -0.4617,  0.0455, -0.7444, -0.9122, -0.5014,  0.6288,\n","         -0.1315, -0.7314, -1.3194],\n","        [ 4.6325, -1.2126, -0.4509,  0.1633, -0.7562, -0.9506, -0.5011,  0.5204,\n","         -0.1414, -0.7299, -1.3330]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0642, device='cuda:0')\n","Loss.item: 0.06419333070516586\n","During training one batch, the logits of the output is: tensor([[ 4.6575, -1.1464, -0.3855,  0.2412, -0.7207, -0.9388, -0.4573,  0.1900,\n","         -0.0454, -0.5882, -1.4003],\n","        [ 4.7353, -1.1833, -0.3519,  0.2491, -0.6900, -0.9574, -0.5040,  0.3954,\n","         -0.1503, -0.5487, -1.4908],\n","        [-0.6840,  4.5606, -0.4630, -0.3028, -0.8055, -0.6801, -0.2838,  0.6797,\n","         -0.6216, -0.7002, -0.6999]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0664, device='cuda:0')\n","Loss.item: 0.0663868710398674\n","During training one batch, the logits of the output is: tensor([[-0.7070,  4.5521, -0.4228, -0.3093, -0.7859, -0.6863, -0.2902,  0.6946,\n","         -0.6362, -0.7051, -0.6865],\n","        [ 4.7407, -1.1991, -0.3536,  0.2815, -0.6781, -0.9926, -0.5302,  0.4626,\n","         -0.1534, -0.5494, -1.4890],\n","        [-0.4688, -0.8597,  0.3204, -0.1127,  4.7006, -1.1412, -0.6895,  0.1836,\n","         -1.0893, -0.2754,  0.1778]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0509, device='cuda:0')\n","Loss.item: 0.05091458559036255\n","During training one batch, the logits of the output is: tensor([[-6.3821e-01, -7.7672e-01,  3.0157e-01, -2.8794e-03,  4.7428e+00,\n","         -1.0986e+00, -6.8355e-01, -1.5031e-02, -1.1337e+00, -2.1184e-01,\n","          2.3334e-01],\n","        [-8.1271e-01, -6.3919e-01, -1.6028e-01,  4.5834e-02, -2.4290e-01,\n","         -5.5192e-01, -1.9454e-01,  5.9744e-02, -3.1627e-01, -1.5500e+00,\n","          4.7660e+00],\n","        [-4.1803e-01, -9.0458e-01, -1.2243e-01, -7.6923e-01,  2.8994e-02,\n","         -4.8552e-01,  4.3699e-03, -2.8189e-01, -1.3721e-01,  5.4185e+00,\n","         -1.1022e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0302, device='cuda:0')\n","Loss.item: 0.030159935355186462\n","During training one batch, the logits of the output is: tensor([[-5.2474e-01, -9.0159e-01,  1.4478e-03, -6.6120e-01,  1.1165e-01,\n","         -5.8424e-01,  1.5892e-02, -3.4646e-01, -2.9631e-01,  5.4244e+00,\n","         -9.7668e-01],\n","        [-4.9895e-01, -8.2538e-01, -3.8159e-02, -7.1105e-01, -1.5757e-02,\n","         -4.5025e-01,  1.0969e-02, -3.3035e-01, -2.5800e-01,  5.4391e+00,\n","         -1.1802e+00],\n","        [-5.1233e-01, -8.8776e-01, -1.2118e-02, -6.4314e-01,  1.0348e-01,\n","         -5.8248e-01,  2.2833e-02, -3.5564e-01, -3.2541e-01,  5.4279e+00,\n","         -1.0112e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0595, device='cuda:0')\n","Loss.item: 0.05953255295753479\n","During training one batch, the logits of the output is: tensor([[-5.0991e-01, -8.7782e-01, -3.0451e-02, -6.4012e-01,  9.5103e-02,\n","         -5.8647e-01,  2.3324e-02, -3.3641e-01, -3.0568e-01,  5.4369e+00,\n","         -1.0211e+00],\n","        [ 5.9567e-01, -3.6547e-01, -5.0598e-01,  4.3706e+00, -5.1979e-03,\n","         -1.0398e+00, -4.2902e-01,  2.3845e-01, -9.9932e-01, -6.7927e-01,\n","          3.4211e-02],\n","        [-9.5021e-01, -6.2528e-01, -2.4466e-01, -4.7063e-02, -3.9250e-01,\n","         -3.2397e-01, -2.6383e-01,  1.1972e-01, -4.0504e-01, -1.4209e+00,\n","          4.9196e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0585, device='cuda:0')\n","Loss.item: 0.05849934741854668\n","During training one batch, the logits of the output is: tensor([[-0.9615,  0.1215, -0.7829, -0.5175, -0.7701, -0.1290, -0.3859,  1.1271,\n","         -0.3484, -1.6865,  4.1904],\n","        [-0.7606, -0.1417, -0.3328, -1.3819, -1.1690,  5.3470, -0.5715, -1.0965,\n","          0.1339, -1.1370, -0.8389],\n","        [-0.5188, -0.6340, -0.3265, -0.6934, -0.1217, -0.4906, -0.0347,  0.0275,\n","         -0.3500,  5.4756, -1.2962]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1980, device='cuda:0')\n","Loss.item: 0.19797949492931366\n","During training one batch, the logits of the output is: tensor([[-6.0004e-01, -6.7158e-01, -4.3780e-01, -7.6609e-01, -7.7263e-02,\n","         -5.4116e-01, -1.7219e-01,  6.1067e-01, -5.3540e-01,  5.2473e+00,\n","         -1.1656e+00],\n","        [-9.6092e-01, -6.3595e-01, -2.0676e-01, -2.0308e-02, -3.0419e-01,\n","         -3.9383e-01, -2.6195e-01,  1.2343e-01, -4.1657e-01, -1.4202e+00,\n","          4.9222e+00],\n","        [ 1.0417e+00, -6.1676e-01, -6.1662e-01, -8.7813e-01, -1.3140e+00,\n","          7.7131e-01,  6.0843e-01, -4.0537e-01,  2.7940e+00,  3.5301e-03,\n","         -6.2954e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0295, device='cuda:0')\n","Loss.item: 0.029542043805122375\n","During training one batch, the logits of the output is: tensor([[-0.5079,  0.0310, -1.2625, -1.0006, -0.4468, -0.8481, -0.9489,  4.8424,\n","         -0.7677, -0.4422,  0.1368],\n","        [-1.0729, -0.1220, -0.3413, -1.1615, -1.1155,  5.4550, -0.6832, -1.1594,\n","         -0.3414, -0.6331, -0.9222],\n","        [-0.9446, -0.2093, -0.5191, -1.1077, -1.1659,  5.4460, -0.6707, -0.9926,\n","         -0.3042, -0.9516, -0.6734]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0245, device='cuda:0')\n","Loss.item: 0.02448333241045475\n","During training one batch, the logits of the output is: tensor([[-9.4016e-01, -2.6739e-01, -6.0567e-01, -1.0573e+00, -1.1306e+00,\n","          5.4154e+00, -6.5881e-01, -9.6928e-01, -2.8969e-01, -9.6243e-01,\n","         -5.9831e-01],\n","        [-8.7219e-01, -2.7415e-01, -5.1948e-01, -1.1099e+00, -1.1049e+00,\n","          5.4487e+00, -6.3955e-01, -9.6982e-01, -2.5398e-01, -1.0414e+00,\n","         -6.2200e-01],\n","        [-3.3755e-01, -7.4961e-01, -1.9630e-01, -6.4351e-01, -1.9166e-01,\n","         -5.7411e-01,  1.1442e-03, -2.4777e-01, -2.8426e-01,  5.3873e+00,\n","         -1.2545e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1585, device='cuda:0')\n","Loss.item: 0.15850193798542023\n","During training one batch, the logits of the output is: tensor([[-0.4527, -0.7249, -0.2091, -0.6272, -0.0696, -0.6905, -0.0394, -0.0309,\n","         -0.4068,  5.4211, -1.1353],\n","        [ 0.7650, -0.1885, -0.6594, -0.6458, -1.4548,  0.7460,  0.7102, -0.6889,\n","          2.9742, -0.0837, -0.5353],\n","        [-1.0446, -0.2661, -0.4351, -1.1821, -1.1962,  5.5183, -0.7199, -1.1039,\n","         -0.2518, -0.7196, -0.7546]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2656, device='cuda:0')\n","Loss.item: 0.2656259834766388\n","During training one batch, the logits of the output is: tensor([[ 0.6780, -0.1070, -0.7452, -0.6979, -1.4775,  0.8834,  0.6773, -0.6675,\n","          2.9672, -0.2362, -0.4088],\n","        [ 4.4668, -1.0870, -0.4731,  0.6576, -0.9046, -1.0309, -0.2884,  0.3524,\n","          0.1456, -0.6067, -1.7157],\n","        [ 3.7520, -0.9005, -0.9127,  2.0332, -0.5904, -1.4050, -0.4545,  1.0683,\n","         -0.6640, -0.9819, -1.4212]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0757, device='cuda:0')\n","Loss.item: 0.07570820301771164\n","During training one batch, the logits of the output is: tensor([[ 4.3854, -1.0571, -0.5274,  0.8596, -0.9208, -1.0387, -0.2622,  0.3676,\n","          0.1399, -0.6449, -1.7400],\n","        [-0.6607, -0.8095,  0.1120,  0.0982,  4.7842, -1.2265, -0.6452,  0.0996,\n","         -1.0861, -0.1222,  0.1722],\n","        [-0.5641,  4.5140, -0.6646, -0.3584, -1.0827, -0.3744, -0.3101,  0.5463,\n","         -0.6906, -0.8033, -0.9482]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.3040, device='cuda:0')\n","Loss.item: 0.3040492832660675\n","During training one batch, the logits of the output is: tensor([[-1.2639,  3.0949, -0.9136, -1.2527, -1.7824,  3.1115, -0.3829,  0.2643,\n","         -0.1512, -1.3439, -1.1818],\n","        [-0.5433,  4.5552, -0.6350, -0.3845, -1.0793, -0.4279, -0.2919,  0.5117,\n","         -0.6595, -0.8018, -0.8850],\n","        [-0.6004,  4.6234, -0.7196, -0.3883, -0.9749, -0.5361, -0.2786,  0.5775,\n","         -0.6161, -0.7667, -0.8712]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0212, device='cuda:0')\n","Loss.item: 0.02119758166372776\n","During training one batch, the logits of the output is: tensor([[-1.0842,  0.2408, -0.5524, -1.1201, -1.2446,  5.4557, -0.6760, -0.8510,\n","         -0.4882, -0.8847, -0.8863],\n","        [-1.0906, -0.0841, -0.4832, -1.0580, -1.1602,  5.4833, -0.6861, -0.9982,\n","         -0.4275, -0.9023, -0.6687],\n","        [-1.0179,  0.1390, -0.7267, -1.1837, -1.3120,  5.4691, -0.6917, -0.9462,\n","         -0.2330, -0.7733, -0.9490]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0715, device='cuda:0')\n","Loss.item: 0.07146429270505905\n","During training one batch, the logits of the output is: tensor([[ 1.1389, -0.5124, -0.5325,  4.2609, -0.1539, -0.9972, -0.4122, -0.1355,\n","         -0.7940, -0.5195, -0.0518],\n","        [-0.5162,  0.1712, -1.2399, -1.0243, -0.0975, -0.9446, -0.9256,  4.8597,\n","         -0.7089, -0.1397, -0.4562],\n","        [-0.5990,  0.1860, -1.2331, -1.0751, -0.0532, -0.8995, -0.9793,  4.8430,\n","         -0.7194, -0.0701, -0.4237]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.6376, device='cuda:0')\n","Loss.item: 0.6375816464424133\n","During training one batch, the logits of the output is: tensor([[ 0.6300, -0.4780, -0.2143, -0.9743, -0.7675, -0.3834, -0.2831,  0.4238,\n","          1.1686,  0.1556,  0.8828],\n","        [-0.2334, -0.1983, -1.1000, -0.6641,  1.1756, -1.3151, -0.8641,  4.0827,\n","         -0.4651, -0.6587, -0.5413],\n","        [-0.3531, -0.7176, -0.0439, -0.6184, -0.0411, -0.5557,  0.0394, -0.2189,\n","         -0.5070,  5.2865, -1.3402]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0428, device='cuda:0')\n","Loss.item: 0.042813271284103394\n","During training one batch, the logits of the output is: tensor([[-0.5033, -0.7014, -0.2336, -0.5432, -0.0161, -0.5876, -0.0246, -0.0542,\n","         -0.6913,  5.3128, -1.1581],\n","        [-0.5278, -0.6885, -0.2408, -0.5305, -0.0612, -0.5660,  0.0054, -0.1493,\n","         -0.5854,  5.3671, -1.1030],\n","        [-0.9083,  4.6584, -0.7050, -0.3751, -0.7941, -0.5007, -0.3623,  0.8020,\n","         -0.7500, -0.7956, -0.5691]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0682, device='cuda:0')\n","Loss.item: 0.06818000972270966\n","During training one batch, the logits of the output is: tensor([[-0.6339,  4.5315, -0.6592, -0.2951, -0.9866, -0.5006, -0.2919,  0.5675,\n","         -0.7187, -0.7495, -0.8491],\n","        [-0.9675,  4.6292, -0.6988, -0.4805, -0.8288, -0.2809, -0.3941,  0.8499,\n","         -0.7624, -0.8796, -0.6000],\n","        [-0.7302,  4.5926, -0.6549, -0.3130, -0.8344, -0.5068, -0.3652,  0.7361,\n","         -0.7669, -0.8063, -0.7427]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0879, device='cuda:0')\n","Loss.item: 0.08791705965995789\n","During training one batch, the logits of the output is: tensor([[-0.7175, -0.7713,  0.1606, -0.0178,  4.8235, -1.2076, -0.6762,  0.2103,\n","         -1.1274, -0.1408,  0.0915],\n","        [-1.5417,  4.1733, -0.5769, -0.7930, -1.2247,  1.6323, -0.5976,  0.4313,\n","         -0.8940, -1.1945, -0.6193],\n","        [-0.6960, -0.7364,  0.2351,  0.2098,  4.7711, -1.2517, -0.6943,  0.0738,\n","         -1.1824, -0.1955,  0.1639]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2119, device='cuda:0')\n","Loss.item: 0.21194235980510712\n","During training one batch, the logits of the output is: tensor([[-0.6990, -0.7191,  0.1741,  0.2168,  4.7360, -1.2754, -0.7364,  0.2280,\n","         -1.2471, -0.2620,  0.1243],\n","        [ 1.0547, -0.6505, -0.3013, -0.9980, -1.1958,  0.4374,  0.4425, -0.3228,\n","          2.7962,  0.0089, -0.4823],\n","        [-0.7886,  4.4030, -0.6747, -0.3007, -0.7504, -0.2009, -0.5075,  0.8330,\n","         -0.9023, -0.8934, -0.6484]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2210, device='cuda:0')\n","Loss.item: 0.22098715603351593\n","During training one batch, the logits of the output is: tensor([[-0.4515, -0.5967, -0.5383, -0.9184, -0.1630, -0.4243, -0.2741,  1.0112,\n","         -0.4270,  5.1311, -1.5600],\n","        [-0.2944, -0.6186, -1.0754, -1.2615, -0.1882, -0.7181, -0.6719,  3.0305,\n","         -0.4455,  3.5168, -1.4440],\n","        [-0.4207, -0.6001, -0.5817, -1.0251, -0.1779, -0.4388, -0.3173,  1.2208,\n","         -0.4101,  5.0080, -1.5740]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1048, device='cuda:0')\n","Loss.item: 0.1047804057598114\n","During training one batch, the logits of the output is: tensor([[-0.3976, -0.6175, -0.6134, -1.0419, -0.1881, -0.4647, -0.3359,  1.3187,\n","         -0.3965,  4.9650, -1.5674],\n","        [-0.4675, -0.4355, -0.8120, -1.1486, -0.1768, -0.4265, -0.4897,  1.9828,\n","         -0.4979,  4.3349, -1.6327],\n","        [-0.5242, -0.4396,  4.2479, -0.3601,  0.6436, -0.4969, -0.6698, -0.8471,\n","         -0.6916, -0.1983,  0.0926]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0635, device='cuda:0')\n","Loss.item: 0.06346088647842407\n","During training one batch, the logits of the output is: tensor([[-1.1305, -0.1115, -0.4283, -1.1718, -0.9776,  5.4142, -0.7000, -0.8945,\n","         -0.4757, -0.8903, -0.7357],\n","        [-0.7381,  1.0209, -1.3755, -0.8212, -0.2753, -0.8916, -0.8737,  4.5784,\n","         -0.7452, -0.3267, -0.5806],\n","        [ 0.2342, -0.1105, -0.5095,  4.3501, -0.1726, -0.8466, -0.3697, -0.0199,\n","         -0.8389, -0.3838,  0.0173]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0487, device='cuda:0')\n","Loss.item: 0.048736777156591415\n","During training one batch, the logits of the output is: tensor([[-0.2282, -1.0075, -0.3449, -0.8428,  1.4132, -0.9792, -0.2516,  0.5111,\n","         -0.6744,  4.5639, -1.3826],\n","        [-0.9252,  0.0154,  0.1091, -1.2168, -1.2021,  5.3390, -0.6801, -1.1635,\n","         -0.3495, -1.1173, -0.8617],\n","        [-0.7800, -0.2677, -0.3193, -1.1617, -1.2158,  5.4560, -0.6289, -1.1698,\n","         -0.1159, -0.9692, -0.8250]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0215, device='cuda:0')\n","Loss.item: 0.02147677354514599\n","During training one batch, the logits of the output is: tensor([[-0.8093, -0.1525, -0.0959, -1.1932, -1.2191,  5.4054, -0.6488, -1.2086,\n","         -0.2185, -0.9990, -0.8765],\n","        [-0.9784, -0.0324, -0.4258, -1.1788, -1.1915,  5.4868, -0.6686, -0.9788,\n","         -0.3039, -0.9368, -0.8834],\n","        [-0.9484, -0.0301, -0.5769, -1.1989, -1.2035,  5.4823, -0.6730, -0.9044,\n","         -0.2858, -0.9820, -0.8209]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0350, device='cuda:0')\n","Loss.item: 0.03499606251716614\n","During training one batch, the logits of the output is: tensor([[-0.9881, -0.0378, -0.4762, -1.1849, -1.1853,  5.4926, -0.7001, -0.9199,\n","         -0.3473, -1.0239, -0.7728],\n","        [-0.6709, -0.7361,  0.2860,  0.1325,  4.7673, -1.1453, -0.6891, -0.0589,\n","         -1.1711, -0.0720,  0.1885],\n","        [-1.1035,  0.0567, -0.4557, -1.1752, -1.1392,  5.4534, -0.6571, -0.9309,\n","         -0.3875, -1.0224, -0.7821]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0747, device='cuda:0')\n","Loss.item: 0.07469477504491806\n","During training one batch, the logits of the output is: tensor([[-0.8778,  4.4485, -0.4137, -0.6260, -0.9166, -0.4402, -0.3537,  1.2971,\n","         -0.6691, -1.0245, -0.5651],\n","        [-0.8749,  4.4994, -0.5098, -0.4476, -0.8196, -0.6619, -0.3080,  1.1416,\n","         -0.6672, -0.8643, -0.5407],\n","        [-0.0643,  0.0615, -1.1171, -0.9110, -0.3440, -1.0123, -0.7957,  4.8772,\n","         -0.6462, -0.4819, -0.5213]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0881, device='cuda:0')\n","Loss.item: 0.08810728788375854\n","During training one batch, the logits of the output is: tensor([[-0.1688,  0.1787, -1.1582, -0.8996, -0.2862, -1.0213, -0.8083,  4.8605,\n","         -0.7227, -0.4938, -0.4445],\n","        [-0.6088,  0.6691, -1.2097, -0.7076, -0.0145, -1.0064, -0.8808,  4.6403,\n","         -0.8944, -0.6797, -0.3605],\n","        [ 0.5461, -0.2110, -0.7670,  3.9804,  0.0352, -1.2788, -0.3357,  0.8102,\n","         -0.8446, -0.6876, -0.1798]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0356, device='cuda:0')\n","Loss.item: 0.035586003214120865\n","During training one batch, the logits of the output is: tensor([[-0.9339, -0.0507, -0.4211, -1.1593, -1.2089,  5.4815, -0.6369, -1.0373,\n","         -0.2919, -0.9580, -0.8508],\n","        [-0.9316, -0.1151, -0.4348, -1.2280, -1.2962,  5.4702, -0.6561, -1.1268,\n","         -0.0819, -0.6564, -0.9985],\n","        [-0.8433,  4.6647, -0.5984, -0.3663, -0.7894, -0.5429, -0.3768,  0.7705,\n","         -0.7356, -0.8257, -0.5158]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2473, device='cuda:0')\n","Loss.item: 0.247253879904747\n","During training one batch, the logits of the output is: tensor([[-8.5275e-01, -4.0316e-02, -3.4655e-01, -1.2604e+00, -1.4097e+00,\n","          5.4404e+00, -6.4930e-01, -1.1359e+00, -5.2249e-03, -7.2881e-01,\n","         -1.0247e+00],\n","        [-4.6344e-01, -2.1943e-01, -1.2849e+00, -1.1900e+00, -2.2690e-01,\n","         -9.9856e-01, -9.4273e-01,  4.4087e+00, -7.5472e-01,  9.5893e-01,\n","         -4.0592e-01],\n","        [-5.3722e-01, -4.8266e-01,  2.6115e+00, -3.6116e-01,  1.6240e+00,\n","         -1.0011e+00, -8.6516e-01,  2.6941e-02, -1.0459e+00, -7.0292e-02,\n","          6.8237e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0816, device='cuda:0')\n","Loss.item: 0.08157574385404587\n","During training one batch, the logits of the output is: tensor([[ 4.6054, -1.2146, -0.4557,  0.2880, -0.4789, -1.2052, -0.6076,  0.8590,\n","         -0.4102, -0.7701, -1.4737],\n","        [ 0.3576, -0.3886, -0.2786,  4.3756,  0.3053, -0.9776, -0.4168, -0.1996,\n","         -0.9505, -0.4318,  0.1663],\n","        [ 4.5387, -1.1238, -0.4359,  0.2419, -0.5687, -0.9368, -0.5266,  0.6380,\n","         -0.2285, -0.7178, -1.6830]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1918, device='cuda:0')\n","Loss.item: 0.19179348647594452\n","During training one batch, the logits of the output is: tensor([[-0.9938,  0.1053, -0.4396, -1.1939, -1.2047,  5.4789, -0.6453, -0.9755,\n","         -0.3300, -1.0008, -0.8794],\n","        [-0.9544,  4.5223, -0.6051, -0.6726, -1.1132,  0.3596, -0.4466,  0.8244,\n","         -0.6745, -1.2019, -0.8076],\n","        [ 3.2568, -0.9955, -0.8935, -0.2871, -1.1490, -0.9793, -0.8687,  2.3212,\n","         -0.1981, -1.8581,  0.7269]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2611, device='cuda:0')\n","Loss.item: 0.26110970973968506\n","During training one batch, the logits of the output is: tensor([[ 0.1688, -0.4853,  4.5010, -0.3261,  0.0784, -0.3533, -0.4278, -1.2147,\n","         -0.1500, -0.1858, -0.5719],\n","        [-0.0616, -0.4779,  4.5177, -0.3244,  0.1697, -0.3302, -0.4578, -1.1558,\n","         -0.2665, -0.2153, -0.4164],\n","        [ 1.0571, -0.4357, -0.6527, -0.7141, -1.1261,  2.5209,  0.1216,  0.0225,\n","          1.1625, -1.2190, -1.1455]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0244, device='cuda:0')\n","Loss.item: 0.024350492283701897\n","During training one batch, the logits of the output is: tensor([[-0.9724, -0.0971, -0.4223, -1.2106, -1.2457,  5.4569, -0.6151, -1.2295,\n","         -0.0395, -0.6348, -0.9671],\n","        [-1.0526, -0.3242, -0.4280, -1.1378, -1.1104,  5.4672, -0.7127, -1.1435,\n","         -0.2482, -0.5955, -0.8263],\n","        [-0.1571, -0.7996, -0.4237, -0.6651, -0.1622, -0.6783, -0.0446,  0.1563,\n","         -0.2684,  5.4236, -1.3942]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0262, device='cuda:0')\n","Loss.item: 0.026150241494178772\n","During training one batch, the logits of the output is: tensor([[-4.5365e-03, -8.0413e-01, -4.5830e-01, -7.1292e-01, -1.6540e-01,\n","         -7.2973e-01, -3.7638e-02,  1.9494e-01, -2.2866e-01,  5.3883e+00,\n","         -1.4934e+00],\n","        [-8.0373e-01, -1.4696e-01, -4.6164e-01, -1.1697e+00, -1.2381e+00,\n","          5.3958e+00, -6.1757e-01, -1.1710e+00, -2.7664e-02, -7.5644e-01,\n","         -8.6576e-01],\n","        [-9.1173e-01, -1.7217e-01, -4.6130e-01, -1.1461e+00, -1.1183e+00,\n","          5.3965e+00, -6.5593e-01, -1.1501e+00, -1.8421e-01, -6.9851e-01,\n","         -7.9860e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0408, device='cuda:0')\n","Loss.item: 0.040809739381074905\n","During training one batch, the logits of the output is: tensor([[-0.8168, -0.0968,  0.3715, -1.3672, -1.2326,  5.1572, -0.6939, -1.1643,\n","         -0.0459, -0.9323, -1.0536],\n","        [-0.7202,  4.6206, -0.7243, -0.5072, -1.0302, -0.4563, -0.3640,  0.9382,\n","         -0.6201, -0.9854, -0.8034],\n","        [-0.8650, -0.1343, -0.3924, -1.1549, -1.2234,  5.4728, -0.6441, -1.0671,\n","         -0.2081, -0.9598, -0.8794]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0348, device='cuda:0')\n","Loss.item: 0.03481784462928772\n","During training one batch, the logits of the output is: tensor([[-0.8654, -0.0380, -0.4651, -1.1442, -1.1715,  5.4272, -0.6232, -0.9285,\n","         -0.3849, -0.9880, -0.8293],\n","        [-0.5459, -0.6866, -0.1708, -0.6253, -0.1652, -0.4412,  0.0307, -0.3055,\n","         -0.4683,  5.4196, -1.1938],\n","        [-1.1273, -0.5320, -0.0192, -0.1868, -0.2812, -0.2855, -0.3243,  0.1907,\n","         -0.5661, -1.3815,  4.8456]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0539, device='cuda:0')\n","Loss.item: 0.05390098690986633\n","During training one batch, the logits of the output is: tensor([[-1.0774, -0.5956, -0.1078, -0.1690, -0.2680, -0.1694, -0.3814,  0.2007,\n","         -0.5760, -1.4227,  4.8361],\n","        [-1.0756, -0.4914, -0.1585, -0.1834, -0.4022, -0.2033, -0.3282,  0.1910,\n","         -0.5143, -1.4302,  4.8346],\n","        [-1.0874, -0.5157, -0.0186, -0.2605, -0.3304, -0.3296, -0.3520,  0.3157,\n","         -0.5985, -1.3895,  4.8249]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.9813, device='cuda:0')\n","Loss.item: 1.9812836647033691\n","During training one batch, the logits of the output is: tensor([[ 0.3883, -0.5639, -0.0633, -0.8956, -1.0047,  0.7164, -0.0501, -0.4017,\n","          1.6230, -0.0594,  0.6420],\n","        [ 1.7212, -0.8276, -0.5169, -0.4483, -0.9921,  0.0870,  0.5076, -0.4684,\n","          2.4865,  0.5172, -0.9839],\n","        [ 0.5211, -0.9170, -0.8362, -0.1911,  2.5532, -1.2411, -0.5704,  1.1964,\n","         -0.0192,  0.7674, -0.8098]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(3.5376, device='cuda:0')\n","Loss.item: 3.5375890731811523\n","During training one batch, the logits of the output is: tensor([[ 0.1210, -0.5592,  3.7225, -1.0814, -0.2879,  0.3330, -0.3861, -1.0567,\n","          0.6422, -0.3606, -0.5103],\n","        [ 0.0409, -0.5072,  3.8683, -1.0342, -0.2524,  0.3376, -0.4205, -1.1194,\n","          0.5205, -0.3397, -0.5982],\n","        [-0.0186, -0.4377,  4.0526, -0.9518, -0.1333,  0.1376, -0.4968, -1.1096,\n","          0.3341, -0.2370, -0.7361]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0359, device='cuda:0')\n","Loss.item: 0.03590719774365425\n","During training one batch, the logits of the output is: tensor([[-9.5604e-01,  4.4822e-03, -5.3504e-01, -1.2558e+00, -1.3595e+00,\n","          5.4642e+00, -6.4989e-01, -1.0831e+00, -1.0141e-01, -7.0820e-01,\n","         -9.7798e-01],\n","        [-8.8543e-01,  2.2889e-01, -4.7421e-01, -1.2147e+00, -1.4246e+00,\n","          5.4209e+00, -6.3207e-01, -9.4287e-01, -2.8925e-01, -8.7917e-01,\n","         -9.5893e-01],\n","        [-7.9979e-01,  4.6482e+00, -6.5186e-01, -4.4401e-01, -1.0803e+00,\n","         -3.0830e-01, -2.5428e-01,  6.2732e-01, -6.2879e-01, -7.7676e-01,\n","         -7.7751e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1574, device='cuda:0')\n","Loss.item: 0.15742109715938568\n","During training one batch, the logits of the output is: tensor([[ 0.0866, -0.5952, -0.3593, -1.1299, -1.3944,  3.5346,  0.0735, -1.1905,\n","          2.2469, -0.7500, -0.3023],\n","        [-0.2664, -0.0973, -1.4732, -1.0621, -0.2362, -0.6352, -0.9244,  4.7135,\n","         -0.6268, -0.4755, -0.1945],\n","        [-0.2162, -0.0732, -1.5144, -1.1047, -0.4752, -0.4114, -0.8093,  4.6098,\n","         -0.4607, -0.7164, -0.1276]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0601, device='cuda:0')\n","Loss.item: 0.06006366014480591\n","During training one batch, the logits of the output is: tensor([[-2.4249e-01, -1.2679e-03, -1.3737e+00, -1.1437e+00, -5.4478e-01,\n","         -3.1139e-01, -9.8354e-01,  4.6791e+00, -6.8305e-01, -4.8996e-01,\n","         -3.4188e-01],\n","        [-7.2420e-01, -6.9572e-01,  2.4021e-01,  9.4779e-02,  4.7426e+00,\n","         -1.1396e+00, -6.9136e-01, -1.9180e-02, -1.2151e+00, -1.0257e-01,\n","          1.6266e-01],\n","        [-7.2760e-01, -7.1837e-01,  2.8322e-01,  7.3054e-02,  4.7470e+00,\n","         -1.0911e+00, -6.7553e-01, -7.6338e-02, -1.1748e+00, -1.2116e-01,\n","          1.9681e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.7042, device='cuda:0')\n","Loss.item: 0.7041730880737305\n","During training one batch, the logits of the output is: tensor([[-0.7065, -0.7175,  0.2981,  0.0831,  4.7466, -1.0795, -0.6679, -0.0979,\n","         -1.1838, -0.1017,  0.1619],\n","        [ 0.7275, -0.7568,  1.1371, -0.8920,  0.0819, -0.9414, -0.7169,  0.5166,\n","          0.3510,  0.1396,  0.9165],\n","        [-0.4145, -0.5182,  4.4475, -0.5248,  0.4688, -0.3525, -0.6114, -0.9806,\n","         -0.4455, -0.2393, -0.0689]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0377, device='cuda:0')\n","Loss.item: 0.037660952657461166\n","During training one batch, the logits of the output is: tensor([[-0.4060, -0.7071, -0.0952, -0.6131, -0.2151, -0.4293,  0.0538, -0.5215,\n","         -0.3249,  5.3997, -1.2171],\n","        [-0.2690, -0.8203, -0.1334, -0.5918, -0.1117, -0.5831,  0.0136, -0.4876,\n","         -0.3000,  5.3723, -1.1935],\n","        [ 0.0395,  0.1247, -1.2673, -1.0293, -0.5783, -0.8785, -0.6865,  4.7206,\n","         -0.4251, -0.4882, -0.8098]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0776, device='cuda:0')\n","Loss.item: 0.0775725245475769\n","During training one batch, the logits of the output is: tensor([[-0.4693, -0.4451,  4.4114, -0.4843,  0.5138, -0.3921, -0.6633, -0.9197,\n","         -0.5435, -0.3020, -0.0064],\n","        [-0.5651, -0.4067,  4.4002, -0.5352,  0.4919, -0.3701, -0.6561, -0.8841,\n","         -0.5214, -0.2951, -0.0077],\n","        [-0.6987, -0.7914,  0.1717,  0.0472,  4.8123, -1.1929, -0.6788,  0.1223,\n","         -1.1420, -0.1243,  0.1409]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.9058, device='cuda:0')\n","Loss.item: 1.9058359861373901\n","During training one batch, the logits of the output is: tensor([[-0.5264, -0.5539,  2.1731, -0.3335,  1.7743, -1.1041, -0.8658,  0.1864,\n","         -0.9729, -0.0890,  1.0285],\n","        [ 4.0498, -0.8820, -0.7936, -0.2217, -1.1403, -0.8724, -0.4788,  1.3367,\n","          0.0753, -1.1463, -1.0291]], device='cuda:0')\n","Validation loss: 0.2472570093369551\n","F1 Score (Weighted): 0.9400389253698999\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1ce0364c07648f6baae20c58e187faa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=798.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 2\n","\rTraining loss: 0.1899838079208214\n","During training one batch, the loss of the output is: tensor(0.0106, device='cuda:0')\n","Loss.item: 0.010575339198112488\n","During training one batch, the logits of the output is: tensor([[-0.8802, -1.0575, -0.2376, -0.5923,  6.5041, -1.0575, -0.5482,  0.3080,\n","         -1.0764, -0.3656, -0.3453],\n","        [-0.7761, -0.8334, -0.3644, -0.8552, -0.3385, -0.5625, -0.0344, -0.4794,\n","         -0.4332,  7.1737, -1.0443],\n","        [-1.4345, -0.4258,  6.0115, -1.2174, -0.2932,  1.2439, -0.8951, -1.2289,\n","         -1.1962, -0.9744, -0.4086]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0133, device='cuda:0')\n","Loss.item: 0.013289575465023518\n","During training one batch, the logits of the output is: tensor([[-1.4404, -0.4261,  6.0154, -1.2216, -0.2941,  1.2466, -0.8949, -1.2353,\n","         -1.1917, -0.9678, -0.4099],\n","        [-0.3379, -0.1994, -0.6676,  6.4729, -0.1613, -0.8580, -0.1293, -0.3541,\n","         -0.8133, -0.5412,  0.1045],\n","        [-0.4375, -0.1383, -0.7194,  6.4468, -0.2178, -0.8007, -0.1231, -0.2514,\n","         -0.8184, -0.5363,  0.0454]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0083, device='cuda:0')\n","Loss.item: 0.008250846527516842\n","During training one batch, the logits of the output is: tensor([[-0.4411, -0.1542, -0.6631,  6.4611, -0.1775, -0.7816, -0.1152, -0.4126,\n","         -0.7654, -0.5353,  0.1043],\n","        [ 6.7707, -1.3221, -0.4371, -0.4987, -0.5246, -1.2208, -0.7460, -0.0573,\n","         -0.4564, -0.7756, -1.1817],\n","        [-1.2163, -0.8893, -0.5406, -0.3516, -0.4591, -0.5668, -0.3293, -0.0244,\n","         -0.5529, -1.4187,  6.5838]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0099, device='cuda:0')\n","Loss.item: 0.009872081689536572\n","During training one batch, the logits of the output is: tensor([[-1.3811, -0.6443, -1.0300, -0.4156, -0.3992, -0.2357, -0.5233,  0.5788,\n","         -0.9362, -1.6269,  6.1798],\n","        [-0.3853, -0.0664, -0.6131,  6.4360, -0.2747, -0.7469, -0.1624, -0.1955,\n","         -0.9840, -0.6068, -0.0336],\n","        [ 6.8047, -1.3020, -0.2158, -0.2768, -0.7157, -1.1103, -0.7063, -0.4463,\n","         -0.4070, -0.7549, -1.3054]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.9808, device='cuda:0')\n","Loss.item: 1.9808253049850464\n","During training one batch, the logits of the output is: tensor([[-0.4702, -0.4528, -1.3748, -1.0004, -0.3998, -0.9749, -1.2276,  6.9076,\n","         -1.0729, -0.6261, -0.5149],\n","        [-0.8849, -1.0484, -0.1768, -0.3989,  6.5580, -0.9968, -0.4565, -0.0925,\n","         -1.0591, -0.3976, -0.3059],\n","        [-1.4573, -0.4041,  5.8527, -1.2764, -0.0564,  0.9947, -0.9755, -1.0395,\n","         -1.3405, -1.0309, -0.0516]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0174, device='cuda:0')\n","Loss.item: 0.017395274713635445\n","During training one batch, the logits of the output is: tensor([[-0.9099, -1.4611,  1.8655, -1.1134,  5.5805, -0.2879, -0.6772, -0.7166,\n","         -0.7554, -0.7215, -0.5583],\n","        [-1.0577, -0.9213, -0.5071, -0.2558, -0.6281, -0.5680, -0.2722, -0.1769,\n","         -0.4571, -1.4894,  6.6226],\n","        [-0.7449, -0.0929, -1.3470, -0.9422, -0.4451, -0.9804, -1.1170,  6.8370,\n","         -1.1811, -0.8525, -0.3991]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0530, device='cuda:0')\n","Loss.item: 0.05297181382775307\n","During training one batch, the logits of the output is: tensor([[-0.3472, -0.4033, -1.4437, -0.8994, -0.2268, -1.2303, -1.1744,  6.8138,\n","         -1.2088, -0.6000, -0.4019],\n","        [ 0.4356, -0.1917, -0.5232, -0.0790, -1.2202, -0.5556, -0.2150, -0.7866,\n","          4.2417, -0.5830,  0.1042],\n","        [-0.0113, -0.4791, -0.9574, -0.9405, -1.4960,  1.0163,  0.2323, -0.8013,\n","          5.1125, -0.5538, -0.4693]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0158, device='cuda:0')\n","Loss.item: 0.01582431234419346\n","During training one batch, the logits of the output is: tensor([[ 0.1909, -0.5254, -0.9763, -0.7280, -1.3958,  0.3131,  0.1702, -0.7421,\n","          5.2058, -0.5918, -0.2277],\n","        [-0.5231, -0.3502, -1.3127, -0.9304, -0.4380, -1.0511, -1.0771,  6.8510,\n","         -1.1744, -0.8795, -0.3727],\n","        [-1.6379, -0.7003, -0.6250, -1.2347, -0.9298,  7.0825, -0.9328, -0.9543,\n","         -0.8013, -1.1086, -0.7358]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.005047776270657778\n","During training one batch, the logits of the output is: tensor([[-0.8056, -0.3324, -1.3360, -1.0646, -0.5180, -0.7745, -1.1336,  6.8535,\n","         -1.0703, -0.7115, -0.4579],\n","        [-0.7049, -0.2357, -1.4802, -0.8826, -0.3876, -1.1384, -1.1088,  6.8179,\n","         -1.1161, -0.6343, -0.3087],\n","        [-0.3452, -0.3456, -1.4951, -0.9478, -0.3896, -1.2510, -1.0969,  6.8788,\n","         -1.0338, -0.5739, -0.4811]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0744, device='cuda:0')\n","Loss.item: 0.07440415024757385\n","During training one batch, the logits of the output is: tensor([[-1.2367, -0.6336, -0.5707, -0.6989, -0.9369, -0.3874, -0.3818,  0.6436,\n","         -0.8197, -1.6703,  6.4239],\n","        [-0.2454,  0.0922, -1.2049, -0.4846, -0.6884, -0.9270,  3.5480,  0.1050,\n","          0.2132, -0.1756, -1.0145],\n","        [-1.4991,  5.9390, -0.9532, -0.4482, -0.7546, -0.9131, -0.4417,  1.6293,\n","         -1.3137, -0.9035, -0.4402]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.0037738673854619265\n","During training one batch, the logits of the output is: tensor([[-0.2381, -0.4346, -1.4048, -1.1298, -0.3562, -1.0798, -1.2275,  6.9078,\n","         -1.1028, -0.5770, -0.6355],\n","        [-1.7600, -0.5504, -0.3846, -1.2393, -0.9165,  7.0815, -0.9816, -0.9974,\n","         -0.9169, -1.2471, -0.7281],\n","        [-1.5194, -0.6433, -0.6116, -1.2266, -1.0146,  7.1113, -0.9059, -1.0372,\n","         -0.6698, -1.4179, -0.7759]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0076, device='cuda:0')\n","Loss.item: 0.007593046873807907\n","During training one batch, the logits of the output is: tensor([[-1.7050, -0.6481, -0.5993, -1.2287, -0.9066,  7.0324, -0.9344, -0.9446,\n","         -0.8098, -1.4407, -0.5044],\n","        [-1.3383, -0.8027, -0.6155, -0.4227, -0.5398, -0.3007, -0.3777,  0.0413,\n","         -0.6475, -1.4548,  6.5283],\n","        [-0.3750, -0.1409, -0.6360,  6.4621, -0.1875, -0.8534, -0.1333, -0.1983,\n","         -0.9198, -0.5978,  0.0479]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0138, device='cuda:0')\n","Loss.item: 0.013844170607626438\n","During training one batch, the logits of the output is: tensor([[-0.8538, -0.6354,  6.2810, -0.8695, -0.3238,  0.7190, -0.7795, -1.4591,\n","         -0.9603, -0.7378, -0.7490],\n","        [-1.0455, -0.5382,  6.2691, -1.0099, -0.3223,  0.6112, -0.7867, -1.2783,\n","         -0.8525, -0.8492, -0.6697],\n","        [ 0.0636, -1.2533, -0.6745, -0.5135, -0.9854, -0.5289, -0.6714,  1.1711,\n","         -0.6991, -2.0400,  6.0165]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0069, device='cuda:0')\n","Loss.item: 0.006875252351164818\n","During training one batch, the logits of the output is: tensor([[-0.5214, -0.4356, -1.3486, -1.0614, -0.4083, -1.0296, -1.2185,  6.8898,\n","         -1.0765, -0.4493, -0.5807],\n","        [-0.4475, -0.5072, -1.3778, -1.0452, -0.2874, -1.1018, -1.2483,  6.9156,\n","         -1.0313, -0.4685, -0.5504],\n","        [-1.1171, -0.6094,  6.2624, -1.0640, -0.2301,  0.5384, -0.8388, -1.1959,\n","         -0.8572, -0.8377, -0.5945]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0176, device='cuda:0')\n","Loss.item: 0.017571067437529564\n","During training one batch, the logits of the output is: tensor([[ 0.2903, -0.7900, -0.6083, -0.7667, -1.5377,  0.1444,  0.1899, -0.6483,\n","          5.2535, -0.6394, -0.0667],\n","        [-0.4998, -0.3035, -1.4054, -1.0711, -0.5441, -0.9598, -1.1986,  6.9451,\n","         -0.9276, -0.7478, -0.5901],\n","        [-0.2294, -0.2520, -0.6192,  6.4992, -0.0759, -0.8370, -0.1880, -0.4261,\n","         -0.8710, -0.5346,  0.0469]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0075, device='cuda:0')\n","Loss.item: 0.007541988044977188\n","During training one batch, the logits of the output is: tensor([[-0.1715, -0.2840, -0.6495,  6.4911, -0.0690, -0.8602, -0.1995, -0.3736,\n","         -0.8772, -0.5632,  0.0356],\n","        [ 6.8033, -1.3500, -0.4487, -0.2076, -0.8879, -1.1565, -0.6572, -0.2280,\n","         -0.1683, -0.8178, -1.1861],\n","        [ 6.8034, -1.3832, -0.4558, -0.1850, -0.6797, -1.3300, -0.7872,  0.1953,\n","         -0.3678, -0.8089, -1.3525]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0044, device='cuda:0')\n","Loss.item: 0.004428497981280088\n","During training one batch, the logits of the output is: tensor([[-6.3571e-01, -7.4360e-01, -5.4994e-01, -9.1318e-01, -5.0751e-01,\n","         -6.8555e-01, -2.2796e-02, -4.2438e-01, -1.1981e-01,  7.1992e+00,\n","         -1.3377e+00],\n","        [-7.5321e-01, -7.5042e-01, -4.5757e-01, -7.4908e-01, -4.1029e-01,\n","         -5.8386e-01, -1.9692e-02, -1.1908e-01, -6.7463e-01,  7.1755e+00,\n","         -1.2651e+00],\n","        [-7.4444e-01, -7.3421e-01, -3.4209e-01, -7.2012e-01, -4.0172e-01,\n","         -5.4582e-01,  6.2573e-03, -3.6790e-01, -6.7130e-01,  7.2025e+00,\n","         -1.2442e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0573, device='cuda:0')\n","Loss.item: 0.05733807012438774\n","During training one batch, the logits of the output is: tensor([[-0.6198, -0.4776, -1.4655, -0.9871, -0.3443, -0.9723, -1.1896,  6.8124,\n","         -1.1660, -0.6328, -0.3261],\n","        [-0.5587,  0.0495, -1.0427, -0.1557, -0.5765, -0.7360,  3.6953, -0.6043,\n","          0.2450, -0.2016, -0.6794],\n","        [ 6.7905, -1.3445, -0.4265, -0.3151, -0.8392, -1.3274, -0.6255,  0.0275,\n","         -0.1538, -0.8629, -1.3824]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0072, device='cuda:0')\n","Loss.item: 0.007193650584667921\n","During training one batch, the logits of the output is: tensor([[-0.2585, -0.2304, -0.5057,  6.4867, -0.0269, -0.8221, -0.1786, -0.4550,\n","         -1.0296, -0.4957,  0.0244],\n","        [-0.5998, -0.2122, -1.4112, -0.9679, -0.4413, -1.0581, -1.1477,  6.9330,\n","         -1.0277, -0.6573, -0.5919],\n","        [ 6.7547, -1.2913, -0.3931, -0.2598, -0.9454, -1.3192, -0.5727, -0.0340,\n","         -0.1383, -0.8964, -1.3887]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0111, device='cuda:0')\n","Loss.item: 0.011050413362681866\n","During training one batch, the logits of the output is: tensor([[ 2.3881e-01, -3.3852e-01, -5.3749e-01,  6.4527e+00, -8.6040e-02,\n","         -8.8704e-01, -2.6069e-01, -4.8280e-01, -1.0495e+00, -5.8633e-01,\n","         -1.5613e-02],\n","        [ 5.3777e-01, -3.9851e-01, -6.1748e-01,  6.3707e+00, -6.8322e-02,\n","         -9.6703e-01, -3.4395e-01, -3.8547e-01, -1.1058e+00, -6.9119e-01,\n","         -9.4672e-03],\n","        [-8.7647e-01, -1.0304e+00, -4.5102e-03, -3.0738e-01,  6.4739e+00,\n","         -9.2790e-01, -4.7492e-01, -1.3613e-01, -1.2586e+00, -5.0600e-01,\n","         -3.0702e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0101, device='cuda:0')\n","Loss.item: 0.010079539380967617\n","During training one batch, the logits of the output is: tensor([[-0.7648, -0.8190, -0.4603, -0.7830, -0.3205, -0.6933, -0.0665, -0.0451,\n","         -0.5691,  7.2029, -1.2065],\n","        [-1.6956,  0.1234, -1.1199, -1.5123, -1.5969,  6.4537, -0.1422, -0.9931,\n","          0.1247, -1.4136, -1.0041],\n","        [-1.2912,  6.0911, -0.8632, -0.6121, -1.0368, -0.6972, -0.4805,  1.4153,\n","         -1.3086, -0.8642, -0.7844]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0065, device='cuda:0')\n","Loss.item: 0.006470379885286093\n","During training one batch, the logits of the output is: tensor([[-1.0723, -0.8137, -0.4265, -0.3937, -0.6778, -0.6649, -0.2700, -0.1085,\n","         -0.6060, -1.4598,  6.5863],\n","        [-0.8740, -1.0489, -0.2154, -0.3876,  6.5348, -0.9763, -0.5049,  0.0459,\n","         -1.1456, -0.4387, -0.3238],\n","        [-1.4767, -0.5883, -0.8554, -1.2818, -1.1152,  7.1076, -0.8725, -0.8998,\n","         -0.6245, -1.2167, -1.0293]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0556, device='cuda:0')\n","Loss.item: 0.05563843250274658\n","During training one batch, the logits of the output is: tensor([[-0.4543, -0.0684, -1.0223, -0.1274, -0.4914, -0.8499,  3.6927, -0.3985,\n","         -0.0153, -0.1125, -0.7696],\n","        [-0.5079, -0.4004, -1.3974, -1.0459, -0.4762, -0.9400, -1.2118,  6.9214,\n","         -1.0362, -0.6156, -0.5146],\n","        [-0.5600, -0.3941, -1.4043, -1.0405, -0.4970, -0.9102, -1.1909,  6.9248,\n","         -0.9756, -0.5917, -0.5739]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.004981124307960272\n","During training one batch, the logits of the output is: tensor([[-0.5107, -0.3446, -1.2676, -0.9936, -0.4370, -1.0766, -1.1224,  6.8746,\n","         -1.0908, -0.9215, -0.3403],\n","        [-0.6746, -0.0745, -1.3681, -0.9702, -0.5653, -1.0164, -1.0353,  6.8717,\n","         -1.0412, -0.9788, -0.3861],\n","        [-0.7318, -0.1442, -1.3116, -0.9930, -0.4434, -1.0123, -1.1592,  6.8295,\n","         -1.2011, -0.7639, -0.3524]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.0049866545014083385\n","During training one batch, the logits of the output is: tensor([[-0.7938, -0.0593, -1.3883, -0.9967, -0.4192, -0.9820, -1.1406,  6.8297,\n","         -1.1544, -0.7695, -0.3687],\n","        [-0.2578, -0.4863, -1.3708, -0.9135, -0.2842, -1.1250, -1.1306,  6.8972,\n","         -0.9967, -0.8657, -0.5390],\n","        [-0.1344, -0.5771, -1.3558, -0.9441, -0.3180, -1.1242, -1.1850,  6.8926,\n","         -1.0586, -0.7492, -0.5651]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0092, device='cuda:0')\n","Loss.item: 0.00924764946103096\n","During training one batch, the logits of the output is: tensor([[-0.0193, -0.6245, -1.3949, -0.9965, -0.3144, -1.1048, -1.1879,  6.9080,\n","         -0.9053, -0.8439, -0.6597],\n","        [-1.0639, -0.4968,  6.2307, -0.9402, -0.2523,  0.5899, -0.8449, -1.3347,\n","         -1.0432, -0.8125, -0.5704],\n","        [-1.0365, -0.5460,  6.2608, -0.9976, -0.2294,  0.5482, -0.8405, -1.3232,\n","         -0.9737, -0.8267, -0.5792]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0101, device='cuda:0')\n","Loss.item: 0.010112977586686611\n","During training one batch, the logits of the output is: tensor([[-0.7409, -0.8370, -0.3967, -0.8308, -0.2273, -0.6405, -0.0317, -0.4326,\n","         -0.3897,  7.2029, -1.1677],\n","        [-1.1887, -0.4987, -0.5737, -0.6567, -0.9108, -0.4452, -0.3889,  0.3146,\n","         -0.6647, -1.5874,  6.4793],\n","        [-1.0433, -0.5057, -0.7746, -0.9082, -1.1798, -0.4471, -0.4464,  1.2998,\n","         -0.6320, -1.7645,  6.1141]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0111, device='cuda:0')\n","Loss.item: 0.011084054596722126\n","During training one batch, the logits of the output is: tensor([[-1.3645,  6.0829, -0.8785, -0.5933, -0.9610, -0.9106, -0.4210,  1.6491,\n","         -1.2339, -1.0059, -0.6458],\n","        [ 6.7791, -1.4644, -0.5627, -0.1459, -0.7387, -1.2254, -0.8448,  0.4462,\n","         -0.4327, -0.9198, -1.3052],\n","        [ 6.7178, -1.3476, -0.0857, -0.6019, -0.6428, -0.9992, -0.8108, -0.1984,\n","         -0.5064, -0.7995, -1.2253]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0058, device='cuda:0')\n","Loss.item: 0.005816624965518713\n","During training one batch, the logits of the output is: tensor([[ 6.7663, -1.3487, -0.1108, -0.3840, -0.6551, -1.0229, -0.7441, -0.4254,\n","         -0.3677, -0.8466, -1.2836],\n","        [ 6.8320, -1.3184, -0.3065, -0.1748, -0.7013, -1.2337, -0.7082, -0.3213,\n","         -0.4004, -0.8498, -1.1890],\n","        [ 6.8057, -1.3546, -0.2589, -0.4234, -0.6208, -1.2053, -0.7752, -0.1358,\n","         -0.4925, -0.7673, -1.1856]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0071, device='cuda:0')\n","Loss.item: 0.007066082209348679\n","During training one batch, the logits of the output is: tensor([[-1.4645, -0.5246, -0.9814, -1.3621, -1.2467,  7.0965, -0.9393, -0.8749,\n","         -0.4535, -1.1955, -0.9472],\n","        [-0.8883, -1.0361, -0.2549, -0.5382,  6.5086, -0.9830, -0.5042,  0.2994,\n","         -1.0658, -0.5426, -0.3929],\n","        [-0.8923, -1.0214, -0.2630, -0.5576,  6.5042, -0.9553, -0.5064,  0.3085,\n","         -1.0532, -0.5901, -0.3792]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.4550, device='cuda:0')\n","Loss.item: 1.4549955129623413\n","During training one batch, the logits of the output is: tensor([[-0.2463, -0.5605, -1.3561, -1.0477, -0.4140, -1.0337, -1.1974,  6.9126,\n","         -1.0251, -0.6907, -0.5629],\n","        [-1.4855, -0.4146,  5.8856, -1.3309, -0.3775,  1.5577, -0.9381, -1.2270,\n","         -1.1923, -0.9961, -0.3807],\n","        [-0.8620, -0.9403, -0.2472, -0.5320,  6.5146, -1.0494, -0.5011,  0.1842,\n","         -1.0903, -0.5130, -0.3730]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0119, device='cuda:0')\n","Loss.item: 0.011945267207920551\n","During training one batch, the logits of the output is: tensor([[-0.5094, -0.4295, -1.3103, -0.9992, -0.4006, -0.9816, -1.1449,  6.8906,\n","         -1.0852, -0.8158, -0.4808],\n","        [-0.3945, -0.5458, -1.2986, -0.9779, -0.3911, -1.0244, -1.1151,  6.9080,\n","         -1.0068, -0.8601, -0.4737],\n","        [-1.3615,  5.9374, -0.9319, -0.6273, -1.0446, -0.6746, -0.5456,  1.8628,\n","         -1.3302, -1.0039, -0.7779]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0147, device='cuda:0')\n","Loss.item: 0.014680434949696064\n","During training one batch, the logits of the output is: tensor([[-1.6954,  5.4871, -0.9234, -0.3073, -0.4581, -0.3823, -0.3944,  1.5455,\n","         -2.0777, -0.9499, -0.2640],\n","        [-1.3649, -0.6871, -0.6693, -1.2336, -1.0904,  7.0909, -0.9515, -1.0548,\n","         -0.5540, -1.3881, -0.9010],\n","        [-1.4572, -0.6518, -0.6372, -1.2431, -1.0544,  7.1089, -0.9519, -1.0409,\n","         -0.6521, -1.3449, -0.8661]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0064, device='cuda:0')\n","Loss.item: 0.006376267876476049\n","During training one batch, the logits of the output is: tensor([[-1.4442, -0.7372, -0.6347, -1.2029, -1.0083,  7.0906, -0.9589, -1.0548,\n","         -0.6726, -1.2694, -0.8646],\n","        [-1.6352, -0.7364, -0.6121, -1.1826, -1.0019,  7.1374, -0.9536, -1.1172,\n","         -0.7619, -1.0462, -0.7773],\n","        [-0.3501, -0.8761,  6.1146, -1.1131, -0.2450,  0.0904, -0.8574, -0.9939,\n","         -0.8658, -0.0960, -0.9208]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0185, device='cuda:0')\n","Loss.item: 0.01851438730955124\n","During training one batch, the logits of the output is: tensor([[-1.0552, -0.6637,  6.1642, -1.1508, -0.0254,  0.2155, -0.9062, -1.1316,\n","         -0.8826, -0.7157, -0.3462],\n","        [-0.5435, -1.2014,  5.6023, -1.3083,  0.0344, -0.1110, -0.9396, -0.7208,\n","         -1.0492,  1.1467, -0.7478],\n","        [-1.1977, -0.5023,  6.1271, -1.2075, -0.3056,  1.0460, -0.9239, -1.2174,\n","         -1.1001, -0.9856, -0.5010]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0155, device='cuda:0')\n","Loss.item: 0.015468583442270756\n","During training one batch, the logits of the output is: tensor([[-1.1844, -0.5824,  6.1482, -1.2386, -0.2862,  1.1014, -0.9259, -1.3075,\n","         -1.0750, -0.8766, -0.5487],\n","        [-1.2248, -0.5493,  6.1101, -1.2221, -0.2669,  1.0972, -0.9423, -1.2236,\n","         -1.1410, -0.8835, -0.5234],\n","        [-1.2667, -0.5305,  6.0593, -1.2303, -0.2992,  1.1487, -0.9289, -1.1301,\n","         -1.1621, -0.9152, -0.4931]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0076, device='cuda:0')\n","Loss.item: 0.007566878106445074\n","During training one batch, the logits of the output is: tensor([[-1.1514, -0.8619, -0.4840, -0.3321, -0.6236, -0.5397, -0.2681, -0.0551,\n","         -0.5460, -1.5133,  6.6202],\n","        [-1.1787, -0.8777, -0.4803, -0.2684, -0.5281, -0.5517, -0.2509, -0.1813,\n","         -0.5303, -1.4980,  6.6188],\n","        [-1.1365, -0.9041, -0.4815, -0.2312, -0.5399, -0.6019, -0.2434, -0.2164,\n","         -0.4685, -1.4199,  6.6271]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0173, device='cuda:0')\n","Loss.item: 0.017330769449472427\n","During training one batch, the logits of the output is: tensor([[-1.1495, -0.9040, -0.4602, -0.2338, -0.5329, -0.5623, -0.2303, -0.2601,\n","         -0.4722, -1.4459,  6.6266],\n","        [-1.2902,  6.0298, -0.9541, -0.6580, -0.9626, -0.8799, -0.5766,  1.7601,\n","         -1.2417, -0.9551, -0.6599],\n","        [-1.2704,  6.0460, -0.8700, -0.6684, -1.0669, -0.8299, -0.5663,  1.7177,\n","         -1.2382, -0.9763, -0.7202]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0053, device='cuda:0')\n","Loss.item: 0.005339289549738169\n","During training one batch, the logits of the output is: tensor([[-7.4002e-01, -2.0803e-01, -1.3065e+00, -1.0072e+00, -4.5201e-01,\n","         -8.5671e-01, -1.1560e+00,  6.8576e+00, -1.1500e+00, -9.1666e-01,\n","         -4.2388e-01],\n","        [-4.9908e-01, -4.7151e-01, -1.3508e+00, -1.0387e+00, -2.7782e-01,\n","         -1.1689e+00, -1.2247e+00,  6.8839e+00, -1.0825e+00, -2.6640e-01,\n","         -5.7034e-01],\n","        [ 6.7647e+00, -1.3547e+00, -4.8995e-01, -4.4051e-01, -7.8965e-01,\n","         -1.1210e+00, -7.4469e-01, -1.1262e-03, -1.1572e-01, -8.9246e-01,\n","         -1.3252e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0032, device='cuda:0')\n","Loss.item: 0.0032418749760836363\n","During training one batch, the logits of the output is: tensor([[-1.8166, -0.4645, -0.5213, -1.2647, -1.0374,  7.0745, -1.0010, -0.9336,\n","         -0.9017, -1.1182, -0.7217],\n","        [-1.7458, -0.4426, -0.4361, -1.3148, -1.1419,  7.1154, -0.9999, -1.0064,\n","         -0.7691, -1.2136, -0.7978],\n","        [-1.7131, -0.5596, -0.6058, -1.2336, -1.0375,  7.1369, -0.9722, -1.0519,\n","         -0.7792, -1.1239, -0.7869]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.3312, device='cuda:0')\n","Loss.item: 0.3311816453933716\n","During training one batch, the logits of the output is: tensor([[-2.5006,  1.5179, -0.9686, -1.0146, -0.7810,  5.8885, -0.8171, -0.3211,\n","         -1.5153, -1.2891, -0.6120],\n","        [-2.4492,  3.9225, -0.8596, -0.9185, -0.9283,  3.5343, -0.8924,  0.7690,\n","         -1.9601, -1.3956, -0.7498],\n","        [-1.0255, -0.5974,  6.2770, -1.0279, -0.2705,  0.6196, -0.7935, -1.2761,\n","         -0.9315, -0.7401, -0.7006]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0059, device='cuda:0')\n","Loss.item: 0.0059422957710921764\n","During training one batch, the logits of the output is: tensor([[ 6.7722, -1.4347, -0.5629, -0.3251, -0.8175, -1.0896, -0.6926, -0.1536,\n","         -0.1355, -0.9530, -1.0704],\n","        [ 6.7912, -1.4382, -0.5317, -0.3546, -0.7579, -1.1140, -0.6647, -0.1091,\n","         -0.2380, -0.9081, -1.1333],\n","        [ 6.7808, -1.4185, -0.5200, -0.2800, -0.7890, -1.1297, -0.6624, -0.1916,\n","         -0.2088, -0.9126, -1.1469]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0101, device='cuda:0')\n","Loss.item: 0.01010004710406065\n","During training one batch, the logits of the output is: tensor([[ 6.7616, -1.3570, -0.4736, -0.2640, -0.7410, -1.1494, -0.6094, -0.4728,\n","         -0.1055, -0.7974, -1.1711],\n","        [ 6.8251, -1.3448, -0.4318, -0.2874, -0.7395, -1.1822, -0.7054, -0.1860,\n","         -0.2944, -0.7611, -1.2798],\n","        [-1.2955,  6.0674, -0.7273, -0.4837, -0.9047, -0.9130, -0.5038,  1.3963,\n","         -1.2532, -0.8458, -0.6339]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0112, device='cuda:0')\n","Loss.item: 0.011183805763721466\n","During training one batch, the logits of the output is: tensor([[-1.2962,  6.0707, -0.6844, -0.4879, -0.9136, -0.8854, -0.5089,  1.3665,\n","         -1.2631, -0.8477, -0.6710],\n","        [ 6.8265, -1.3631, -0.4342, -0.2466, -0.7214, -1.2193, -0.7393, -0.1075,\n","         -0.3072, -0.7705, -1.2883],\n","        [-0.6767, -1.0705, -0.0401, -0.5649,  6.4478, -0.9426, -0.5815,  0.1270,\n","         -1.1337, -0.6101, -0.2683]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0074, device='cuda:0')\n","Loss.item: 0.007387455552816391\n","During training one batch, the logits of the output is: tensor([[-0.8220, -1.0061, -0.0700, -0.4590,  6.5293, -0.8911, -0.5015, -0.1932,\n","         -1.1011, -0.5025, -0.2548],\n","        [-1.1379, -0.8596, -0.4719, -0.2483, -0.4064, -0.8002, -0.1765, -0.2835,\n","         -0.1477, -1.6339,  6.4773],\n","        [-0.7760, -0.8358, -0.4632, -0.9050, -0.1474, -0.6879, -0.0585, -0.2621,\n","         -0.3697,  7.1944, -1.1652]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0044, device='cuda:0')\n","Loss.item: 0.004387156572192907\n","During training one batch, the logits of the output is: tensor([[-0.8342, -0.8144, -0.3191, -0.8137, -0.1731, -0.6983, -0.0266, -0.4005,\n","         -0.4998,  7.2052, -1.0524],\n","        [-0.7628, -0.7538, -0.3571, -0.8454, -0.3287, -0.5830, -0.0444, -0.4539,\n","         -0.3906,  7.2199, -1.2644],\n","        [-0.7991, -0.8283, -0.3056, -0.7959, -0.1882, -0.6922, -0.0260, -0.3764,\n","         -0.5573,  7.2061, -1.0943]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0075, device='cuda:0')\n","Loss.item: 0.007491440046578646\n","During training one batch, the logits of the output is: tensor([[-0.8058, -0.8145, -0.3296, -0.7755, -0.1964, -0.7100, -0.0227, -0.3662,\n","         -0.5510,  7.2028, -1.0730],\n","        [-0.3485, -0.1946, -0.6010,  6.4877, -0.1527, -0.7741, -0.1222, -0.2854,\n","         -0.9790, -0.5481,  0.0261],\n","        [-1.1014, -0.9084, -0.5259, -0.2880, -0.6452, -0.5212, -0.2390, -0.2508,\n","         -0.4290, -1.4471,  6.6310]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0089, device='cuda:0')\n","Loss.item: 0.008881151676177979\n","During training one batch, the logits of the output is: tensor([[-1.5174, -0.1712, -1.0675, -0.6175, -0.5932, -0.6212, -0.2084,  1.1094,\n","         -0.7574, -1.7638,  5.9729],\n","        [-1.6340, -0.5289, -0.7660, -1.3113, -0.9570,  7.0730, -0.9152, -0.9548,\n","         -0.5641, -1.4640, -0.6918],\n","        [-0.7772, -0.6960, -0.3972, -0.7176, -0.3589, -0.6304, -0.0139, -0.3317,\n","         -0.6451,  7.2190, -1.2020]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0161, device='cuda:0')\n","Loss.item: 0.0160666611045599\n","During training one batch, the logits of the output is: tensor([[-0.7870, -0.7839, -0.5568, -0.8653, -0.3998, -0.6056, -0.1336,  0.3568,\n","         -0.6874,  7.0663, -1.2553],\n","        [-1.1060, -0.9202, -0.4777, -0.2538, -0.5554, -0.6001, -0.2422, -0.2597,\n","         -0.4331, -1.4493,  6.6279],\n","        [ 0.0479, -0.7020, -0.8467, -0.5711, -1.1224, -0.0119,  0.3553, -0.8355,\n","          5.2887, -0.6367, -0.1054]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0037213123869150877\n","During training one batch, the logits of the output is: tensor([[-0.8150, -0.2847, -1.4471, -1.1059, -0.5258, -0.8693, -1.1690,  6.8947,\n","         -1.0357, -0.6243, -0.3247],\n","        [-1.5813, -0.6521, -0.5262, -1.2029, -0.9934,  7.1114, -0.9301, -1.1554,\n","         -0.8098, -1.0720, -0.8519],\n","        [-1.5373, -0.7445, -0.7658, -1.1696, -0.9958,  7.0911, -0.9412, -1.0173,\n","         -0.6576, -1.2822, -0.6790]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.0038383726496249437\n","During training one batch, the logits of the output is: tensor([[-1.5038, -0.7962, -0.8519, -1.1354, -0.9650,  7.0697, -0.9112, -1.0156,\n","         -0.6221, -1.2744, -0.6502],\n","        [-1.5878, -0.8283, -0.7052, -1.1160, -0.7818,  7.0117, -0.9371, -0.8960,\n","         -0.8218, -1.3236, -0.4917],\n","        [-0.6003, -0.8098, -0.3891, -0.6941, -0.3752, -0.8266, -0.0655, -0.4221,\n","         -0.4203,  7.1555, -1.1388]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0158, device='cuda:0')\n","Loss.item: 0.015781283378601074\n","During training one batch, the logits of the output is: tensor([[-0.6988, -0.7844, -0.3896, -0.6951, -0.2945, -0.8693, -0.0759, -0.2363,\n","         -0.5538,  7.1726, -1.0845],\n","        [ 0.1484, -0.4921, -0.9457, -0.7156, -1.3773,  0.3208,  0.3407, -0.7512,\n","          5.2093, -0.5977, -0.3492],\n","        [-1.6909, -0.7127, -0.6927, -1.2154, -0.9828,  7.1305, -0.9873, -1.0431,\n","         -0.7434, -1.1253, -0.6855]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1030, device='cuda:0')\n","Loss.item: 0.10301998257637024\n","During training one batch, the logits of the output is: tensor([[ 0.1055, -0.3977, -1.1099, -0.7083, -1.3270,  0.2285,  0.2953, -0.6574,\n","          5.2330, -0.7886, -0.1531],\n","        [ 6.7643, -1.3576, -0.4985, -0.2373, -0.8019, -1.2464, -0.5730, -0.2240,\n","         -0.0258, -0.7612, -1.4196],\n","        [ 4.6346, -1.2207, -1.2461,  3.1995, -0.4330, -1.7302, -0.8054,  1.3463,\n","         -0.9924, -1.4690, -0.9881]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0142, device='cuda:0')\n","Loss.item: 0.014242880046367645\n","During training one batch, the logits of the output is: tensor([[ 6.7471e+00, -1.3813e+00, -5.3763e-01, -1.5929e-01, -7.8626e-01,\n","         -1.2626e+00, -5.5503e-01, -1.8937e-01, -5.9756e-03, -7.8465e-01,\n","         -1.4588e+00],\n","        [-8.8217e-01, -1.0577e+00, -1.9337e-01, -3.5238e-01,  6.5513e+00,\n","         -9.9432e-01, -4.1529e-01, -1.6401e-01, -1.0312e+00, -4.1702e-01,\n","         -2.6869e-01],\n","        [-1.2535e+00,  5.9033e+00, -9.9944e-01, -6.3432e-01, -1.1084e+00,\n","         -7.1357e-01, -5.8868e-01,  1.9048e+00, -1.2989e+00, -1.0387e+00,\n","         -8.4945e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0807, device='cuda:0')\n","Loss.item: 0.08067161589860916\n","During training one batch, the logits of the output is: tensor([[-2.3812,  2.8710, -1.6154, -1.5878, -1.8353,  4.6196, -0.2674,  0.6776,\n","         -0.1785, -1.8950, -1.1386],\n","        [-1.2351,  6.0071, -0.9120, -0.6350, -1.1158, -0.7682, -0.5643,  1.6774,\n","         -1.2765, -0.9885, -0.7421],\n","        [-1.2667,  6.0463, -0.9732, -0.6323, -0.9821, -0.8804, -0.5315,  1.6406,\n","         -1.2565, -0.9321, -0.6940]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.003122969763353467\n","During training one batch, the logits of the output is: tensor([[-1.4906, -0.4103, -0.8119, -1.1798, -1.2022,  7.1220, -0.8939, -0.9687,\n","         -0.7265, -1.2567, -0.9564],\n","        [-1.5098, -0.6552, -0.7410, -1.1133, -1.0956,  7.1451, -0.8907, -1.1275,\n","         -0.6922, -1.2835, -0.7377],\n","        [-1.6905, -0.5570, -0.9198, -1.1897, -1.0041,  7.0978, -0.9535, -0.9271,\n","         -0.7794, -1.1515, -0.7234]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0070, device='cuda:0')\n","Loss.item: 0.007022053003311157\n","During training one batch, the logits of the output is: tensor([[ 0.1935, -0.3544, -0.6954,  6.4369, -0.1493, -0.9633, -0.1741, -0.4571,\n","         -0.8005, -0.5645,  0.0093],\n","        [-0.6775, -0.3638, -1.4261, -1.0734, -0.3613, -0.9491, -1.2190,  6.8885,\n","         -1.0363, -0.4341, -0.5843],\n","        [-0.7123, -0.3712, -1.4583, -1.0720, -0.3286, -0.9394, -1.2356,  6.8834,\n","         -1.0333, -0.3814, -0.5733]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.3141, device='cuda:0')\n","Loss.item: 0.3140774369239807\n","During training one batch, the logits of the output is: tensor([[ 0.3194, -0.7226, -0.0291, -0.6920,  1.1948, -1.0072, -1.1901,  1.4674,\n","         -0.8353, -0.6073,  2.1086],\n","        [-1.2791,  0.5142, -1.3782, -0.8161, -0.3613, -0.8671, -1.2625,  6.4797,\n","         -1.2469, -1.0827, -0.1134],\n","        [-0.5684, -0.7586, -0.3758, -0.6979, -0.2644, -0.7763, -0.0181, -0.3430,\n","         -0.6441,  7.1785, -1.2868]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0127, device='cuda:0')\n","Loss.item: 0.012729811482131481\n","During training one batch, the logits of the output is: tensor([[-6.8870e-01, -7.3172e-01, -4.7355e-01, -7.1800e-01, -3.5198e-01,\n","         -6.9185e-01, -4.0615e-02, -1.0017e-01, -7.3261e-01,  7.1546e+00,\n","         -1.2543e+00],\n","        [-7.1426e-01, -7.1757e-01, -4.1874e-01, -6.6358e-01, -3.5423e-01,\n","         -6.8444e-01,  2.6950e-04, -3.6911e-01, -6.7690e-01,  7.1877e+00,\n","         -1.1606e+00],\n","        [-1.3464e+00,  5.8935e+00, -1.0119e+00, -5.9902e-01, -8.7541e-01,\n","         -8.2879e-01, -5.9616e-01,  1.9403e+00, -1.3705e+00, -9.3737e-01,\n","         -6.1516e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0296, device='cuda:0')\n","Loss.item: 0.0296038631349802\n","During training one batch, the logits of the output is: tensor([[-1.2206,  5.8166, -0.9510, -0.5254, -1.0257, -0.8354, -0.4943,  1.8736,\n","         -1.4130, -0.9757, -0.7605],\n","        [-1.4047,  5.9540, -0.9805, -0.6301, -0.8842, -0.5280, -0.6177,  1.7268,\n","         -1.3826, -1.0045, -0.7084],\n","        [-1.3368,  5.8062, -0.9794, -0.6261, -0.9023, -0.7694, -0.6041,  2.0770,\n","         -1.3920, -1.0430, -0.6982]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0148, device='cuda:0')\n","Loss.item: 0.014773648232221603\n","During training one batch, the logits of the output is: tensor([[-0.9333, -0.9794, -0.2165, -0.4853,  6.5395, -0.9658, -0.4868,  0.0501,\n","         -1.0922, -0.4408, -0.3607],\n","        [-1.6607,  5.8928, -0.8160, -0.7107, -0.9697, -0.3178, -0.6038,  1.8070,\n","         -1.4819, -1.0357, -0.5957],\n","        [-0.8952, -0.9830, -0.1271, -0.2711,  6.5445, -1.0170, -0.4646, -0.1783,\n","         -1.1636, -0.4363, -0.2415]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0300, device='cuda:0')\n","Loss.item: 0.02996698021888733\n","During training one batch, the logits of the output is: tensor([[-0.9022, -0.9758, -0.2134, -0.2453,  6.5052, -1.0277, -0.4945,  0.0491,\n","         -1.2168, -0.5525, -0.2743],\n","        [ 0.1603, -0.7575, -0.7109, -0.8072, -1.3067, -0.0669,  0.2181, -0.5779,\n","          5.3466, -0.6202, -0.1556],\n","        [-1.3912,  5.5312, -0.9483, -0.5647, -0.7220, -0.5176, -0.7035,  2.1488,\n","         -1.6683, -1.1060, -0.5282]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0077, device='cuda:0')\n","Loss.item: 0.0076628620736300945\n","During training one batch, the logits of the output is: tensor([[-0.7584, -0.7146, -0.3559, -0.8356, -0.4016, -0.4859, -0.0952, -0.1250,\n","         -0.6480,  7.1961, -1.3624],\n","        [-0.6929, -0.8427, -0.8921, -1.1917, -0.4262, -0.5637, -0.4154,  1.6774,\n","         -0.5584,  6.5394, -1.6638],\n","        [-0.7162, -0.7557, -0.4351, -0.9205, -0.3840, -0.5219, -0.1333,  0.1351,\n","         -0.5878,  7.1632, -1.4501]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0085, device='cuda:0')\n","Loss.item: 0.008457263000309467\n","During training one batch, the logits of the output is: tensor([[-0.7116, -0.7655, -0.4565, -0.9327, -0.3869, -0.5305, -0.1439,  0.1905,\n","         -0.5764,  7.1535, -1.4544],\n","        [-0.7448, -0.7188, -0.6212, -1.0725, -0.4511, -0.3652, -0.2249,  0.6842,\n","         -0.5985,  6.9179, -1.6506],\n","        [-1.2551, -0.4971,  6.1582, -1.0904, -0.2380,  0.9505, -0.8733, -1.3012,\n","         -1.1581, -0.8214, -0.4953]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0065, device='cuda:0')\n","Loss.item: 0.00650286627933383\n","During training one batch, the logits of the output is: tensor([[-1.6340e+00, -5.6581e-01, -7.2130e-01, -1.2478e+00, -8.9333e-01,\n","          7.0362e+00, -1.0164e+00, -8.2351e-01, -8.4475e-01, -1.2070e+00,\n","         -7.8089e-01],\n","        [-6.8214e-01, -3.2693e-01, -1.5234e+00, -1.0382e+00, -2.0425e-01,\n","         -1.1145e+00, -1.2503e+00,  6.8657e+00, -9.9547e-01, -1.2735e-01,\n","         -7.0270e-01],\n","        [-4.4223e-01, -1.1674e-01, -6.2359e-01,  6.4644e+00, -1.7038e-01,\n","         -7.8599e-01, -1.0646e-01, -3.5146e-01, -8.9326e-01, -4.5134e-01,\n","          6.6145e-03]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.003788604633882642\n","During training one batch, the logits of the output is: tensor([[-0.7645, -0.7572, -0.3966, -0.7196, -0.0592, -0.7689, -0.0562, -0.1679,\n","         -0.7464,  7.1401, -1.1736],\n","        [-1.6569, -0.4876, -0.2244, -1.2922, -1.0421,  7.0607, -0.9673, -1.1019,\n","         -0.8315, -1.4474, -0.8286],\n","        [-1.3760, -0.7386, -0.6528, -1.2291, -1.0633,  7.1319, -0.9396, -1.1587,\n","         -0.5588, -1.3118, -0.8201]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0032, device='cuda:0')\n","Loss.item: 0.003212491748854518\n","During training one batch, the logits of the output is: tensor([[-1.4104, -0.6133, -0.4272, -1.2724, -1.1152,  7.1142, -0.9557, -1.2251,\n","         -0.6263, -1.3368, -0.8916],\n","        [-1.6244, -0.5292, -0.6509, -1.2196, -1.0057,  7.1061, -0.9721, -0.9559,\n","         -0.7860, -1.2990, -0.8141],\n","        [-1.6144, -0.5896, -0.7854, -1.2455, -0.9805,  7.0876, -0.9702, -0.8516,\n","         -0.7581, -1.3683, -0.7053]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0051, device='cuda:0')\n","Loss.item: 0.00513973506167531\n","During training one batch, the logits of the output is: tensor([[-1.6323, -0.5590, -0.6932, -1.2415, -0.9831,  7.0616, -1.0163, -0.8515,\n","         -0.7989, -1.3831, -0.7020],\n","        [-0.8719, -0.9612, -0.1069, -0.3044,  6.5451, -0.9592, -0.4599, -0.3273,\n","         -1.1295, -0.3379, -0.2305],\n","        [-1.7636, -0.4558, -0.6634, -1.1978, -0.9364,  7.0446, -0.9526, -0.8652,\n","         -0.8782, -1.2716, -0.7390]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0155, device='cuda:0')\n","Loss.item: 0.015548222698271275\n","During training one batch, the logits of the output is: tensor([[-1.4422,  6.0128, -0.6629, -0.5049, -0.7943, -0.7351, -0.4849,  1.4363,\n","         -1.3966, -0.9068, -0.5326],\n","        [-1.4331,  6.0197, -0.7747, -0.5410, -0.8821, -0.8916, -0.4282,  1.6171,\n","         -1.2890, -0.9444, -0.6075],\n","        [-0.6111, -0.3122, -1.2918, -1.0428, -0.4779, -0.9294, -1.1429,  6.8755,\n","         -1.1210, -0.7538, -0.5033]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0071, device='cuda:0')\n","Loss.item: 0.007059615105390549\n","During training one batch, the logits of the output is: tensor([[-0.6476, -0.2695, -1.3612, -0.9906, -0.3859, -1.0007, -1.1389,  6.8726,\n","         -1.1537, -0.7157, -0.4687],\n","        [-0.8929, -0.0991, -1.3669, -0.9579, -0.3605, -0.9062, -1.1863,  6.8335,\n","         -1.1438, -0.7666, -0.4489],\n","        [-0.2706, -0.2220, -0.7081,  6.4430, -0.0829, -0.8441, -0.0815, -0.1553,\n","         -0.9270, -0.6206, -0.0613]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0105, device='cuda:0')\n","Loss.item: 0.010471534915268421\n","During training one batch, the logits of the output is: tensor([[-1.5622, -0.4934, -0.6697, -1.2398, -1.0749,  7.1229, -0.9110, -1.0188,\n","         -0.7137, -1.3152, -0.8806],\n","        [-1.5040, -0.6123, -0.7087, -1.2754, -1.1385,  7.1462, -0.9184, -1.1133,\n","         -0.6203, -1.1200, -0.9314],\n","        [-1.3083,  5.9516, -0.9177, -0.5979, -0.8869, -0.7593, -0.6565,  1.7974,\n","         -1.3475, -0.9573, -0.6286]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1170, device='cuda:0')\n","Loss.item: 0.11704657226800919\n","During training one batch, the logits of the output is: tensor([[-1.6083, -0.5479, -0.6259, -1.2251, -1.0938,  7.1427, -0.9831, -1.0574,\n","         -0.7642, -1.1217, -0.8552],\n","        [-0.7678, -0.3524, -1.5455, -1.2213, -0.4176, -0.9807, -1.2060,  6.8483,\n","         -0.8865, -0.1185, -0.6330],\n","        [-1.2992, -0.3326,  3.5675, -0.9533,  2.1580, -0.5170, -1.1662,  0.3188,\n","         -2.3172, -0.3920,  0.3421]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0078, device='cuda:0')\n","Loss.item: 0.007792006712406874\n","During training one batch, the logits of the output is: tensor([[ 6.7636, -1.4328, -0.4746, -0.3357, -0.6811, -1.2585, -0.8249,  0.2483,\n","         -0.2886, -0.9978, -1.2877],\n","        [-0.3540, -0.2604, -0.5244,  6.4870,  0.0261, -0.8019, -0.1439, -0.5814,\n","         -0.8359, -0.4550,  0.0492],\n","        [ 6.7499, -1.3586, -0.5381, -0.3551, -0.7647, -1.0718, -0.7979,  0.1986,\n","         -0.2390, -1.0003, -1.3725]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0174, device='cuda:0')\n","Loss.item: 0.01742452196776867\n","During training one batch, the logits of the output is: tensor([[-1.6162, -0.3858, -0.6744, -1.2570, -1.0638,  7.1076, -0.9444, -0.9629,\n","         -0.6988, -1.3637, -0.8913],\n","        [-1.4443,  5.8819, -0.7971, -0.6717, -0.8696, -0.6897, -0.4723,  1.9378,\n","         -1.3962, -1.1303, -0.6625],\n","        [ 6.2501, -1.4344, -0.7265, -0.5776, -0.8221, -1.3957, -1.1228,  1.8864,\n","         -0.7375, -1.4867, -0.5565]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0100, device='cuda:0')\n","Loss.item: 0.010022486560046673\n","During training one batch, the logits of the output is: tensor([[-0.7826, -0.6822,  6.3750, -0.8424, -0.2864,  0.4481, -0.7896, -1.4553,\n","         -0.8236, -0.7731, -0.7244],\n","        [-0.9668, -0.5941,  6.3036, -0.8262, -0.2627,  0.5295, -0.7975, -1.3989,\n","         -0.9907, -0.7527, -0.6217],\n","        [-1.2686, -0.8169, -0.8715, -1.7268, -0.6207,  6.1746, -0.6275, -0.0518,\n","         -0.4431, -1.4590, -0.9883]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.003602951765060425\n","During training one batch, the logits of the output is: tensor([[-1.5558, -0.5861, -0.6896, -1.2537, -1.1082,  7.1402, -0.9127, -1.2382,\n","         -0.5441, -1.0889, -0.8881],\n","        [-1.5847, -0.7376, -0.6753, -1.1938, -1.0295,  7.1390, -0.9501, -1.1132,\n","         -0.6994, -1.0687, -0.8239],\n","        [-0.4817, -0.8452, -0.5896, -0.7681, -0.3685, -0.8690, -0.0973,  0.1034,\n","         -0.4944,  7.1664, -1.3409]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0037177279591560364\n","During training one batch, the logits of the output is: tensor([[-0.3996, -0.8724, -0.5986, -0.8132, -0.3436, -0.9218, -0.0698,  0.0555,\n","         -0.4221,  7.1812, -1.3566],\n","        [-1.3707, -0.5816, -0.7651, -1.2812, -1.1068,  7.0958, -0.9334, -1.1621,\n","         -0.4608, -1.1158, -0.9360],\n","        [-1.4935, -0.6535, -0.7289, -1.2485, -0.9727,  7.0939, -0.9774, -1.1324,\n","         -0.5728, -1.0465, -0.8271]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0112, device='cuda:0')\n","Loss.item: 0.011203681118786335\n","During training one batch, the logits of the output is: tensor([[-1.4124, -0.5370, -0.1519, -1.5952, -1.4452,  6.8573, -0.9787, -1.1571,\n","          0.0624, -1.4547, -1.0077],\n","        [-1.2734,  5.9656, -0.9984, -0.7002, -1.1283, -0.6317, -0.6300,  1.8879,\n","         -1.2422, -1.1088, -0.8388],\n","        [-1.4620, -0.5256, -0.6968, -1.2410, -1.1127,  7.1296, -0.9325, -1.0527,\n","         -0.6028, -1.3615, -0.9136]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0052, device='cuda:0')\n","Loss.item: 0.005228051450103521\n","During training one batch, the logits of the output is: tensor([[-1.4475, -0.5615, -0.6679, -1.2289, -1.0144,  7.0777, -0.9481, -0.9393,\n","         -0.7752, -1.3194, -0.8302],\n","        [-0.7282, -0.7347, -0.3363, -0.6863, -0.4026, -0.5970,  0.0073, -0.5224,\n","         -0.6458,  7.2064, -1.1799],\n","        [-1.3647, -0.7702, -0.4168, -0.5121, -0.6837, -0.3827, -0.3430,  0.0482,\n","         -0.6193, -1.4126,  6.5569]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0085, device='cuda:0')\n","Loss.item: 0.008497882634401321\n","During training one batch, the logits of the output is: tensor([[-1.2942, -0.8287, -0.4623, -0.4899, -0.6296, -0.3353, -0.3872,  0.0446,\n","         -0.6089, -1.4784,  6.5583],\n","        [-1.2654, -0.7469, -0.5382, -0.5137, -0.7679, -0.3561, -0.3460,  0.0598,\n","         -0.5320, -1.4889,  6.5603],\n","        [-1.3055, -0.7888, -0.4857, -0.6882, -0.8017, -0.4118, -0.4466,  0.5662,\n","         -0.7065, -1.4550,  6.4643]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0306, device='cuda:0')\n","Loss.item: 0.03061768412590027\n","During training one batch, the logits of the output is: tensor([[-1.8058, -0.6736,  0.1942, -1.3262, -0.7452,  6.6822, -1.0495, -1.2438,\n","         -0.9340, -1.1187, -0.0985],\n","        [ 0.2410, -0.8078, -0.7383, -0.7849, -1.1996,  0.1824,  0.1301, -0.5422,\n","          5.1855, -0.7049, -0.3554],\n","        [ 0.2965, -0.7898, -1.0504, -0.9001, -0.5997, -0.2500, -0.0649, -0.1649,\n","          4.9588, -0.7273, -0.3671]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0424, device='cuda:0')\n","Loss.item: 0.04241938516497612\n","During training one batch, the logits of the output is: tensor([[ 0.0239, -0.7247, -0.6186, -0.9209, -1.2838,  0.1496,  0.2435, -0.5896,\n","          5.2138, -0.4548, -0.4427],\n","        [ 0.0119, -0.7552, -0.6134, -0.9441, -1.2715,  0.1921,  0.2334, -0.6189,\n","          5.2146, -0.4092, -0.4370],\n","        [-0.0147, -0.7559, -0.3932, -1.0345, -0.9407, -0.1138,  0.2247, -0.4350,\n","          4.8725, -0.0667, -0.8256]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0082, device='cuda:0')\n","Loss.item: 0.008198104798793793\n","During training one batch, the logits of the output is: tensor([[-1.6299, -0.5356, -0.7802, -1.2462, -1.0871,  7.1357, -0.9065, -1.0710,\n","         -0.7166, -1.1200, -0.8407],\n","        [-1.2264, -0.3752, -0.7887, -1.3496, -1.4740,  7.0396, -0.8689, -0.9862,\n","         -0.3562, -1.4022, -1.0736],\n","        [-1.3248,  6.1175, -0.9318, -0.5920, -1.1713, -0.7228, -0.3455,  1.4782,\n","         -1.1835, -0.9029, -0.8219]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.0049655623733997345\n","During training one batch, the logits of the output is: tensor([[-1.4000, -0.5960, -0.7280, -1.3030, -1.0628,  7.0471, -0.7855, -1.1458,\n","         -0.5757, -1.1696, -0.8770],\n","        [-0.2058, -0.6063, -1.5342, -1.0420,  0.1359, -1.1358, -1.2874,  6.7738,\n","         -1.1230, -0.6805, -0.5561],\n","        [-0.3444, -0.6069, -1.5798, -1.0626, -0.0390, -0.9851, -1.2272,  6.7683,\n","         -1.0886, -0.8786, -0.2727]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0074, device='cuda:0')\n","Loss.item: 0.007354550063610077\n","During training one batch, the logits of the output is: tensor([[-0.1992, -0.5360, -1.3902, -1.0544, -0.2881, -1.1008, -1.2610,  6.8610,\n","         -1.1773, -0.6454, -0.4985],\n","        [-0.9058, -0.9351, -0.1298, -0.3498,  6.5366, -0.9427, -0.4554, -0.2650,\n","         -1.1415, -0.3693, -0.2724],\n","        [-0.9255, -0.9382, -0.0979, -0.3813,  6.5463, -0.8877, -0.4443, -0.3337,\n","         -1.0766, -0.4048, -0.2673]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2527, device='cuda:0')\n","Loss.item: 0.2526538372039795\n","During training one batch, the logits of the output is: tensor([[-0.9112, -0.9331, -0.0987, -0.3576,  6.5296, -0.8511, -0.4450, -0.3689,\n","         -1.1035, -0.3897, -0.2952],\n","        [ 2.6380, -1.4555, -0.2933, -0.1531,  1.3727, -1.9492, -1.2596,  1.7823,\n","         -1.7711,  0.5587,  0.1255],\n","        [-1.2151, -0.6218,  6.1662, -1.1014, -0.1645,  0.8658, -0.8955, -1.3087,\n","         -1.0836, -0.8190, -0.4540]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0046, device='cuda:0')\n","Loss.item: 0.004602505825459957\n","During training one batch, the logits of the output is: tensor([[-6.2289e-01, -7.4451e-01, -2.8724e-01, -6.5897e-01, -3.9706e-01,\n","         -6.7118e-01, -5.5697e-03, -6.9104e-01, -5.2416e-01,  7.1789e+00,\n","         -1.1566e+00],\n","        [-4.6731e-01, -8.3706e-01, -3.1174e-01, -6.3718e-01, -3.1738e-01,\n","         -7.9516e-01, -7.0431e-02, -6.0117e-01, -5.6104e-01,  7.1128e+00,\n","         -1.1597e+00],\n","        [-4.6796e-01, -3.9232e-01, -1.3927e+00, -1.1395e+00, -4.6745e-01,\n","         -9.5565e-01, -1.0973e+00,  6.9116e+00, -1.0162e+00, -6.9428e-01,\n","         -6.3250e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0128, device='cuda:0')\n","Loss.item: 0.012802938930690289\n","During training one batch, the logits of the output is: tensor([[-1.2598, -0.4993,  6.0972, -1.1243, -0.2339,  0.9860, -0.9313, -1.1971,\n","         -1.1766, -0.9482, -0.4144],\n","        [-1.3514, -0.4299,  6.1013, -1.1685, -0.2925,  0.9748, -0.9180, -1.1455,\n","         -1.1177, -0.9261, -0.4640],\n","        [-0.8913, -1.0453, -0.2035, -0.4219,  6.5443, -0.9919, -0.4689, -0.0361,\n","         -1.0958, -0.3938, -0.3150]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(2.0854, device='cuda:0')\n","Loss.item: 2.0853655338287354\n","During training one batch, the logits of the output is: tensor([[-0.8993, -0.4983,  1.3787, -1.0220,  3.2933, -0.9612, -1.2049,  1.1542,\n","         -2.0829, -0.3201,  0.7500],\n","        [ 6.7752, -1.4042, -0.6265, -0.2689, -0.7718, -1.1985, -0.7500,  0.2765,\n","         -0.1541, -1.0277, -1.2856]], device='cuda:0')\n","Validation loss: 0.0870018239797543\n","F1 Score (Weighted): 0.9850506701536448\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66b324d36ddc444ba97cca59938d9d49","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=798.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 3\n","\rTraining loss: 0.10497941752664633\n","During training one batch, the loss of the output is: tensor(0.0039, device='cuda:0')\n","Loss.item: 0.0038989621680229902\n","During training one batch, the logits of the output is: tensor([[-0.9326, -1.0005, -0.2449, -0.6353,  7.1609, -1.0428, -0.5020, -0.1358,\n","         -0.9132, -0.3481, -0.5821],\n","        [-0.7205, -0.7060, -0.3395, -0.8514, -0.5987, -0.5420, -0.0350, -0.7630,\n","         -0.5424,  7.8038, -1.1204],\n","        [-1.1393, -0.6189,  6.9851, -1.0930, -0.3011,  0.5451, -0.8737, -1.6412,\n","         -1.1224, -0.7912, -0.7378]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0052, device='cuda:0')\n","Loss.item: 0.0052443030290305614\n","During training one batch, the logits of the output is: tensor([[-1.1523, -0.6141,  6.9847, -1.1001, -0.3010,  0.5523, -0.8772, -1.6411,\n","         -1.1219, -0.7840, -0.7413],\n","        [-0.4987, -0.1645, -0.6696,  7.1588, -0.3103, -0.8290,  0.0400, -0.5914,\n","         -0.6873, -0.4809, -0.0616],\n","        [-0.5482, -0.1134, -0.7049,  7.1415, -0.3716, -0.7735,  0.0504, -0.5416,\n","         -0.6855, -0.4544, -0.1189]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.004065901972353458\n","During training one batch, the logits of the output is: tensor([[-0.5442, -0.1246, -0.6729,  7.1463, -0.3472, -0.7797,  0.0324, -0.6009,\n","         -0.6609, -0.4930, -0.0655],\n","        [ 7.3880, -1.1637, -0.4554, -0.6166, -0.5300, -1.2485, -0.8022, -0.3083,\n","         -0.6573, -0.9036, -1.0974],\n","        [-1.2845, -0.7412, -0.6672, -0.5443, -0.6009, -0.6435, -0.3458, -0.2558,\n","         -0.5483, -1.3357,  7.2014]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0042, device='cuda:0')\n","Loss.item: 0.004229293670505285\n","During training one batch, the logits of the output is: tensor([[-1.3644, -0.6207, -0.7286, -0.4567, -0.4393, -0.5378, -0.2995, -0.4663,\n","         -0.7248, -1.3519,  7.1204],\n","        [-0.5360, -0.0269, -0.6452,  7.1067, -0.4493, -0.7327,  0.0140, -0.4081,\n","         -0.8022, -0.5552, -0.1716],\n","        [ 7.3995, -1.1602, -0.2080, -0.4014, -0.7568, -1.0954, -0.7644, -0.7188,\n","         -0.5914, -0.8355, -1.2908]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(2.0347, device='cuda:0')\n","Loss.item: 2.0346782207489014\n","During training one batch, the logits of the output is: tensor([[-0.6208, -0.1616, -1.5681, -1.0885, -0.7807, -1.0541, -1.2572,  7.6239,\n","         -0.7793, -0.8431, -0.9695],\n","        [-0.9252, -1.0249, -0.2389, -0.4727,  7.1761, -0.9947, -0.4105, -0.3612,\n","         -0.9142, -0.3895, -0.5884],\n","        [-1.4547, -0.4473,  6.3718, -1.1108,  0.2848,  0.4403, -1.0692, -1.2103,\n","         -1.6023, -0.7935, -0.0707]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0047, device='cuda:0')\n","Loss.item: 0.004695398733019829\n","During training one batch, the logits of the output is: tensor([[-0.9461, -1.2405,  1.0453, -0.9014,  6.8053, -0.7361, -0.5095, -0.8298,\n","         -0.6911, -0.5490, -0.7540],\n","        [-1.0698, -0.8185, -0.5906, -0.4415, -0.8460, -0.6136, -0.3117, -0.4566,\n","         -0.4727, -1.3884,  7.2581],\n","        [-0.9975,  0.5813, -1.6164, -0.9998, -0.8767, -1.1411, -1.0924,  7.5060,\n","         -0.8924, -1.0202, -0.9446]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0184, device='cuda:0')\n","Loss.item: 0.018389543518424034\n","During training one batch, the logits of the output is: tensor([[-0.8923,  0.2604, -1.7087, -0.9245, -0.3944, -1.3090, -1.1918,  7.4592,\n","         -0.9809, -0.6545, -0.7870],\n","        [-0.2710,  0.3131, -0.6913, -0.7333, -1.3502, -0.2281, -0.1108, -1.0685,\n","          5.4923, -0.5144, -0.3884],\n","        [-0.3711, -0.4259, -0.9493, -1.0883, -1.5350,  1.2409,  0.0747, -0.9205,\n","          5.7722, -0.6714, -0.7452]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0063, device='cuda:0')\n","Loss.item: 0.00633215019479394\n","During training one batch, the logits of the output is: tensor([[-0.1367, -0.4946, -0.9236, -0.9013, -1.4292,  0.2377,  0.0467, -0.7589,\n","          6.0325, -0.6061, -0.6110],\n","        [-0.8074,  0.0733, -1.5072, -1.0440, -0.8720, -1.1391, -1.0030,  7.5686,\n","         -0.8870, -1.0808, -0.8327],\n","        [-1.4038, -0.6947, -0.7340, -1.2481, -1.1876,  7.7746, -0.9496, -1.1717,\n","         -0.8870, -1.1338, -1.1237]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.002341298619285226\n","During training one batch, the logits of the output is: tensor([[-0.8553, -0.0972, -1.5306, -1.1416, -0.8893, -0.9135, -1.1240,  7.5790,\n","         -0.8250, -0.9030, -0.9541],\n","        [-0.9603,  0.1745, -1.7358, -0.9466, -0.7405, -1.2782, -1.1290,  7.5242,\n","         -0.7803, -0.7216, -0.6988],\n","        [-0.7397,  0.1350, -1.7241, -1.0254, -0.7755, -1.3494, -1.1265,  7.5765,\n","         -0.7216, -0.6686, -0.9351]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0159, device='cuda:0')\n","Loss.item: 0.015888508409261703\n","During training one batch, the logits of the output is: tensor([[-1.2169, -0.5948, -0.5359, -0.6185, -0.8890, -0.5330, -0.3217, -0.3805,\n","         -0.7372, -1.3777,  7.2155],\n","        [-0.5322, -0.0324, -0.8663, -0.3021, -0.5294, -0.6126,  5.1276, -1.3199,\n","         -0.0413, -0.0348, -1.0346],\n","        [-1.2352,  6.7981, -0.9062, -0.4736, -1.0914, -0.9676, -0.5542,  0.7800,\n","         -1.0923, -0.7817, -0.8204]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017522905254736543\n","During training one batch, the logits of the output is: tensor([[-0.7147,  0.0781, -1.5999, -1.2220, -0.6859, -1.1626, -1.2747,  7.5961,\n","         -0.8722, -0.6531, -1.0098],\n","        [-1.5067, -0.4898, -0.6031, -1.2408, -1.1597,  7.7825, -0.9690, -1.2449,\n","         -0.9858, -1.2610, -1.1551],\n","        [-1.3165, -0.6234, -0.7300, -1.2419, -1.2503,  7.7736, -0.9134, -1.2685,\n","         -0.7159, -1.4324, -1.1666]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0036799164954572916\n","During training one batch, the logits of the output is: tensor([[-1.5367, -0.5418, -0.7394, -1.2665, -1.1682,  7.7674, -0.9489, -1.1950,\n","         -0.8934, -1.4729, -0.9359],\n","        [-1.3605, -0.6899, -0.7634, -0.6229, -0.7183, -0.2055, -0.3906, -0.3537,\n","         -0.6157, -1.3290,  7.1305],\n","        [-0.5135, -0.1071, -0.6669,  7.1463, -0.3797, -0.8392,  0.0480, -0.4299,\n","         -0.7361, -0.5485, -0.1008]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0042, device='cuda:0')\n","Loss.item: 0.004176255781203508\n","During training one batch, the logits of the output is: tensor([[-0.4132, -0.6609,  7.0868, -0.6650, -0.4232, -0.1548, -0.7082, -1.7548,\n","         -0.8295, -0.5526, -1.1260],\n","        [-0.6609, -0.5225,  7.0841, -0.8032, -0.4011, -0.2207, -0.7234, -1.6231,\n","         -0.7306, -0.6983, -1.0372],\n","        [-0.8510, -0.9017, -0.5767, -0.5709, -0.8741, -0.5262, -0.5118, -0.1353,\n","         -0.6042, -1.5237,  7.1775]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.0027937323320657015\n","During training one batch, the logits of the output is: tensor([[-0.7145, -0.0601, -1.5953, -1.1677, -0.8247, -1.1314, -1.2276,  7.5931,\n","         -0.7613, -0.5653, -1.1163],\n","        [-0.6625, -0.1272, -1.5926, -1.1362, -0.6447, -1.2095, -1.2963,  7.6305,\n","         -0.7410, -0.6318, -1.0237],\n","        [-0.6418, -0.5993,  7.1095, -0.8255, -0.3523, -0.3092, -0.7595, -1.5875,\n","         -0.7272, -0.6819, -1.0077]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0070, device='cuda:0')\n","Loss.item: 0.006994839757680893\n","During training one batch, the logits of the output is: tensor([[-0.1344, -0.6641, -0.6986, -0.7799, -1.5013, -0.0501,  0.0333, -0.6922,\n","          6.1265, -0.7895, -0.2700],\n","        [-0.5728,  0.0272, -1.6190, -1.1506, -0.9445, -1.0881, -1.2336,  7.5827,\n","         -0.6477, -0.8902, -1.1402],\n","        [-0.3974, -0.2143, -0.6214,  7.1734, -0.2330, -0.8010, -0.0345, -0.6793,\n","         -0.7220, -0.4644, -0.1083]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0037126776296645403\n","During training one batch, the logits of the output is: tensor([[-0.3806, -0.2316, -0.6559,  7.1769, -0.2139, -0.8252, -0.0376, -0.6245,\n","         -0.7180, -0.4918, -0.1087],\n","        [ 7.3852, -1.2141, -0.3792, -0.3479, -0.9166, -1.1615, -0.6795, -0.6109,\n","         -0.3678, -0.8884, -1.1089],\n","        [ 7.4259, -1.2550, -0.4114, -0.2553, -0.6923, -1.3640, -0.7842, -0.2434,\n","         -0.4911, -0.8721, -1.3452]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.002238229848444462\n","During training one batch, the logits of the output is: tensor([[-6.7510e-01, -6.2777e-01, -4.3427e-01, -8.1625e-01, -6.9150e-01,\n","         -7.0037e-01, -2.0333e-02, -7.0047e-01, -4.3324e-01,  7.8380e+00,\n","         -1.2755e+00],\n","        [-7.0281e-01, -6.5775e-01, -3.7008e-01, -6.9471e-01, -6.3887e-01,\n","         -6.0982e-01,  6.0982e-03, -5.0748e-01, -8.0801e-01,  7.8114e+00,\n","         -1.2593e+00],\n","        [-6.7008e-01, -6.5408e-01, -2.9558e-01, -6.9760e-01, -6.3912e-01,\n","         -5.8224e-01,  1.3819e-02, -6.6371e-01, -7.7806e-01,  7.8168e+00,\n","         -1.2869e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0150, device='cuda:0')\n","Loss.item: 0.015041656792163849\n","During training one batch, the logits of the output is: tensor([[-0.8936, -0.0989, -1.7144, -1.0897, -0.7446, -1.0316, -1.1770,  7.5405,\n","         -0.8313, -0.7538, -0.7697],\n","        [-0.6099,  0.0190, -0.8344, -0.1956, -0.5907, -0.5106,  5.0870, -1.4101,\n","          0.1332, -0.1118, -1.1385],\n","        [ 7.4016, -1.2000, -0.3837, -0.3146, -0.8694, -1.3699, -0.6184, -0.3937,\n","         -0.3400, -0.9369, -1.3727]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.0036463194992393255\n","During training one batch, the logits of the output is: tensor([[-0.4246, -0.2139, -0.5518,  7.1557, -0.1907, -0.8047, -0.0247, -0.6485,\n","         -0.8659, -0.4492, -0.1046],\n","        [-0.7374,  0.2704, -1.6025, -1.0314, -0.7773, -1.2044, -1.1689,  7.5927,\n","         -0.8008, -0.9318, -1.0612],\n","        [ 7.3288, -1.1804, -0.3747, -0.1303, -1.0275, -1.3945, -0.5063, -0.3981,\n","         -0.2973, -1.0002, -1.3844]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.005014815833419561\n","During training one batch, the logits of the output is: tensor([[-0.1795, -0.2707, -0.5552,  7.1601, -0.1965, -0.8404, -0.0786, -0.6705,\n","         -0.8745, -0.5101, -0.1157],\n","        [-0.0908, -0.2902, -0.6073,  7.1462, -0.1361, -0.8749, -0.1237, -0.6189,\n","         -0.9003, -0.5720, -0.0806],\n","        [-0.9687, -0.9893, -0.0882, -0.3762,  7.1354, -0.9336, -0.3870, -0.4145,\n","         -1.0824, -0.5140, -0.5584]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0039, device='cuda:0')\n","Loss.item: 0.003851324552670121\n","During training one batch, the logits of the output is: tensor([[-0.6939, -0.7180, -0.3933, -0.7259, -0.5625, -0.7368, -0.0450, -0.4173,\n","         -0.6911,  7.8497, -1.2327],\n","        [-1.6496,  0.1528, -1.1244, -1.4037, -1.5091,  7.3356, -0.1731, -1.4462,\n","         -0.4292, -1.2432, -1.4020],\n","        [-1.0171,  6.7946, -0.8447, -0.5451, -1.2807, -0.8238, -0.5803,  0.7051,\n","         -1.2078, -0.7054, -1.0219]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0032, device='cuda:0')\n","Loss.item: 0.0031975938472896814\n","During training one batch, the logits of the output is: tensor([[-1.0897, -0.7531, -0.4542, -0.5002, -0.7481, -0.7294, -0.3133, -0.5638,\n","         -0.6287, -1.2816,  7.2217],\n","        [-0.9338, -1.0253, -0.2769, -0.4736,  7.1695, -0.9561, -0.4607, -0.2419,\n","         -0.9847, -0.4373, -0.5731],\n","        [-1.3335, -0.5434, -0.9695, -1.2833, -1.2908,  7.7002, -0.7966, -1.1653,\n","         -0.6742, -1.2106, -1.3907]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0137, device='cuda:0')\n","Loss.item: 0.013714795000851154\n","During training one batch, the logits of the output is: tensor([[-5.9393e-01, -9.7206e-02, -7.3747e-01, -4.1965e-02, -4.6181e-01,\n","         -5.4403e-01,  5.1541e+00, -1.4830e+00, -1.6527e-01, -2.0952e-02,\n","         -1.0872e+00],\n","        [-7.3049e-01,  2.9684e-02, -1.6299e+00, -1.1422e+00, -9.4123e-01,\n","         -1.0373e+00, -1.1837e+00,  7.6216e+00, -7.1563e-01, -9.2803e-01,\n","         -9.1892e-01],\n","        [-7.2134e-01,  6.3828e-03, -1.6307e+00, -1.1195e+00, -9.2178e-01,\n","         -1.0092e+00, -1.1648e+00,  7.6062e+00, -6.6669e-01, -8.9738e-01,\n","         -1.0610e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.002409200882539153\n","During training one batch, the logits of the output is: tensor([[-0.6990, -0.0293, -1.4572, -1.0597, -0.8231, -1.2104, -1.1104,  7.5748,\n","         -0.8243, -1.0925, -0.7957],\n","        [-0.9229,  0.4403, -1.6330, -1.0294, -0.9840, -1.1673, -0.9631,  7.4949,\n","         -0.7931, -1.1369, -0.8751],\n","        [-0.8859,  0.1540, -1.5057, -1.0768, -0.8068, -1.1475, -1.1666,  7.5522,\n","         -0.9151, -0.8470, -0.7854]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0023053805343806744\n","During training one batch, the logits of the output is: tensor([[-0.9672,  0.2599, -1.6253, -1.0664, -0.7223, -1.1667, -1.1582,  7.5260,\n","         -0.8755, -0.7797, -0.7908],\n","        [-0.4334, -0.2555, -1.5292, -0.9861, -0.4914, -1.2404, -1.1243,  7.5812,\n","         -0.7473, -1.1222, -0.9738],\n","        [-0.3415, -0.3123, -1.5298, -1.0109, -0.5984, -1.2306, -1.1862,  7.6089,\n","         -0.7924, -0.9806, -1.0158]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.003511593909934163\n","During training one batch, the logits of the output is: tensor([[-0.3144, -0.3220, -1.5712, -1.0668, -0.5399, -1.1882, -1.1913,  7.5859,\n","         -0.6615, -1.0778, -1.0956],\n","        [-0.6200, -0.5393,  7.0824, -0.7260, -0.3558, -0.2332, -0.7838, -1.6912,\n","         -0.8885, -0.6413, -0.9762],\n","        [-0.6383, -0.5771,  7.1109, -0.8133, -0.3121, -0.2714, -0.7813, -1.6758,\n","         -0.8093, -0.6848, -0.9624]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0034, device='cuda:0')\n","Loss.item: 0.0033548325300216675\n","During training one batch, the logits of the output is: tensor([[-0.6609, -0.6978, -0.3776, -0.8181, -0.4830, -0.6414, -0.0305, -0.7412,\n","         -0.4992,  7.8346, -1.2604],\n","        [-1.2161, -0.4793, -0.4980, -0.6434, -0.8914, -0.5583, -0.4022, -0.4674,\n","         -0.6460, -1.3335,  7.1764],\n","        [-1.2009, -0.4789, -0.5269, -0.6996, -0.9397, -0.5672, -0.3690, -0.3922,\n","         -0.5888, -1.3614,  7.1723]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.0040883650071918964\n","During training one batch, the logits of the output is: tensor([[-1.1226,  6.8450, -0.7803, -0.5084, -1.2153, -0.8574, -0.5615,  0.6555,\n","         -1.1470, -0.7892, -0.9279],\n","        [ 7.4265, -1.3195, -0.4771, -0.1936, -0.7395, -1.2079, -0.8189, -0.2061,\n","         -0.5576, -0.9443, -1.3056],\n","        [ 7.3312, -1.1841, -0.0128, -0.7376, -0.6534, -1.0129, -0.8572, -0.5230,\n","         -0.7230, -0.8663, -1.2348]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0030, device='cuda:0')\n","Loss.item: 0.002988075604662299\n","During training one batch, the logits of the output is: tensor([[ 7.3550, -1.2053, -0.0383, -0.5029, -0.7190, -1.0135, -0.7941, -0.6975,\n","         -0.5199, -0.9598, -1.3036],\n","        [ 7.4212, -1.1789, -0.3080, -0.1092, -0.7786, -1.2776, -0.7366, -0.6108,\n","         -0.5562, -0.9933, -1.1834],\n","        [ 7.4264, -1.2218, -0.2523, -0.5504, -0.6821, -1.1826, -0.8179, -0.4047,\n","         -0.6319, -0.8563, -1.2004]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.0033100962173193693\n","During training one batch, the logits of the output is: tensor([[-1.3992, -0.5608, -1.0174, -1.3201, -1.3061,  7.7425, -0.9588, -1.1002,\n","         -0.6186, -1.2130, -1.2352],\n","        [-0.9594, -0.9742, -0.2718, -0.5707,  7.1762, -0.9667, -0.4326, -0.1600,\n","         -0.8746, -0.5451, -0.6303],\n","        [-0.9653, -0.9596, -0.2917, -0.5857,  7.1753, -0.9465, -0.4298, -0.1578,\n","         -0.8395, -0.5839, -0.6190]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.7873, device='cuda:0')\n","Loss.item: 0.7872521877288818\n","During training one batch, the logits of the output is: tensor([[-0.4084, -0.3111, -1.5537, -1.1235, -0.7666, -1.1465, -1.2129,  7.6097,\n","         -0.7480, -0.8534, -1.0395],\n","        [-1.9288, -0.6810,  5.6253, -1.6518, -0.7256,  3.3807, -1.1574, -1.6753,\n","         -1.2228, -1.0366, -0.4430],\n","        [-0.9793, -0.9065, -0.2521, -0.5128,  7.1725, -0.9708, -0.3945, -0.3689,\n","         -0.8717, -0.4770, -0.6173]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0042, device='cuda:0')\n","Loss.item: 0.004168145824223757\n","During training one batch, the logits of the output is: tensor([[-0.7191, -0.1008, -1.5377, -1.0780, -0.7678, -1.0906, -1.1104,  7.6113,\n","         -0.7891, -0.9991, -0.9778],\n","        [-0.4953, -0.3221, -1.5263, -1.0605, -0.7672, -1.1530, -1.0741,  7.5983,\n","         -0.6611, -1.0748, -1.0119],\n","        [-1.1332,  6.7485, -0.9850, -0.6450, -1.3716, -0.7322, -0.6372,  1.2747,\n","         -1.2210, -0.9226, -1.1202]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0040, device='cuda:0')\n","Loss.item: 0.004005018621683121\n","During training one batch, the logits of the output is: tensor([[-1.5399,  6.5488, -0.9022, -0.3833, -0.7385, -0.5612, -0.2734,  0.6233,\n","         -1.5632, -0.7299, -0.5748],\n","        [-1.2600, -0.6626, -0.7718, -1.2262, -1.2416,  7.7205, -0.9926, -1.2163,\n","         -0.6611, -1.3650, -1.2442],\n","        [-1.2994, -0.6347, -0.7459, -1.2486, -1.2471,  7.7376, -0.9826, -1.2059,\n","         -0.7281, -1.3432, -1.2336]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0027, device='cuda:0')\n","Loss.item: 0.0026551950722932816\n","During training one batch, the logits of the output is: tensor([[-1.2853, -0.7143, -0.7396, -1.2016, -1.1866,  7.7330, -0.9918, -1.2206,\n","         -0.7814, -1.2707, -1.2202],\n","        [-1.4418, -0.7642, -0.7618, -1.1688, -1.1822,  7.7843, -0.9637, -1.2683,\n","         -0.8533, -1.1399, -1.0872],\n","        [-0.1712, -0.6957,  6.9739, -0.7768, -0.4039, -0.5281, -0.7051, -1.5992,\n","         -0.7810, -0.3092, -1.0584]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.004957422614097595\n","During training one batch, the logits of the output is: tensor([[-0.7231, -0.5654,  7.0281, -0.8981, -0.1580, -0.5550, -0.8406, -1.5371,\n","         -0.7933, -0.6717, -0.5942],\n","        [-0.3612, -0.9579,  6.8237, -0.9319, -0.1663, -0.7128, -0.8460, -1.3098,\n","         -0.9619,  0.1579, -0.6559],\n","        [-0.7452, -0.6487,  7.0720, -1.0270, -0.3546,  0.1977, -0.8663, -1.6262,\n","         -0.9906, -0.7943, -0.8611]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0045, device='cuda:0')\n","Loss.item: 0.00445275055244565\n","During training one batch, the logits of the output is: tensor([[-0.7674, -0.7369,  7.0720, -1.0406, -0.2850,  0.2522, -0.8885, -1.6912,\n","         -0.9986, -0.6630, -0.8953],\n","        [-0.8286, -0.7163,  7.0609, -1.0190, -0.2628,  0.2499, -0.9037, -1.6227,\n","         -1.0546, -0.6690, -0.8764],\n","        [-0.8263, -0.7244,  7.0621, -1.0442, -0.3214,  0.2831, -0.8566, -1.5931,\n","         -1.0137, -0.6740, -0.9182]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.003620341420173645\n","During training one batch, the logits of the output is: tensor([[-1.1170, -0.7698, -0.5866, -0.5071, -0.8297, -0.6098, -0.2775, -0.3847,\n","         -0.5371, -1.4044,  7.2682],\n","        [-1.1564, -0.7836, -0.5788, -0.4601, -0.7575, -0.6028, -0.2672, -0.4848,\n","         -0.5196, -1.3956,  7.2657],\n","        [-1.1178, -0.8144, -0.5886, -0.4300, -0.7668, -0.6510, -0.2799, -0.4700,\n","         -0.4905, -1.3399,  7.2735]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0060, device='cuda:0')\n","Loss.item: 0.006027506198734045\n","During training one batch, the logits of the output is: tensor([[-1.1332, -0.8013, -0.5721, -0.4460, -0.7716, -0.6162, -0.2420, -0.5197,\n","         -0.4796, -1.3566,  7.2665],\n","        [-1.0967,  6.8306, -0.9577, -0.6708, -1.2704, -0.9109, -0.7116,  1.1615,\n","         -1.1362, -0.8851, -0.9580],\n","        [-1.0611,  6.8076, -0.8742, -0.6540, -1.3417, -0.8515, -0.7105,  1.0933,\n","         -1.1524, -0.8780, -1.0102]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024927109479904175\n","During training one batch, the logits of the output is: tensor([[-0.8496,  0.1561, -1.5508, -1.0697, -0.7374, -1.0192, -1.1787,  7.5657,\n","         -0.8939, -1.1102, -0.9166],\n","        [-0.6173, -0.1624, -1.5199, -1.0624, -0.6283, -1.2668, -1.2817,  7.6097,\n","         -0.8490, -0.5594, -0.9917],\n","        [ 7.3839, -1.2384, -0.4807, -0.6013, -0.7730, -1.1454, -0.8022, -0.2460,\n","         -0.3382, -0.9622, -1.3111]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0015218589687719941\n","During training one batch, the logits of the output is: tensor([[-1.5936, -0.4339, -0.7106, -1.2325, -1.2554,  7.7584, -0.9991, -1.1762,\n","         -0.9842, -1.1582, -1.0782],\n","        [-1.5252, -0.4470, -0.7228, -1.2534, -1.3016,  7.7704, -0.9892, -1.2113,\n","         -0.8731, -1.1963, -1.1336],\n","        [-1.5172, -0.5128, -0.7778, -1.2152, -1.2525,  7.7794, -0.9739, -1.2360,\n","         -0.8702, -1.1659, -1.1374]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.7218, device='cuda:0')\n","Loss.item: 0.7217769026756287\n","During training one batch, the logits of the output is: tensor([[-2.5625,  4.1067, -1.1043, -0.8092, -1.0685,  4.4608, -0.3317, -0.6110,\n","         -1.7835, -1.0546, -1.1294],\n","        [-2.3291,  4.9419, -0.9463, -0.9031, -1.2234,  3.5825, -0.8683,  0.0699,\n","         -1.8511, -1.2417, -1.0932],\n","        [-0.5579, -0.6339,  7.1197, -0.7938, -0.3786, -0.2671, -0.7013, -1.6287,\n","         -0.8117, -0.5786, -1.0693]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.0029431134462356567\n","During training one batch, the logits of the output is: tensor([[ 7.3721, -1.3425, -0.5420, -0.4595, -0.8190, -1.0557, -0.7105, -0.5308,\n","         -0.3547, -1.0071, -1.0251],\n","        [ 7.3993, -1.3373, -0.5066, -0.4952, -0.7328, -1.0633, -0.6882, -0.4856,\n","         -0.5152, -0.9556, -1.0939],\n","        [ 7.3878, -1.3234, -0.4916, -0.4001, -0.7858, -1.1043, -0.6742, -0.5264,\n","         -0.4704, -0.9567, -1.1343]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.004063628148287535\n","During training one batch, the logits of the output is: tensor([[ 7.3780, -1.2654, -0.4503, -0.3837, -0.7641, -1.1300, -0.6325, -0.7298,\n","         -0.3546, -0.8719, -1.1873],\n","        [ 7.4271, -1.2224, -0.4185, -0.4606, -0.7395, -1.1688, -0.7572, -0.4734,\n","         -0.4846, -0.8171, -1.2671],\n","        [-1.1253,  6.8340, -0.7097, -0.5232, -1.2148, -0.8879, -0.6309,  0.7282,\n","         -1.1073, -0.7693, -0.9670]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0046, device='cuda:0')\n","Loss.item: 0.004551655612885952\n","During training one batch, the logits of the output is: tensor([[-1.1043,  6.8210, -0.6738, -0.5147, -1.2081, -0.8630, -0.6319,  0.6564,\n","         -1.1156, -0.7543, -1.0037],\n","        [ 7.4327, -1.2366, -0.4179, -0.4056, -0.7237, -1.2116, -0.7866, -0.4108,\n","         -0.4871, -0.8286, -1.2879],\n","        [-0.8051, -0.9756, -0.0963, -0.6577,  7.1224, -0.9642, -0.5366, -0.1575,\n","         -1.0102, -0.6004, -0.4887]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.003640713868662715\n","During training one batch, the logits of the output is: tensor([[-0.8907, -0.9708, -0.1270, -0.5693,  7.1678, -0.9044, -0.4702, -0.4104,\n","         -0.9434, -0.5010, -0.5226],\n","        [-1.2650, -0.6769, -0.5944, -0.5017, -0.6357, -0.7976, -0.1939, -0.5528,\n","         -0.0871, -1.5428,  7.1125],\n","        [-0.7217, -0.6744, -0.4222, -0.8765, -0.3782, -0.6969, -0.0501, -0.6384,\n","         -0.4921,  7.8290, -1.2209]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0022079392801970243\n","During training one batch, the logits of the output is: tensor([[-0.7590, -0.6761, -0.2897, -0.7908, -0.4091, -0.7192, -0.0265, -0.7182,\n","         -0.6157,  7.8486, -1.1326],\n","        [-0.6889, -0.6446, -0.3027, -0.7790, -0.5602, -0.6192, -0.0385, -0.7619,\n","         -0.5549,  7.8329, -1.2931],\n","        [-0.7025, -0.7099, -0.2781, -0.7578, -0.4475, -0.7334, -0.0197, -0.6787,\n","         -0.6671,  7.8507, -1.1613]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0036952781956642866\n","During training one batch, the logits of the output is: tensor([[-0.7264, -0.6959, -0.2994, -0.7312, -0.4419, -0.7426, -0.0189, -0.6591,\n","         -0.6866,  7.8406, -1.1170],\n","        [-0.4943, -0.1715, -0.6000,  7.1642, -0.3055, -0.7444,  0.0577, -0.5599,\n","         -0.8298, -0.4608, -0.1246],\n","        [-1.0844, -0.8028, -0.6122, -0.4960, -0.8693, -0.5704, -0.2780, -0.5234,\n","         -0.4572, -1.3354,  7.2615]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0131, device='cuda:0')\n","Loss.item: 0.013103972189128399\n","During training one batch, the logits of the output is: tensor([[-1.8938e+00,  1.4739e+00, -1.4453e+00, -9.5524e-01, -9.8171e-01,\n","         -6.4849e-01,  3.7952e-01,  4.3947e-01, -3.1282e-01, -1.7799e+00,\n","          5.6174e+00],\n","        [-1.4581e+00, -4.5368e-01, -9.0808e-01, -1.3343e+00, -1.2222e+00,\n","          7.7548e+00, -9.3409e-01, -1.1532e+00, -6.4538e-01, -1.4525e+00,\n","         -1.1002e+00],\n","        [-7.4068e-01, -5.8448e-01, -3.3742e-01, -6.6266e-01, -5.8660e-01,\n","         -6.3886e-01,  4.0922e-03, -6.6337e-01, -8.0475e-01,  7.8237e+00,\n","         -1.2127e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0064, device='cuda:0')\n","Loss.item: 0.006388211157172918\n","During training one batch, the logits of the output is: tensor([[-0.7744, -0.6391, -0.4499, -0.8109, -0.6658, -0.5898, -0.0937, -0.1376,\n","         -0.7965,  7.7573, -1.2738],\n","        [-1.1022, -0.8125, -0.5510, -0.4550, -0.7748, -0.6487, -0.2835, -0.5412,\n","         -0.4780, -1.3445,  7.2675],\n","        [-0.3516, -0.6346, -0.8832, -0.6747, -1.1379, -0.1381,  0.1471, -0.8055,\n","          6.1439, -0.7411, -0.3138]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0017414335161447525\n","During training one batch, the logits of the output is: tensor([[-0.8895, -0.0108, -1.6322, -1.1747, -0.9043, -1.0090, -1.2072,  7.5965,\n","         -0.8042, -0.8290, -0.7218],\n","        [-1.3327, -0.6458, -0.6731, -1.2006, -1.2355,  7.7488, -0.9177, -1.3519,\n","         -0.8503, -1.1565, -1.2113],\n","        [-1.3100, -0.7053, -0.8953, -1.2175, -1.2609,  7.7642, -0.9513, -1.2103,\n","         -0.6940, -1.3128, -1.1105]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017957004019990563\n","During training one batch, the logits of the output is: tensor([[-1.2745, -0.7592, -0.9700, -1.2019, -1.2423,  7.7503, -0.9129, -1.2106,\n","         -0.6369, -1.3168, -1.0976],\n","        [-1.3537, -0.7884, -0.8286, -1.1865, -1.1046,  7.7804, -0.9367, -1.1952,\n","         -0.8582, -1.3587, -0.9240],\n","        [-0.5923, -0.7185, -0.3258, -0.6129, -0.5672, -0.8363, -0.0700, -0.6775,\n","         -0.6536,  7.7626, -1.1194]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0061, device='cuda:0')\n","Loss.item: 0.006138244643807411\n","During training one batch, the logits of the output is: tensor([[-0.7017, -0.6716, -0.3144, -0.6264, -0.5118, -0.8401, -0.0667, -0.5423,\n","         -0.7727,  7.7959, -1.0882],\n","        [-0.2018, -0.4680, -0.9210, -0.7925, -1.3504,  0.1094,  0.1593, -0.7689,\n","          6.0653, -0.7287, -0.5515],\n","        [-1.5031, -0.6774, -0.8246, -1.2010, -1.1780,  7.8024, -1.0002, -1.2341,\n","         -0.8696, -1.1904, -1.0279]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.6768, device='cuda:0')\n","Loss.item: 0.6768155097961426\n","During training one batch, the logits of the output is: tensor([[-0.2564, -0.3561, -1.0681, -0.8504, -1.3217,  0.1510,  0.1484, -0.7146,\n","          6.0769, -0.8648, -0.4471],\n","        [ 7.3417, -1.2690, -0.5155, -0.0664, -0.8697, -1.3703, -0.5272, -0.4107,\n","         -0.2137, -0.9378, -1.4884],\n","        [ 3.3847, -0.7510, -1.3653,  5.2251, -0.3845, -1.7575, -0.4998,  0.8056,\n","         -0.8492, -1.5516, -1.0454]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0063, device='cuda:0')\n","Loss.item: 0.006328262854367495\n","During training one batch, the logits of the output is: tensor([[ 7.2997, -1.2949, -0.5861,  0.1097, -0.8493, -1.3934, -0.4922, -0.3859,\n","         -0.1847, -0.9657, -1.5493],\n","        [-0.9451, -1.0363, -0.2581, -0.4565,  7.1767, -0.9879, -0.3483, -0.4383,\n","         -0.8459, -0.4278, -0.5472],\n","        [-1.0231,  6.5669, -1.0245, -0.6244, -1.4495, -0.7052, -0.7166,  1.5185,\n","         -1.2701, -1.0024, -1.2011]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.4669, device='cuda:0')\n","Loss.item: 0.4669346511363983\n","During training one batch, the logits of the output is: tensor([[-2.2113,  4.9054, -1.3150, -1.1845, -1.7652,  3.8312, -0.1349, -0.6353,\n","         -0.5994, -1.5953, -1.6922],\n","        [-1.0315,  6.7155, -0.9102, -0.6189, -1.4035, -0.7672, -0.7038,  1.1423,\n","         -1.2434, -0.8961, -1.0218],\n","        [-1.0722,  6.7911, -0.9468, -0.6291, -1.2744, -0.8701, -0.6659,  1.0457,\n","         -1.1958, -0.8530, -0.9861]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0015407841419801116\n","During training one batch, the logits of the output is: tensor([[-1.3567, -0.4573, -0.8139, -1.1153, -1.2960,  7.7516, -0.8974, -1.2849,\n","         -0.9182, -1.2393, -1.2264],\n","        [-1.3224, -0.6715, -0.8394, -1.1245, -1.2929,  7.7686, -0.8761, -1.3623,\n","         -0.7385, -1.3149, -1.0919],\n","        [-1.5022, -0.3584, -1.0914, -1.2253, -1.3115,  7.7356, -0.9191, -1.1715,\n","         -0.7822, -1.2222, -1.1610]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.0032909198198467493\n","During training one batch, the logits of the output is: tensor([[-0.1905, -0.2811, -0.7033,  7.1453, -0.3082, -0.9073,  0.0517, -0.6407,\n","         -0.6012, -0.5099, -0.1471],\n","        [-0.7744, -0.0144, -1.6864, -1.1509, -0.6800, -1.0848, -1.2658,  7.5905,\n","         -0.7448, -0.5626, -1.1326],\n","        [-0.8171,  0.0090, -1.7210, -1.1843, -0.6979, -1.0564, -1.2899,  7.5810,\n","         -0.7115, -0.5071, -1.1337]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1624, device='cuda:0')\n","Loss.item: 0.16244012117385864\n","During training one batch, the logits of the output is: tensor([[-1.3184e+00,  6.0928e-01, -1.8536e-01, -5.1631e-01,  1.0377e+00,\n","         -3.5339e-01, -8.9465e-01,  4.5912e-01, -6.9965e-01, -1.9627e-01,\n","          2.8214e+00],\n","        [-1.3510e+00,  9.7538e-01, -1.7267e+00, -9.4439e-01, -7.2907e-01,\n","         -1.0530e+00, -1.3337e+00,  7.2153e+00, -8.6988e-01, -1.0474e+00,\n","         -5.9961e-01],\n","        [-5.6903e-01, -6.1778e-01, -2.9418e-01, -6.6210e-01, -5.0774e-01,\n","         -7.4393e-01,  1.1646e-03, -7.2316e-01, -7.6429e-01,  7.8117e+00,\n","         -1.3117e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.005049651954323053\n","During training one batch, the logits of the output is: tensor([[-6.4771e-01, -6.2808e-01, -3.8605e-01, -6.6190e-01, -5.7246e-01,\n","         -6.8916e-01, -1.8047e-02, -4.8221e-01, -8.7805e-01,  7.7959e+00,\n","         -1.2653e+00],\n","        [-6.4607e-01, -6.4598e-01, -3.8033e-01, -6.1662e-01, -5.8775e-01,\n","         -6.9614e-01, -2.4944e-03, -5.9439e-01, -8.3873e-01,  7.7991e+00,\n","         -1.1901e+00],\n","        [-1.1404e+00,  6.6625e+00, -1.0872e+00, -6.5646e-01, -1.2561e+00,\n","         -8.4781e-01, -7.4486e-01,  1.5860e+00, -1.2602e+00, -9.2902e-01,\n","         -1.0027e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0106, device='cuda:0')\n","Loss.item: 0.010558034293353558\n","During training one batch, the logits of the output is: tensor([[-0.9976,  6.4191, -1.0402, -0.5408, -1.3354, -0.8210, -0.6350,  1.6362,\n","         -1.3904, -0.9503, -1.1303],\n","        [-1.1511,  6.7903, -0.9494, -0.6135, -1.2371, -0.6093, -0.6994,  1.0063,\n","         -1.2493, -0.9098, -1.0623],\n","        [-1.1258,  6.6296, -0.9648, -0.6349, -1.2767, -0.7556, -0.7435,  1.5292,\n","         -1.3523, -0.9616, -1.0596]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0054, device='cuda:0')\n","Loss.item: 0.005366401746869087\n","During training one batch, the logits of the output is: tensor([[-0.9828, -0.9382, -0.2478, -0.5565,  7.1655, -0.9528, -0.4517, -0.2749,\n","         -0.9300, -0.4436, -0.6167],\n","        [-1.3602,  6.7930, -0.9016, -0.7089, -1.2952, -0.4988, -0.7000,  1.1789,\n","         -1.2950, -0.9263, -1.0079],\n","        [-0.9446, -0.9767, -0.1872, -0.3734,  7.1758, -1.0142, -0.4186, -0.4244,\n","         -1.0019, -0.4484, -0.4872]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0110, device='cuda:0')\n","Loss.item: 0.010959492065012455\n","During training one batch, the logits of the output is: tensor([[-0.9969, -0.9384, -0.2685, -0.3318,  7.1507, -0.9903, -0.4302, -0.2582,\n","         -1.0341, -0.5639, -0.5259],\n","        [-0.3203, -0.5743, -0.7836, -0.8363, -1.2522, -0.1839,  0.0946, -0.6568,\n","          6.1729, -0.7594, -0.4055],\n","        [-1.2147,  6.3064, -0.8661, -0.5075, -1.0620, -0.4704, -0.8019,  1.6295,\n","         -1.6477, -0.9797, -0.8421]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0025582381058484316\n","During training one batch, the logits of the output is: tensor([[-0.6878, -0.6224, -0.2758, -0.7680, -0.6364, -0.5319, -0.0672, -0.5163,\n","         -0.7864,  7.8227, -1.3706],\n","        [-0.6504, -0.6268, -0.6553, -1.0426, -0.6931, -0.5381, -0.2613,  0.5445,\n","         -0.6321,  7.5776, -1.7265],\n","        [-0.6811, -0.6424, -0.3078, -0.8215, -0.6071, -0.5656, -0.0823, -0.3701,\n","         -0.7508,  7.8264, -1.4182]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0030, device='cuda:0')\n","Loss.item: 0.0030480276327580214\n","During training one batch, the logits of the output is: tensor([[-0.6803, -0.6510, -0.3232, -0.8301, -0.6053, -0.5746, -0.0897, -0.3331,\n","         -0.7411,  7.8256, -1.4188],\n","        [-0.6921, -0.5630, -0.4655, -0.9573, -0.6887, -0.3867, -0.1398, -0.0204,\n","         -0.7255,  7.6738, -1.6635],\n","        [-0.8540, -0.7001,  7.1055, -0.9242, -0.2601,  0.0660, -0.7946, -1.7183,\n","         -0.9894, -0.5993, -0.8535]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.0032869980204850435\n","During training one batch, the logits of the output is: tensor([[-1.4190, -0.5102, -0.8707, -1.2804, -1.1700,  7.7402, -1.0562, -1.0321,\n","         -0.8993, -1.2786, -1.1580],\n","        [-1.0174,  0.2796, -1.8224, -1.1476, -0.2929, -1.1866, -1.3602,  7.4350,\n","         -0.7642, -0.1903, -1.2349],\n","        [-0.4892, -0.1250, -0.6569,  7.1449, -0.3433, -0.7983,  0.0255, -0.5256,\n","         -0.7714, -0.4358, -0.1219]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018489836947992444\n","During training one batch, the logits of the output is: tensor([[-0.8149, -0.5898, -0.3149, -0.6527, -0.1127, -0.8067, -0.0554, -0.5360,\n","         -0.9042,  7.7481, -1.2053],\n","        [-1.4261, -0.4589, -0.5732, -1.2644, -1.2548,  7.7607, -0.9517, -1.2983,\n","         -0.9122, -1.4050, -1.1934],\n","        [-1.2494, -0.7192, -0.7689, -1.2344, -1.2445,  7.7724, -0.9680, -1.3075,\n","         -0.6809, -1.3090, -1.1456]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0015170187689363956\n","During training one batch, the logits of the output is: tensor([[-1.2771, -0.5923, -0.6718, -1.2411, -1.2659,  7.7626, -0.9719, -1.3617,\n","         -0.7713, -1.2955, -1.1908],\n","        [-1.4212, -0.5057, -0.7775, -1.2266, -1.2358,  7.7636, -1.0038, -1.1354,\n","         -0.8758, -1.2972, -1.1905],\n","        [-1.4575, -0.5250, -0.8956, -1.2742, -1.2095,  7.7613, -0.9980, -1.0256,\n","         -0.8746, -1.3604, -1.0873]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024731524754315615\n","During training one batch, the logits of the output is: tensor([[-1.4809, -0.5012, -0.7910, -1.2312, -1.1813,  7.7588, -1.0478, -1.0822,\n","         -0.9382, -1.3179, -1.1007],\n","        [-0.9109, -0.9367, -0.1717, -0.4170,  7.1575, -0.9523, -0.4294, -0.5717,\n","         -0.9899, -0.3497, -0.4879],\n","        [-1.5446, -0.4046, -0.8378, -1.2299, -1.2099,  7.7486, -0.9655, -1.1119,\n","         -0.9085, -1.3012, -1.1436]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0050, device='cuda:0')\n","Loss.item: 0.005045308731496334\n","During training one batch, the logits of the output is: tensor([[-1.2324,  6.7770, -0.6391, -0.4491, -1.0344, -0.7471, -0.5725,  0.5314,\n","         -1.2281, -0.7196, -0.8378],\n","        [-1.1429,  6.7692, -0.6782, -0.3821, -1.0782, -0.8837, -0.4958,  0.4158,\n","         -1.1744, -0.6675, -0.8934],\n","        [-0.7059, -0.0355, -1.4647, -1.1248, -0.8607, -1.0491, -1.1343,  7.5930,\n","         -0.8739, -0.9508, -0.9991]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.003535303520038724\n","During training one batch, the logits of the output is: tensor([[-7.6323e-01, -5.6027e-03, -1.5362e+00, -1.0827e+00, -7.4400e-01,\n","         -1.1135e+00, -1.1288e+00,  7.5951e+00, -8.8855e-01, -8.7991e-01,\n","         -9.3580e-01],\n","        [-1.0117e+00,  4.7408e-01, -1.6657e+00, -1.0052e+00, -7.2515e-01,\n","         -1.0428e+00, -1.1701e+00,  7.4989e+00, -8.4467e-01, -1.0354e+00,\n","         -1.0642e+00],\n","        [-4.8430e-01, -1.6379e-01, -7.1917e-01,  7.0843e+00, -2.3141e-01,\n","         -7.9340e-01,  2.1550e-01, -5.3672e-01, -6.9692e-01, -5.2187e-01,\n","         -2.5140e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.0035172700881958008\n","During training one batch, the logits of the output is: tensor([[-1.3587, -0.4961, -0.8176, -1.2432, -1.3013,  7.7641, -0.9199, -1.2358,\n","         -0.7405, -1.3389, -1.2346],\n","        [-1.3408, -0.6439, -0.8195, -1.2656, -1.3164,  7.7529, -0.9096, -1.2946,\n","         -0.6622, -1.1832, -1.2368],\n","        [-1.1078,  6.8003, -0.8940, -0.6320, -1.2504, -0.7945, -0.8201,  1.1639,\n","         -1.2062, -0.8832, -0.9276]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0061, device='cuda:0')\n","Loss.item: 0.006110355723649263\n","During training one batch, the logits of the output is: tensor([[-1.4042, -0.5175, -0.7822, -1.2229, -1.3069,  7.7777, -0.9938, -1.2398,\n","         -0.8251, -1.2028, -1.2049],\n","        [-0.8578, -0.0215, -1.7622, -1.3073, -0.7481, -1.1020, -1.3093,  7.4682,\n","         -0.6716, -0.0720, -1.1632],\n","        [-1.0400, -0.1523,  6.1255, -0.7605,  0.7353, -0.7251, -1.0815, -0.7657,\n","         -1.8504, -0.6260, -0.2447]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.003815865144133568\n","During training one batch, the logits of the output is: tensor([[ 7.3804, -1.3184, -0.5219, -0.4142, -0.6633, -1.3086, -0.9059,  0.1064,\n","         -0.4763, -1.1671, -1.2812],\n","        [-0.4385, -0.2407, -0.5495,  7.1516, -0.1584, -0.8022, -0.0103, -0.7670,\n","         -0.7077, -0.4267, -0.1205],\n","        [ 7.3861, -1.2490, -0.5423, -0.4624, -0.7285, -1.0947, -0.8156, -0.2072,\n","         -0.3949, -1.0697, -1.3258]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0078, device='cuda:0')\n","Loss.item: 0.007791378069669008\n","During training one batch, the logits of the output is: tensor([[-1.4117, -0.3652, -0.8290, -1.2731, -1.3110,  7.7509, -0.9691, -1.1909,\n","         -0.6782, -1.3981, -1.2671],\n","        [-1.2648,  6.8223, -0.7114, -0.5945, -1.1381, -0.5711, -0.4808,  0.7248,\n","         -1.2670, -0.9510, -1.0366],\n","        [ 6.4890, -1.2857, -0.8765, -0.9104, -1.0927, -1.4212, -1.2310,  1.8316,\n","         -0.7688, -1.8142,  0.3111]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0044, device='cuda:0')\n","Loss.item: 0.004386700689792633\n","During training one batch, the logits of the output is: tensor([[-0.3989, -0.6800,  7.1235, -0.6323, -0.3651, -0.3919, -0.7240, -1.7365,\n","         -0.7270, -0.5977, -1.0460],\n","        [-0.5238, -0.6265,  7.1120, -0.6090, -0.3727, -0.3236, -0.7327, -1.7106,\n","         -0.8401, -0.5831, -0.9927],\n","        [-2.0432, -0.2257, -1.0161, -1.3121, -0.2949,  6.8405, -0.1420, -0.9283,\n","         -1.1482, -1.3046, -0.8589]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017626032931730151\n","During training one batch, the logits of the output is: tensor([[-1.4615, -0.5548, -0.8156, -1.2334, -1.2581,  7.7511, -0.9389, -1.3955,\n","         -0.6437, -1.1374, -1.1797],\n","        [-1.4013, -0.7510, -0.8224, -1.1931, -1.2222,  7.7729, -0.9643, -1.2637,\n","         -0.7630, -1.1495, -1.1427],\n","        [-0.4988, -0.7095, -0.5134, -0.7232, -0.5899, -0.8724, -0.0742, -0.2974,\n","         -0.6125,  7.8215, -1.3725]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018275114707648754\n","During training one batch, the logits of the output is: tensor([[-0.4241, -0.7369, -0.5238, -0.7634, -0.5506, -0.9291, -0.0574, -0.3078,\n","         -0.5663,  7.8299, -1.3800],\n","        [-1.2292, -0.6274, -0.8434, -1.2830, -1.2578,  7.7143, -0.9805, -1.3548,\n","         -0.5105, -1.1531, -1.2387],\n","        [-1.2893, -0.6850, -0.8541, -1.2759, -1.1890,  7.7252, -1.0314, -1.3398,\n","         -0.5434, -1.0894, -1.1766]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0037005709018558264\n","During training one batch, the logits of the output is: tensor([[-1.3735, -0.5198, -0.7586, -1.3177, -1.3858,  7.7264, -0.9857, -1.2730,\n","         -0.5441, -1.3004, -1.2211],\n","        [-1.0064,  6.7664, -0.9862, -0.7108, -1.4895, -0.6407, -0.7382,  1.2539,\n","         -1.0992, -1.0604, -1.2253],\n","        [-1.2916, -0.5396, -0.8096, -1.2457, -1.3167,  7.7544, -0.9683, -1.2366,\n","         -0.6301, -1.3723, -1.2624]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0025162065867334604\n","During training one batch, the logits of the output is: tensor([[-1.2629, -0.4968, -0.7967, -1.2459, -1.2635,  7.7394, -0.9751, -1.1414,\n","         -0.8411, -1.3429, -1.2163],\n","        [-0.6839, -0.6411, -0.2780, -0.6338, -0.6270, -0.5990,  0.0083, -0.8018,\n","         -0.8183,  7.8021, -1.1841],\n","        [-1.2906, -0.6498, -0.5161, -0.5845, -0.8263, -0.5194, -0.2955, -0.4507,\n","         -0.6123, -1.2928,  7.2444]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.0037143698427826166\n","During training one batch, the logits of the output is: tensor([[-1.2666, -0.7338, -0.5536, -0.5700, -0.7583, -0.3947, -0.3914, -0.4360,\n","         -0.6235, -1.3332,  7.2280],\n","        [-1.2235, -0.6573, -0.5995, -0.6245, -0.8932, -0.4676, -0.3303, -0.4221,\n","         -0.5161, -1.3607,  7.2390],\n","        [-1.2474, -0.6721, -0.5064, -0.6533, -0.8654, -0.5544, -0.3898, -0.2732,\n","         -0.6561, -1.2760,  7.2369]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0109, device='cuda:0')\n","Loss.item: 0.010850630700588226\n","During training one batch, the logits of the output is: tensor([[-1.7503, -0.1354, -0.0755, -1.3735, -1.1956,  7.5915, -1.0794, -1.5240,\n","         -1.0407, -1.1859, -0.8141],\n","        [-0.1725, -0.5858, -0.7835, -0.8988, -1.1684,  0.0433, -0.0288, -0.5610,\n","          6.0011, -0.8571, -0.5870],\n","        [-0.2365, -0.4576, -0.9858, -1.0057, -0.9270, -0.2194, -0.0346, -0.4145,\n","          5.9613, -0.8228, -0.5467]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0187, device='cuda:0')\n","Loss.item: 0.01865316741168499\n","During training one batch, the logits of the output is: tensor([[-0.3317, -0.5553, -0.7191, -0.8867, -1.2799, -0.1160,  0.1269, -0.5903,\n","          6.0483, -0.6728, -0.6019],\n","        [-0.3206, -0.5773, -0.7485, -0.9022, -1.2688, -0.1238,  0.1339, -0.5898,\n","          6.0470, -0.5896, -0.6260],\n","        [-0.3182, -0.5160, -0.6836, -0.9094, -0.6029, -0.7300,  0.2328, -0.4285,\n","          5.4649,  0.0295, -1.1158]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.0030855482909828424\n","During training one batch, the logits of the output is: tensor([[-1.4592, -0.5123, -0.9146, -1.2418, -1.2898,  7.7722, -0.8927, -1.2800,\n","         -0.7817, -1.1835, -1.1608],\n","        [-1.3175, -0.3938, -0.8005, -1.2322, -1.4352,  7.7348, -0.9183, -1.3336,\n","         -0.6716, -1.3616, -1.1899],\n","        [-1.1151,  6.8651, -0.9168, -0.5692, -1.4844, -0.6734, -0.3811,  0.7578,\n","         -1.0833, -0.8396, -1.1915]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0025544019881635904\n","During training one batch, the logits of the output is: tensor([[-1.3098, -0.4839, -0.7737, -1.2599, -1.2187,  7.6695, -0.6817, -1.3738,\n","         -0.7732, -1.1588, -1.3257],\n","        [-0.9197,  0.1986, -1.8506, -1.1670, -0.1235, -1.1435, -1.3155,  7.4072,\n","         -0.7670, -0.7573, -0.9445],\n","        [-1.0454,  0.1870, -1.8885, -1.1750, -0.2833, -1.0326, -1.1966,  7.3896,\n","         -0.7432, -0.9576, -0.6469]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.0036024253349751234\n","During training one batch, the logits of the output is: tensor([[-0.7777,  0.0737, -1.6184, -1.1999, -0.7343, -1.0748, -1.2687,  7.5838,\n","         -0.8404, -0.7790, -0.9019],\n","        [-0.9606, -0.9051, -0.1895, -0.4554,  7.1585, -0.9306, -0.4163, -0.5206,\n","         -0.9723, -0.3931, -0.5298],\n","        [-0.9698, -0.9185, -0.1647, -0.4922,  7.1656, -0.8855, -0.3999, -0.5808,\n","         -0.8750, -0.4439, -0.5478]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.7774, device='cuda:0')\n","Loss.item: 0.7773656845092773\n","During training one batch, the logits of the output is: tensor([[-0.9521, -0.9021, -0.1714, -0.4798,  7.1468, -0.8464, -0.4094, -0.6030,\n","         -0.9110, -0.4169, -0.5805],\n","        [ 0.3598,  0.3874, -0.2523, -0.9445,  0.6943, -0.8077, -1.4396,  0.5024,\n","         -0.6861,  0.6969,  1.3101],\n","        [-0.9932, -0.7242,  7.0326, -0.9410, -0.1264,  0.1690, -0.8955, -1.6408,\n","         -1.0157, -0.6215, -0.7609]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.002317876322194934\n","During training one batch, the logits of the output is: tensor([[-0.6130, -0.6388, -0.2195, -0.6136, -0.6161, -0.6520, -0.0166, -0.9356,\n","         -0.7185,  7.7846, -1.1705],\n","        [-0.4793, -0.7239, -0.2488, -0.6068, -0.5619, -0.7729, -0.0776, -0.8455,\n","         -0.7204,  7.7568, -1.1660],\n","        [-0.6054, -0.1031, -1.6149, -1.2381, -0.8608, -1.1029, -1.0130,  7.5700,\n","         -0.7188, -0.8475, -1.1557]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0043, device='cuda:0')\n","Loss.item: 0.004302723798900843\n","During training one batch, the logits of the output is: tensor([[-0.9385, -0.6268,  7.0548, -0.9567, -0.2267,  0.1286, -0.9026, -1.5743,\n","         -1.0434, -0.7447, -0.7570],\n","        [-0.9400, -0.5952,  7.0858, -0.9810, -0.3473,  0.0756, -0.8480, -1.6169,\n","         -0.8916, -0.6990, -0.8602],\n","        [-0.9372, -1.0120, -0.2784, -0.5085,  7.1662, -0.9913, -0.4317, -0.2740,\n","         -0.9523, -0.3902, -0.5953]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.6688, device='cuda:0')\n","Loss.item: 1.6687769889831543\n","During training one batch, the logits of the output is: tensor([[-1.4114,  0.3750,  2.1460, -0.9687,  3.2290, -0.6140, -1.2974,  0.1206,\n","         -2.1970, -0.4639,  0.8173],\n","        [ 7.3648, -1.3209, -0.6030, -0.4047, -0.7798, -1.1764, -0.7592, -0.1406,\n","         -0.2703, -1.1144, -1.1966]], device='cuda:0')\n","Validation loss: 0.0863819622900337\n","F1 Score (Weighted): 0.9737393820431958\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d4072af09144b09a7c8497eb6c2cf1b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=798.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 4\n","\rTraining loss: 0.06131058626818146\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.0027760800439864397\n","During training one batch, the logits of the output is: tensor([[-0.9249, -1.1167, -0.2159, -0.7373,  7.5265, -1.0910, -0.5532, -0.0764,\n","         -0.8483, -0.5105, -0.5103],\n","        [-0.6667, -0.7899, -0.3544, -0.9142, -0.7136, -0.6251, -0.0889, -0.8650,\n","         -0.4038,  8.1427, -1.0952],\n","        [-1.1769, -0.8190,  7.2943, -1.1201, -0.3192,  0.7019, -0.9097, -1.8109,\n","         -1.1423, -0.8159, -0.7682]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.003712427569553256\n","During training one batch, the logits of the output is: tensor([[-1.1959e+00, -8.0791e-01,  7.2925e+00, -1.1241e+00, -3.1469e-01,\n","          7.0400e-01, -9.1777e-01, -1.8013e+00, -1.1510e+00, -8.1429e-01,\n","         -7.6494e-01],\n","        [-4.1153e-01, -2.4291e-01, -6.2505e-01,  7.5039e+00, -3.5170e-01,\n","         -9.0303e-01, -1.4967e-02, -7.2666e-01, -5.9823e-01, -5.4702e-01,\n","         -7.0805e-02],\n","        [-5.1367e-01, -1.9931e-01, -6.4559e-01,  7.4910e+00, -3.8470e-01,\n","         -8.4461e-01, -3.1016e-03, -7.0189e-01, -5.8772e-01, -5.1227e-01,\n","         -9.2022e-02]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.0028048476669937372\n","During training one batch, the logits of the output is: tensor([[-0.4895, -0.2049, -0.6179,  7.4861, -0.3772, -0.8491, -0.0154, -0.7520,\n","         -0.5606, -0.5581, -0.0575],\n","        [ 7.6835, -1.2127, -0.4048, -0.7524, -0.4058, -1.1504, -0.8034, -0.6370,\n","         -0.7918, -0.8367, -1.1298],\n","        [-1.0521, -0.8132, -0.7091, -0.6317, -0.7932, -0.7381, -0.3664, -0.3933,\n","         -0.4844, -1.3569,  7.5622]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.002904510358348489\n","During training one batch, the logits of the output is: tensor([[-1.2259, -0.6908, -0.7650, -0.5484, -0.5927, -0.5737, -0.3451, -0.5476,\n","         -0.7445, -1.3117,  7.4682],\n","        [-0.4778, -0.1224, -0.5919,  7.4701, -0.4539, -0.7898, -0.0451, -0.5111,\n","         -0.7489, -0.6223, -0.1663],\n","        [ 7.6872, -1.2148, -0.1780, -0.4912, -0.5418, -1.1158, -0.8406, -0.8226,\n","         -0.8626, -0.8085, -1.2806]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(2.0918, device='cuda:0')\n","Loss.item: 2.0918333530426025\n","During training one batch, the logits of the output is: tensor([[-0.4383, -0.3596, -1.4961, -1.1374, -0.9161, -1.0661, -1.3254,  7.9315,\n","         -0.8509, -1.0163, -0.8918],\n","        [-0.9418, -1.1203, -0.2210, -0.5642,  7.5591, -1.0360, -0.4566, -0.3175,\n","         -0.8381, -0.5701, -0.5277],\n","        [-1.6275, -0.5818,  6.4194, -1.2972,  0.1614,  1.3517, -1.1811, -1.4567,\n","         -1.6891, -0.9621, -0.0270]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0082, device='cuda:0')\n","Loss.item: 0.008193603716790676\n","During training one batch, the logits of the output is: tensor([[-0.6413, -1.6605,  2.1740, -1.1655,  6.4197, -0.6276, -0.6352, -1.1424,\n","         -0.4053, -0.9082, -0.9409],\n","        [-0.9493, -0.9175, -0.6232, -0.5183, -0.9323, -0.7047, -0.3736, -0.5007,\n","         -0.4880, -1.3458,  7.5880],\n","        [-0.7607,  0.2228, -1.5362, -1.0856, -1.0879, -1.1497, -1.1241,  7.8846,\n","         -0.8490, -1.1774, -0.9668]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0115, device='cuda:0')\n","Loss.item: 0.01149373222142458\n","During training one batch, the logits of the output is: tensor([[ 0.1098, -0.3607, -1.5573, -1.0654, -0.7752, -1.5271, -1.3094,  7.8418,\n","         -1.0070, -0.8719, -0.7924],\n","        [-0.2595,  0.0748, -0.7099, -0.6064, -1.4654, -0.4167, -0.1435, -1.1106,\n","          6.0186, -0.6408, -0.3134],\n","        [-0.4940, -0.6030, -0.9887, -1.0751, -1.5144,  1.3799, -0.0793, -1.0249,\n","          6.1407, -0.8467, -0.7078]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0039, device='cuda:0')\n","Loss.item: 0.003942990209907293\n","During training one batch, the logits of the output is: tensor([[-0.2328, -0.6559, -0.9505, -0.8511, -1.3853,  0.1246, -0.0910, -0.7958,\n","          6.4508, -0.7700, -0.5492],\n","        [-0.4945, -0.2404, -1.4682, -1.1451, -1.0471, -1.1770, -1.0616,  7.9014,\n","         -0.8778, -1.2103, -0.8462],\n","        [-1.3416, -0.8555, -0.8938, -1.2660, -1.1957,  8.1088, -0.9960, -1.2026,\n","         -0.9103, -1.2135, -1.0608]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0015735933557152748\n","During training one batch, the logits of the output is: tensor([[-0.7433, -0.2923, -1.4851, -1.1969, -1.0200, -0.8976, -1.1832,  7.8923,\n","         -0.8430, -1.0485, -0.9223],\n","        [-0.7172, -0.0344, -1.7119, -0.9844, -1.0078, -1.3070, -1.1180,  7.9109,\n","         -0.6861, -1.0118, -0.7906],\n","        [-0.2086, -0.1227, -1.6566, -1.0786, -1.0711, -1.4211, -1.0997,  7.8801,\n","         -0.6433, -1.0440, -1.0382]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0089, device='cuda:0')\n","Loss.item: 0.00885118916630745\n","During training one batch, the logits of the output is: tensor([[-1.0655, -0.7060, -0.5658, -0.7071, -1.0145, -0.5666, -0.3923, -0.5050,\n","         -0.7065, -1.3125,  7.5498],\n","        [-0.6302, -0.2635, -0.8918, -0.3948, -0.4577, -0.8712,  5.6960, -1.5347,\n","          0.0410, -0.1167, -0.6047],\n","        [-1.1947,  7.2185, -0.8004, -0.4076, -1.3474, -0.8767, -0.5716, -0.0186,\n","         -0.7664, -0.8375, -0.9154]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0011905996361747384\n","During training one batch, the logits of the output is: tensor([[-0.4237, -0.1305, -1.5264, -1.2960, -0.8421, -1.2070, -1.3389,  7.9143,\n","         -0.9394, -0.8708, -0.9900],\n","        [-1.3563, -0.7316, -0.7546, -1.2650, -1.1959,  8.1170, -0.9904, -1.3048,\n","         -0.9653, -1.3226, -1.1126],\n","        [-1.2396, -0.8204, -0.8879, -1.2551, -1.2422,  8.1079, -0.9613, -1.2811,\n","         -0.7785, -1.4603, -1.1164]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0025443562772125006\n","During training one batch, the logits of the output is: tensor([[-1.4327, -0.7715, -0.8855, -1.2813, -1.1703,  8.1058, -0.9723, -1.2255,\n","         -0.9457, -1.4964, -0.8943],\n","        [-1.2674, -0.7610, -0.8210, -0.7091, -0.8679, -0.1040, -0.4167, -0.5520,\n","         -0.5776, -1.3237,  7.4472],\n","        [-0.4188, -0.2076, -0.6130,  7.4965, -0.3772, -0.9067, -0.0252, -0.5349,\n","         -0.6946, -0.6268, -0.0926]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.00282246433198452\n","During training one batch, the logits of the output is: tensor([[-0.3681, -0.7670,  7.4579, -0.6312, -0.4156, -0.3016, -0.7338, -1.8684,\n","         -0.8655, -0.6073, -1.1546],\n","        [-0.6251, -0.6164,  7.4598, -0.7547, -0.3969, -0.4036, -0.7327, -1.7362,\n","         -0.7676, -0.7629, -1.0564],\n","        [-0.6276, -1.0114, -0.5808, -0.6565, -1.0039, -0.4847, -0.5440, -0.4558,\n","         -0.5798, -1.4547,  7.4744]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018689966527745128\n","During training one batch, the logits of the output is: tensor([[-0.5120, -0.3570, -1.4941, -1.2316, -0.9572, -1.1658, -1.2865,  7.9304,\n","         -0.7820, -0.7266, -1.0437],\n","        [-0.3724, -0.3878, -1.5019, -1.1858, -0.8187, -1.2623, -1.3766,  7.9549,\n","         -0.7870, -0.8089, -0.9588],\n","        [-0.6188, -0.6850,  7.4828, -0.7574, -0.3450, -0.4915, -0.7662, -1.6935,\n","         -0.8056, -0.7284, -1.0073]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0046, device='cuda:0')\n","Loss.item: 0.0045797619968652725\n","During training one batch, the logits of the output is: tensor([[-0.1228, -0.8279, -0.7147, -0.7510, -1.4882, -0.2004, -0.1222, -0.7332,\n","          6.5200, -0.8926, -0.2937],\n","        [-0.3381, -0.2004, -1.5179, -1.1713, -1.0396, -1.1314, -1.3167,  7.8980,\n","         -0.7202, -1.0748, -1.1091],\n","        [-0.3126, -0.2878, -0.5840,  7.5054, -0.2704, -0.8706, -0.0938, -0.7894,\n","         -0.6444, -0.5470, -0.1131]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.002610002411529422\n","During training one batch, the logits of the output is: tensor([[-0.2371, -0.3138, -0.6217,  7.5048, -0.2629, -0.8933, -0.1115, -0.7304,\n","         -0.6637, -0.5802, -0.1160],\n","        [ 7.6951, -1.2503, -0.3433, -0.4686, -0.7324, -1.1424, -0.7183, -0.8631,\n","         -0.5731, -0.8187, -1.1503],\n","        [ 7.7217, -1.2894, -0.3363, -0.4049, -0.4741, -1.3544, -0.8435, -0.5194,\n","         -0.7717, -0.8134, -1.2685]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.001551020541228354\n","During training one batch, the logits of the output is: tensor([[-0.6525, -0.7187, -0.4176, -0.8697, -0.7991, -0.7535, -0.0645, -0.7915,\n","         -0.3615,  8.1531, -1.2077],\n","        [-0.7043, -0.7482, -0.3422, -0.7191, -0.6939, -0.7144, -0.0278, -0.5860,\n","         -0.7816,  8.1722, -1.1338],\n","        [-0.6958, -0.7422, -0.2644, -0.7222, -0.6944, -0.6777, -0.0174, -0.7317,\n","         -0.7637,  8.1631, -1.1519]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0085, device='cuda:0')\n","Loss.item: 0.008464365266263485\n","During training one batch, the logits of the output is: tensor([[-0.6353, -0.3476, -1.6538, -1.1716, -0.9764, -1.0264, -1.2322,  7.8990,\n","         -0.8147, -1.0010, -0.7655],\n","        [-0.7476, -0.2219, -0.9018, -0.3631, -0.5410, -0.6392,  5.6543, -1.6606,\n","          0.2931, -0.2106, -0.7500],\n","        [ 7.7126, -1.2399, -0.2902, -0.5627, -0.5942, -1.3082, -0.7208, -0.6851,\n","         -0.6759, -0.8193, -1.2977]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.00242213043384254\n","During training one batch, the logits of the output is: tensor([[-0.4146, -0.2722, -0.5050,  7.5010, -0.2444, -0.8497, -0.0769, -0.7337,\n","         -0.7932, -0.4979, -0.1002],\n","        [-0.5394, -0.0350, -1.5484, -1.0912, -0.9471, -1.2215, -1.2256,  7.9405,\n","         -0.7586, -1.0795, -1.0512],\n","        [ 7.6963, -1.2148, -0.2700, -0.5282, -0.7420, -1.3088, -0.6459, -0.6785,\n","         -0.6008, -0.8450, -1.3586]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.003511083545163274\n","During training one batch, the logits of the output is: tensor([[-6.9978e-02, -3.3292e-01, -5.1119e-01,  7.4998e+00, -2.7341e-01,\n","         -9.0131e-01, -1.3071e-01, -7.5924e-01, -8.0469e-01, -5.8620e-01,\n","         -1.4229e-01],\n","        [ 1.2941e-01, -3.5993e-01, -5.5983e-01,  7.4690e+00, -2.6892e-01,\n","         -9.3746e-01, -1.9954e-01, -7.2433e-01, -8.1807e-01, -6.6206e-01,\n","         -1.2151e-01],\n","        [-9.1641e-01, -1.1502e+00,  3.2996e-03, -4.1607e-01,  7.4987e+00,\n","         -9.4773e-01, -4.1938e-01, -4.4127e-01, -1.0107e+00, -7.2190e-01,\n","         -5.1178e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002106480300426483\n","During training one batch, the logits of the output is: tensor([[-0.6755, -0.8201, -0.3605, -0.7466, -0.6193, -0.8453, -0.0834, -0.5029,\n","         -0.6578,  8.1925, -1.1191],\n","        [-1.3585, -0.4824, -1.1999, -1.4161, -1.4763,  7.8958, -0.5311, -1.5726,\n","         -0.2751, -1.3254, -1.2542],\n","        [-1.0618,  7.2451, -0.7771, -0.4386, -1.5122, -0.7062, -0.4333, -0.2900,\n","         -0.7291, -0.8163, -1.1813]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0021528860088437796\n","During training one batch, the logits of the output is: tensor([[-0.9732, -0.8476, -0.4885, -0.5914, -0.8650, -0.8215, -0.3693, -0.5983,\n","         -0.5903, -1.2552,  7.5604],\n","        [-0.9426, -1.1252, -0.2613, -0.5463,  7.5501, -0.9812, -0.5021, -0.2065,\n","         -0.9192, -0.6473, -0.5026],\n","        [-1.2466, -0.8247, -1.0750, -1.2817, -1.2746,  8.0605, -0.8442, -1.2695,\n","         -0.6735, -1.2597, -1.2857]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0077, device='cuda:0')\n","Loss.item: 0.007741739973425865\n","During training one batch, the logits of the output is: tensor([[-0.6715, -0.3191, -0.7837, -0.1670, -0.4135, -0.7906,  5.7161, -1.6504,\n","         -0.0265, -0.1040, -0.7270],\n","        [-0.5055, -0.1730, -1.5796, -1.2113, -1.0858, -1.0801, -1.2512,  7.9301,\n","         -0.7306, -1.1127, -0.8560],\n","        [-0.5967, -0.1684, -1.5795, -1.1735, -1.0680, -1.0326, -1.2165,  7.9170,\n","         -0.6676, -1.0585, -0.9991]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.001559232478030026\n","During training one batch, the logits of the output is: tensor([[-0.5290, -0.3028, -1.3926, -1.1115, -0.9075, -1.2263, -1.1911,  7.8866,\n","         -0.8845, -1.1734, -0.7222],\n","        [-0.6583, -0.0280, -1.5461, -1.0844, -1.0679, -1.2197, -1.0339,  7.8670,\n","         -0.7783, -1.2176, -0.8492],\n","        [-0.6142, -0.2054, -1.4480, -1.1325, -0.9309, -1.1933, -1.2072,  7.9073,\n","         -0.8825, -1.0009, -0.8292]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0016083074733614922\n","During training one batch, the logits of the output is: tensor([[-0.7026, -0.1186, -1.5541, -1.1380, -0.9167, -1.1884, -1.1808,  7.9107,\n","         -0.8024, -0.9739, -0.8423],\n","        [-0.2192, -0.4181, -1.4860, -1.0330, -0.6688, -1.2708, -1.1999,  7.8840,\n","         -0.7922, -1.2574, -0.9576],\n","        [ 0.1373, -0.5550, -1.4619, -1.0668, -0.7542, -1.2959, -1.2834,  7.8644,\n","         -0.8791, -1.2030, -1.0078]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024865218438208103\n","During training one batch, the logits of the output is: tensor([[ 0.4182, -0.5976, -1.5018, -1.1230, -0.7311, -1.3225, -1.3026,  7.7829,\n","         -0.7156, -1.3503, -1.1518],\n","        [-0.5874, -0.6397,  7.4646, -0.6926, -0.3487, -0.4238, -0.7751, -1.8332,\n","         -0.8908, -0.6850, -1.0087],\n","        [-0.5621, -0.6729,  7.4735, -0.7721, -0.3444, -0.4362, -0.7638, -1.8141,\n","         -0.8065, -0.7339, -1.0179]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.002284163376316428\n","During training one batch, the logits of the output is: tensor([[-0.6211, -0.7812, -0.3598, -0.8625, -0.6223, -0.7136, -0.0806, -0.8373,\n","         -0.4091,  8.1709, -1.1873],\n","        [-1.0403, -0.6689, -0.5204, -0.7214, -1.0590, -0.5952, -0.4504, -0.5625,\n","         -0.5588, -1.3122,  7.5231],\n","        [-1.0194, -0.7194, -0.5263, -0.7443, -1.0752, -0.5921, -0.4135, -0.5517,\n","         -0.5166, -1.3091,  7.5171]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.002626836998388171\n","During training one batch, the logits of the output is: tensor([[-1.0836,  7.2452, -0.7020, -0.4086, -1.4353, -0.8347, -0.4153, -0.2618,\n","         -0.6626, -0.8846, -1.0977],\n","        [ 7.7274, -1.3316, -0.3739, -0.3976, -0.5030, -1.1776, -0.8321, -0.6341,\n","         -0.7574, -0.8155, -1.2668],\n","        [ 7.6071, -1.2367, -0.0340, -0.8061, -0.4350, -1.0132, -0.9006, -0.7443,\n","         -0.9483, -0.8050, -1.1840]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002111903391778469\n","During training one batch, the logits of the output is: tensor([[ 7.6674, -1.2561, -0.0751, -0.6560, -0.5188, -1.0163, -0.8537, -0.8329,\n","         -0.7704, -0.8878, -1.2726],\n","        [ 7.7299, -1.2507, -0.2801, -0.3785, -0.5656, -1.2428, -0.8141, -0.7542,\n","         -0.7792, -0.9252, -1.1590],\n","        [ 7.7093, -1.2750, -0.2255, -0.6309, -0.4740, -1.1562, -0.8598, -0.6250,\n","         -0.9063, -0.8236, -1.1604]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022641464602202177\n","During training one batch, the logits of the output is: tensor([[-1.3343, -0.8114, -1.1498, -1.3249, -1.2931,  8.0819, -0.9949, -1.1894,\n","         -0.5922, -1.2985, -1.1228],\n","        [-0.9661, -1.0846, -0.2343, -0.6502,  7.5462, -1.0035, -0.4819, -0.1101,\n","         -0.8172, -0.7428, -0.5493],\n","        [-0.9644, -1.0689, -0.2510, -0.6733,  7.5331, -0.9835, -0.4904, -0.0730,\n","         -0.8059, -0.7929, -0.5309]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018726146081462502\n","During training one batch, the logits of the output is: tensor([[-0.1772, -0.4978, -1.5110, -1.1760, -0.8724, -1.1733, -1.2774,  7.9033,\n","         -0.8019, -1.0250, -1.0108],\n","        [-1.4725, -0.8663, -0.3251, -1.4478, -1.3999,  7.9856, -1.1203, -1.6262,\n","         -0.4284, -1.4379, -0.8672],\n","        [-0.9744, -1.0090, -0.1931, -0.5743,  7.5453, -0.9979, -0.4326, -0.3887,\n","         -0.8073, -0.6619, -0.5496]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0021746112033724785\n","During training one batch, the logits of the output is: tensor([[-0.5160, -0.2929, -1.4836, -1.1303, -0.9105, -1.1133, -1.1877,  7.9277,\n","         -0.8290, -1.1432, -0.9668],\n","        [-0.2546, -0.5115, -1.4646, -1.1142, -0.8801, -1.1855, -1.1608,  7.8967,\n","         -0.7271, -1.2283, -0.9813],\n","        [-1.1143,  7.2863, -0.8697, -0.6325, -1.6356, -0.6943, -0.5180,  0.3955,\n","         -0.7054, -1.0703, -1.3072]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.002040267689153552\n","During training one batch, the logits of the output is: tensor([[-1.4340,  7.1447, -0.9855, -0.4388, -1.1745, -0.4667, -0.3179,  0.0169,\n","         -1.1483, -0.8565, -0.8716],\n","        [-1.2188, -0.8979, -0.8859, -1.2072, -1.1754,  8.0680, -1.0436, -1.2770,\n","         -0.7605, -1.3702, -1.1549],\n","        [-1.2472, -0.8652, -0.8675, -1.2363, -1.1970,  8.0817, -1.0300, -1.2613,\n","         -0.8006, -1.3593, -1.1592]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018421877175569534\n","During training one batch, the logits of the output is: tensor([[-1.2437, -0.9481, -0.8508, -1.1889, -1.1201,  8.0702, -1.0376, -1.2839,\n","         -0.8620, -1.2852, -1.1286],\n","        [-1.4274, -0.9636, -0.8912, -1.1518, -1.1000,  8.1006, -1.0298, -1.2716,\n","         -0.9326, -1.1724, -0.9984],\n","        [ 0.0808, -0.8063,  7.3270, -0.7400, -0.4649, -0.7396, -0.7043, -1.7456,\n","         -0.8242, -0.3770, -1.0968]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.003255523042753339\n","During training one batch, the logits of the output is: tensor([[-0.4368, -0.6967,  7.4376, -0.8578, -0.2913, -0.7557, -0.8164, -1.7748,\n","         -0.6653, -0.7410, -0.7873],\n","        [ 0.3048, -1.0757,  7.2212, -0.9054, -0.4373, -0.8787, -0.8017, -1.6161,\n","         -0.8878, -0.1680, -0.8510],\n","        [-0.6095, -0.7725,  7.4259, -0.9832, -0.3723,  0.0170, -0.8682, -1.7276,\n","         -1.0531, -0.7959, -0.8841]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.0029393502045422792\n","During training one batch, the logits of the output is: tensor([[-0.6246, -0.8511,  7.4360, -1.0134, -0.3267,  0.0687, -0.8852, -1.7967,\n","         -1.0355, -0.6759, -0.9331],\n","        [-0.7123, -0.8311,  7.4326, -0.9864, -0.2918,  0.0626, -0.9011, -1.7370,\n","         -1.0887, -0.6780, -0.9085],\n","        [-0.6961, -0.8884,  7.4240, -1.0019, -0.3377,  0.1602, -0.8722, -1.7189,\n","         -1.0602, -0.6585, -0.9670]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024867106694728136\n","During training one batch, the logits of the output is: tensor([[-1.0183, -0.8570, -0.6542, -0.6081, -0.9391, -0.6377, -0.3599, -0.4252,\n","         -0.5181, -1.3774,  7.6014],\n","        [-1.0658, -0.8539, -0.6343, -0.5540, -0.8765, -0.6409, -0.3347, -0.5324,\n","         -0.5116, -1.3643,  7.5998],\n","        [-1.0341, -0.8975, -0.6393, -0.5125, -0.8597, -0.6996, -0.3607, -0.4949,\n","         -0.5021, -1.3051,  7.6012]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0032, device='cuda:0')\n","Loss.item: 0.003200363367795944\n","During training one batch, the logits of the output is: tensor([[-1.0391, -0.8942, -0.6281, -0.5387, -0.8821, -0.6568, -0.3190, -0.5573,\n","         -0.4511, -1.3301,  7.5997],\n","        [-1.0536,  7.2943, -0.9390, -0.6864, -1.5164, -0.9962, -0.6392,  0.6411,\n","         -0.6750, -1.0564, -1.1353],\n","        [-1.0550,  7.3029, -0.7470, -0.5991, -1.5522, -0.9435, -0.6613,  0.3205,\n","         -0.7549, -0.9515, -1.0623]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0016913822619244456\n","During training one batch, the logits of the output is: tensor([[-0.5571, -0.2393, -1.4965, -1.1320, -0.9473, -1.0256, -1.2226,  7.9141,\n","         -0.8435, -1.2521, -0.9118],\n","        [-0.3004, -0.4066, -1.4293, -1.1426, -0.8145, -1.3268, -1.3412,  7.9303,\n","         -0.8774, -0.7574, -0.9699],\n","        [ 7.6947, -1.2706, -0.3971, -0.6084, -0.5979, -1.1321, -0.8216, -0.6058,\n","         -0.6574, -0.8757, -1.2574]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.0010173310292884707\n","During training one batch, the logits of the output is: tensor([[-1.4794, -0.6440, -0.8762, -1.2258, -1.2561,  8.1210, -1.0350, -1.2902,\n","         -0.9176, -1.2147, -1.0706],\n","        [-1.4402, -0.6358, -0.8888, -1.2393, -1.2859,  8.1158, -1.0263, -1.3151,\n","         -0.8222, -1.2548, -1.1180],\n","        [-1.4346, -0.7124, -0.9363, -1.2106, -1.2330,  8.1173, -1.0234, -1.3190,\n","         -0.8163, -1.2303, -1.1003]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0165, device='cuda:0')\n","Loss.item: 0.016542794182896614\n","During training one batch, the logits of the output is: tensor([[-2.0518,  2.3144, -1.3044, -1.2091, -1.5062,  6.5100, -0.2018, -1.7081,\n","         -0.8888, -1.3380, -1.3786],\n","        [-2.1288,  2.7177, -1.1115, -1.0822, -1.4785,  6.4386, -0.8833, -1.1977,\n","         -1.2957, -1.4702, -1.3648],\n","        [-0.5137, -0.7353,  7.4866, -0.7274, -0.3615, -0.4552, -0.7197, -1.7330,\n","         -0.8829, -0.6170, -1.0721]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002094933995977044\n","During training one batch, the logits of the output is: tensor([[ 7.6793, -1.3699, -0.4756, -0.5405, -0.6144, -1.0630, -0.7669, -0.7650,\n","         -0.6446, -0.9119, -1.0393],\n","        [ 7.6880, -1.3636, -0.4582, -0.5682, -0.5298, -1.0779, -0.7512, -0.7049,\n","         -0.7943, -0.8860, -1.0669],\n","        [ 7.6861, -1.3549, -0.4484, -0.4941, -0.5739, -1.1164, -0.7455, -0.7358,\n","         -0.7614, -0.8892, -1.0863]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0025419590529054403\n","During training one batch, the logits of the output is: tensor([[ 7.6790, -1.3067, -0.4138, -0.4846, -0.5639, -1.1318, -0.7169, -0.8656,\n","         -0.6752, -0.8396, -1.1553],\n","        [ 7.7134, -1.2644, -0.3840, -0.5505, -0.5249, -1.1588, -0.8209, -0.6691,\n","         -0.7733, -0.7870, -1.2316],\n","        [-1.0862,  7.2708, -0.6483, -0.4735, -1.4736, -0.9233, -0.5738,  0.0243,\n","         -0.6863, -0.8450, -1.1141]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0030, device='cuda:0')\n","Loss.item: 0.0029707953799515963\n","During training one batch, the logits of the output is: tensor([[-1.0721,  7.2497, -0.6078, -0.4570, -1.4646, -0.8621, -0.5763, -0.0635,\n","         -0.7071, -0.8205, -1.1587],\n","        [ 7.7189, -1.2771, -0.3831, -0.5021, -0.5126, -1.1940, -0.8441, -0.6358,\n","         -0.7722, -0.7930, -1.2430],\n","        [-0.6442, -1.1381, -0.0429, -0.7774,  7.4117, -1.0013, -0.6217, -0.0244,\n","         -0.9369, -0.8574, -0.4725]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024668530095368624\n","During training one batch, the logits of the output is: tensor([[-0.8631, -1.0887, -0.0989, -0.6548,  7.5272, -0.9276, -0.5192, -0.3756,\n","         -0.8748, -0.7011, -0.4547],\n","        [-1.1385, -0.8071, -0.6270, -0.5544, -0.7185, -0.8857, -0.2682, -0.6308,\n","         -0.1365, -1.4691,  7.4801],\n","        [-0.6362, -0.7582, -0.4180, -0.9383, -0.5677, -0.7747, -0.0924, -0.7776,\n","         -0.3263,  8.1510, -1.1741]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0015280485386028886\n","During training one batch, the logits of the output is: tensor([[-0.6947, -0.7677, -0.2855, -0.8506, -0.5659, -0.8036, -0.0724, -0.8137,\n","         -0.4903,  8.1786, -1.0743],\n","        [-0.6577, -0.7367, -0.2937, -0.8397, -0.6696, -0.7205, -0.0921, -0.8535,\n","         -0.4295,  8.1802, -1.2261],\n","        [-0.6457, -0.8013, -0.2682, -0.8144, -0.5596, -0.8195, -0.0687, -0.7569,\n","         -0.5606,  8.1916, -1.1291]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0025503954384475946\n","During training one batch, the logits of the output is: tensor([[-0.6646, -0.7838, -0.3047, -0.7871, -0.5540, -0.8375, -0.0658, -0.7396,\n","         -0.5721,  8.1856, -1.0815],\n","        [-0.4304, -0.2628, -0.5415,  7.5093, -0.3126, -0.8097, -0.0138, -0.6836,\n","         -0.7768, -0.5224, -0.0957],\n","        [-0.9965, -0.8963, -0.6576, -0.5783, -0.9725, -0.5989, -0.3532, -0.5705,\n","         -0.4651, -1.2913,  7.5919]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0129, device='cuda:0')\n","Loss.item: 0.012900705449283123\n","During training one batch, the logits of the output is: tensor([[-1.7944e+00,  1.7066e+00, -1.4677e+00, -1.0240e+00, -1.3450e+00,\n","         -6.9564e-01,  5.8670e-01,  5.2276e-03,  8.6242e-02, -1.8539e+00,\n","          5.7069e+00],\n","        [-1.3653e+00, -6.9572e-01, -1.0381e+00, -1.3240e+00, -1.2162e+00,\n","          8.1040e+00, -9.5798e-01, -1.2332e+00, -7.0809e-01, -1.4597e+00,\n","         -1.0344e+00],\n","        [-7.3418e-01, -6.8593e-01, -3.2087e-01, -7.0254e-01, -6.4528e-01,\n","         -7.5608e-01, -3.6644e-02, -7.1189e-01, -7.5734e-01,  8.1828e+00,\n","         -1.0993e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.004093141760677099\n","During training one batch, the logits of the output is: tensor([[-7.9262e-01, -7.2529e-01, -3.8154e-01, -8.1503e-01, -7.1841e-01,\n","         -6.8046e-01, -1.1794e-01, -2.9502e-01, -7.8657e-01,  8.1260e+00,\n","         -1.1228e+00],\n","        [-1.0020e+00, -8.9733e-01, -5.9739e-01, -5.4356e-01, -8.7501e-01,\n","         -7.1074e-01, -3.5077e-01, -5.7959e-01, -4.7515e-01, -1.3089e+00,\n","          7.5959e+00],\n","        [-4.3306e-01, -7.6012e-01, -8.7861e-01, -6.2215e-01, -1.1283e+00,\n","         -3.6371e-01,  7.5109e-03, -7.8647e-01,  6.5423e+00, -8.8503e-01,\n","         -3.1718e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0011867958819493651\n","During training one batch, the logits of the output is: tensor([[-0.7636, -0.2260, -1.5697, -1.2132, -1.0362, -1.0081, -1.2547,  7.9192,\n","         -0.8461, -0.9945, -0.6460],\n","        [-1.2720, -0.8918, -0.7997, -1.1977, -1.2016,  8.0974, -0.9874, -1.3811,\n","         -0.8698, -1.2146, -1.1206],\n","        [-1.2863, -0.8979, -1.0404, -1.2265, -1.2229,  8.0886, -1.0097, -1.2342,\n","         -0.7435, -1.3436, -1.0326]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0012382171116769314\n","During training one batch, the logits of the output is: tensor([[-1.2706, -0.9428, -1.0868, -1.2152, -1.1880,  8.0764, -0.9829, -1.2222,\n","         -0.7213, -1.3492, -1.0130],\n","        [-1.2925, -0.9303, -0.9623, -1.2225, -1.1112,  8.0973, -0.9800, -1.2213,\n","         -0.8908, -1.4248, -0.8823],\n","        [-0.5616, -0.7974, -0.3395, -0.6597, -0.6508, -0.9393, -0.1296, -0.7294,\n","         -0.5695,  8.1158, -1.0312]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0039, device='cuda:0')\n","Loss.item: 0.0038808423560112715\n","During training one batch, the logits of the output is: tensor([[-0.6804, -0.7643, -0.3002, -0.6665, -0.5921, -0.9587, -0.1190, -0.5943,\n","         -0.6944,  8.1410, -0.9774],\n","        [-0.3046, -0.6069, -0.9278, -0.7520, -1.3151, -0.0315,  0.0221, -0.7879,\n","          6.4711, -0.8629, -0.5302],\n","        [-1.4677, -0.9185, -0.9474, -1.1751, -1.1087,  8.1258, -1.0552, -1.2579,\n","         -0.9283, -1.2323, -0.9186]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0048, device='cuda:0')\n","Loss.item: 0.004807992372661829\n","During training one batch, the logits of the output is: tensor([[-3.4300e-01, -5.1256e-01, -1.0734e+00, -8.0007e-01, -1.2771e+00,\n","         -2.3250e-03,  4.4212e-03, -7.4644e-01,  6.4785e+00, -1.0047e+00,\n","         -3.9857e-01],\n","        [ 7.6967e+00, -1.2426e+00, -3.7450e-01, -4.4572e-01, -6.5624e-01,\n","         -1.2507e+00, -6.5387e-01, -7.4936e-01, -5.5666e-01, -8.0090e-01,\n","         -1.4054e+00],\n","        [ 7.5370e+00, -1.3705e+00, -6.4896e-01,  8.9638e-01, -6.3971e-01,\n","         -1.5555e+00, -8.2485e-01, -2.4497e-01, -7.8407e-01, -1.2838e+00,\n","         -1.3448e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.0036052477080374956\n","During training one batch, the logits of the output is: tensor([[ 7.6922, -1.2530, -0.4067, -0.4169, -0.6509, -1.2506, -0.6368, -0.7504,\n","         -0.5304, -0.7953, -1.4335],\n","        [-0.9566, -1.1363, -0.2200, -0.5039,  7.5506, -1.0234, -0.3767, -0.4601,\n","         -0.7705, -0.6080, -0.4670],\n","        [-1.0383,  7.0816, -1.0431, -0.7354, -1.7054, -0.8322, -0.6636,  1.2918,\n","         -0.7576, -1.2826, -1.3864]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0030, device='cuda:0')\n","Loss.item: 0.0030019041150808334\n","During training one batch, the logits of the output is: tensor([[-1.7940,  0.4292, -1.2599, -1.3150, -1.5854,  7.6318, -0.6724, -1.6013,\n","         -0.1326, -1.4709, -1.3313],\n","        [-1.0228,  7.2874, -0.7537, -0.5968, -1.6509, -0.8728, -0.6575,  0.3632,\n","         -0.8256, -1.0118, -1.0998],\n","        [-1.0565,  7.3117, -0.8461, -0.6114, -1.5249, -0.9456, -0.5633,  0.2927,\n","         -0.7756, -0.9778, -1.0963]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.0010220151161774993\n","During training one batch, the logits of the output is: tensor([[-1.2917, -0.7664, -0.9206, -1.1132, -1.2369,  8.1125, -0.9460, -1.3549,\n","         -0.9656, -1.2503, -1.0912],\n","        [-1.2717, -0.9031, -0.9615, -1.1361, -1.2443,  8.1039, -0.9365, -1.4032,\n","         -0.7777, -1.3364, -1.0082],\n","        [-1.3981, -0.6834, -1.1875, -1.2403, -1.2860,  8.1042, -0.9746, -1.2660,\n","         -0.7739, -1.2901, -1.0521]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022675946820527315\n","During training one batch, the logits of the output is: tensor([[-0.0338, -0.3716, -0.6735,  7.4666, -0.3198, -1.0036, -0.0090, -0.7450,\n","         -0.5387, -0.5996, -0.1628],\n","        [-0.6992, -0.2284, -1.5887, -1.1946, -0.8837, -1.0612, -1.3252,  7.9371,\n","         -0.7690, -0.7233, -1.0463],\n","        [-0.7654, -0.2082, -1.6180, -1.2316, -0.8964, -1.0313, -1.3668,  7.9263,\n","         -0.7416, -0.6146, -1.0260]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1359, device='cuda:0')\n","Loss.item: 0.13594754040241241\n","During training one batch, the logits of the output is: tensor([[-0.7644,  0.5188, -0.4582, -0.2581,  1.0487, -0.8941, -0.7395,  0.1690,\n","         -0.5294,  0.0741,  3.0144],\n","        [-1.2334,  0.7219, -1.6579, -1.0549, -1.1300, -0.9293, -1.3451,  7.6560,\n","         -0.8068, -1.3023, -0.6639],\n","        [-0.5408, -0.7142, -0.2876, -0.7008, -0.6144, -0.8422, -0.0473, -0.7737,\n","         -0.7167,  8.1888, -1.1979]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.0030840265098959208\n","During training one batch, the logits of the output is: tensor([[-0.6116, -0.7164, -0.3968, -0.7304, -0.6741, -0.7791, -0.0690, -0.4992,\n","         -0.7896,  8.1728, -1.2119],\n","        [-0.6394, -0.7384, -0.3829, -0.6724, -0.6658, -0.7894, -0.0533, -0.6201,\n","         -0.7825,  8.1655, -1.0925],\n","        [-1.0350,  7.1044, -1.1522, -0.7493, -1.6499, -0.9485, -0.7012,  1.4548,\n","         -0.8016, -1.2068, -1.2617]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0056, device='cuda:0')\n","Loss.item: 0.005596727132797241\n","During training one batch, the logits of the output is: tensor([[-0.9524,  6.9066, -1.1213, -0.6468, -1.7562, -0.9633, -0.5703,  1.6746,\n","         -0.9666, -1.3297, -1.3809],\n","        [-1.0943,  7.2772, -0.8712, -0.5487, -1.5395, -0.3902, -0.6677,  0.1200,\n","         -0.7271, -1.0962, -1.3709],\n","        [-1.0687,  7.1832, -0.9688, -0.7344, -1.6106, -0.7815, -0.7176,  1.1117,\n","         -0.8747, -1.2191, -1.3076]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0030, device='cuda:0')\n","Loss.item: 0.0030333988834172487\n","During training one batch, the logits of the output is: tensor([[-0.9976, -1.0445, -0.2361, -0.6519,  7.5499, -0.9872, -0.4901, -0.2542,\n","         -0.8144, -0.6321, -0.5491],\n","        [-1.2837,  7.2859, -0.8555, -0.6601, -1.6163, -0.3043, -0.6178,  0.2530,\n","         -0.8007, -1.0857, -1.2747],\n","        [-0.9649, -1.0687, -0.1733, -0.4401,  7.5586, -1.0504, -0.4406, -0.4468,\n","         -0.8981, -0.6257, -0.4047]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0060, device='cuda:0')\n","Loss.item: 0.006014820653945208\n","During training one batch, the logits of the output is: tensor([[-1.0044, -1.0431, -0.2671, -0.4053,  7.5368, -1.0305, -0.4401, -0.2634,\n","         -0.9025, -0.7712, -0.4315],\n","        [-0.2833, -0.7537, -0.7398, -0.7466, -1.2955, -0.3596, -0.0355, -0.7345,\n","          6.5645, -0.8742, -0.3971],\n","        [-1.2002,  6.9937, -1.0220, -0.6966, -1.4722, -0.5379, -0.8542,  1.5209,\n","         -1.1948, -1.3523, -1.1698]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0016611759783700109\n","During training one batch, the logits of the output is: tensor([[-0.7070, -0.7292, -0.2316, -0.7853, -0.6657, -0.6369, -0.1091, -0.5920,\n","         -0.7774,  8.1771, -1.2319],\n","        [-0.6594, -0.7537, -0.5120, -1.0249, -0.7550, -0.6068, -0.2675,  0.2727,\n","         -0.6384,  8.0133, -1.5485],\n","        [-0.6882, -0.7500, -0.2480, -0.8279, -0.6398, -0.6720, -0.1208, -0.4713,\n","         -0.7501,  8.1813, -1.2700]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.002039968967437744\n","During training one batch, the logits of the output is: tensor([[-0.6889, -0.7566, -0.2599, -0.8349, -0.6372, -0.6797, -0.1275, -0.4401,\n","         -0.7425,  8.1804, -1.2698],\n","        [-0.7246, -0.6903, -0.3725, -0.9599, -0.7458, -0.4616, -0.1641, -0.1605,\n","         -0.7398,  8.0668, -1.4924],\n","        [-0.7504, -0.8643,  7.4561, -0.8889, -0.2746, -0.1023, -0.7894, -1.8453,\n","         -0.9706, -0.5881, -0.9077]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0021152279805392027\n","During training one batch, the logits of the output is: tensor([[-1.3140e+00, -7.3244e-01, -1.0137e+00, -1.2854e+00, -1.1881e+00,\n","          8.0866e+00, -1.0999e+00, -1.1068e+00, -8.9806e-01, -1.3257e+00,\n","         -1.0912e+00],\n","        [-7.2524e-01, -6.8262e-02, -1.7021e+00, -1.2180e+00, -7.5099e-01,\n","         -1.2133e+00, -1.4324e+00,  7.9010e+00, -6.7768e-01, -3.8628e-01,\n","         -1.1671e+00],\n","        [-5.1143e-01, -1.9406e-01, -6.2085e-01,  7.4973e+00, -3.3573e-01,\n","         -8.6862e-01, -3.2209e-03, -6.1445e-01, -7.0907e-01, -4.8886e-01,\n","         -9.1704e-02]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0012403931468725204\n","During training one batch, the logits of the output is: tensor([[-0.7655, -0.6950, -0.2654, -0.6795, -0.4007, -0.8800, -0.0908, -0.5414,\n","         -0.8596,  8.1212, -1.0396],\n","        [-1.2817, -0.7075, -0.8122, -1.2683, -1.2801,  8.1030, -0.9856, -1.3434,\n","         -0.8407, -1.4585, -1.1559],\n","        [-1.1816, -0.9263, -0.9008, -1.2239, -1.2082,  8.0979, -1.0150, -1.3547,\n","         -0.7262, -1.3527, -1.0744]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.0010259051341563463\n","During training one batch, the logits of the output is: tensor([[-1.1674, -0.8174, -0.8450, -1.2322, -1.2577,  8.0877, -1.0143, -1.4221,\n","         -0.7604, -1.3431, -1.1286],\n","        [-1.3473, -0.7734, -0.9092, -1.2150, -1.1903,  8.1082, -1.0539, -1.2010,\n","         -0.9193, -1.3277, -1.0937],\n","        [-1.3889, -0.7632, -1.0001, -1.2662, -1.1767,  8.1078, -1.0366, -1.1173,\n","         -0.9226, -1.3803, -1.0102]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0016449004178866744\n","During training one batch, the logits of the output is: tensor([[-1.4243, -0.7548, -0.9078, -1.2110, -1.1271,  8.0997, -1.0927, -1.1422,\n","         -1.0105, -1.3271, -1.0104],\n","        [-0.9251, -1.0270, -0.1552, -0.4661,  7.5427, -0.9765, -0.4469, -0.6197,\n","         -0.8833, -0.5450, -0.4126],\n","        [-1.4508, -0.6552, -0.9931, -1.2156, -1.2128,  8.1053, -1.0087, -1.1817,\n","         -0.9472, -1.2942, -1.0660]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.002939780242741108\n","During training one batch, the logits of the output is: tensor([[-1.2222,  7.2110, -0.6471, -0.3852, -1.3026, -0.6093, -0.5229, -0.3268,\n","         -0.8347, -0.8086, -1.0290],\n","        [-1.1789,  7.1753, -0.6575, -0.3048, -1.3193, -0.7662, -0.4274, -0.4427,\n","         -0.7783, -0.7457, -1.0038],\n","        [-0.4258, -0.2953, -1.3994, -1.2008, -0.9971, -1.0772, -1.2054,  7.9021,\n","         -0.9113, -1.1151, -0.9985]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022660086397081614\n","During training one batch, the logits of the output is: tensor([[-0.5770, -0.2217, -1.4955, -1.1531, -0.9294, -1.1305, -1.1681,  7.9342,\n","         -0.8701, -1.0445, -0.9555],\n","        [-0.8855,  0.1458, -1.5541, -1.0791, -1.0156, -0.9938, -1.2164,  7.8789,\n","         -0.7892, -1.1790, -1.0225],\n","        [-0.4156, -0.2520, -0.6442,  7.4883, -0.3174, -0.8330,  0.1010, -0.6906,\n","         -0.6263, -0.5893, -0.1783]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.00217447173781693\n","During training one batch, the logits of the output is: tensor([[-1.2891, -0.7162, -0.9806, -1.2481, -1.2892,  8.1051, -0.9713, -1.2822,\n","         -0.7648, -1.3546, -1.1664],\n","        [-1.2877, -0.8522, -0.9578, -1.2633, -1.2794,  8.0932, -0.9727, -1.3222,\n","         -0.7053, -1.2520, -1.1509],\n","        [-1.0076,  7.2019, -0.9483, -0.7303, -1.6152, -0.7832, -0.7758,  0.9434,\n","         -0.6855, -1.1784, -1.2936]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0020897667855024338\n","During training one batch, the logits of the output is: tensor([[-1.3526, -0.8265, -0.9134, -1.1877, -1.2168,  8.1210, -1.0568, -1.2960,\n","         -0.8825, -1.2290, -1.0374],\n","        [-0.7483, -0.2483, -1.6729, -1.3241, -0.8997, -1.0900, -1.3780,  7.8503,\n","         -0.7254, -0.2875, -1.0827],\n","        [-0.5214, -0.4753,  7.2257, -0.7453, -0.0976, -0.7612, -0.8902, -1.3549,\n","         -1.2264, -0.7933, -0.7372]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0026018747594207525\n","During training one batch, the logits of the output is: tensor([[ 7.7171, -1.3387, -0.3489, -0.4954, -0.5019, -1.2232, -0.8610, -0.5524,\n","         -0.7154, -0.9629, -1.2426],\n","        [-0.4017, -0.2980, -0.5086,  7.4924, -0.2071, -0.8684, -0.0564, -0.8610,\n","         -0.6222, -0.4976, -0.1398],\n","        [ 7.7073, -1.2693, -0.3939, -0.5536, -0.5821, -1.0798, -0.8197, -0.6737,\n","         -0.6507, -0.9301, -1.2463]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022945364471524954\n","During training one batch, the logits of the output is: tensor([[-1.3549, -0.6170, -0.9815, -1.2725, -1.2937,  8.0982, -1.0124, -1.2577,\n","         -0.7062, -1.4067, -1.1830],\n","        [-1.1998,  7.2113, -0.6405, -0.5626, -1.4841, -0.2881, -0.4350, -0.1916,\n","         -0.8358, -1.1020, -1.2980],\n","        [ 7.6527, -1.3354, -0.4186, -0.7398, -0.5498, -1.2443, -0.9825, -0.1828,\n","         -0.9109, -1.1167, -0.6328]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.002354290336370468\n","During training one batch, the logits of the output is: tensor([[-0.3146, -0.7526,  7.4574, -0.6012, -0.3755, -0.5846, -0.7331, -1.8218,\n","         -0.7462, -0.6518, -1.0880],\n","        [-0.5000, -0.7152,  7.4759, -0.5697, -0.3490, -0.5301, -0.7461, -1.8169,\n","         -0.8613, -0.6195, -1.0098],\n","        [-1.1010, -0.6573, -1.0230, -1.4049, -1.2171,  7.8484, -0.5103, -1.6182,\n","         -0.7355, -1.3135, -1.0920]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0012079409789294004\n","During training one batch, the logits of the output is: tensor([[-1.4138, -0.8423, -0.9577, -1.2217, -1.2123,  8.1010, -1.0032, -1.4035,\n","         -0.6875, -1.1808, -1.0652],\n","        [-1.3979, -0.9690, -0.9361, -1.1618, -1.1421,  8.1015, -1.0295, -1.2740,\n","         -0.8573, -1.1880, -1.0153],\n","        [-0.4219, -0.8106, -0.4864, -0.7425, -0.6631, -0.9929, -0.1226, -0.4013,\n","         -0.5514,  8.1615, -1.2812]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.0012567209778353572\n","During training one batch, the logits of the output is: tensor([[-0.3154, -0.8425, -0.5134, -0.8043, -0.6704, -1.0407, -0.1142, -0.3563,\n","         -0.4852,  8.1564, -1.3173],\n","        [-1.1769, -0.8387, -0.9594, -1.2826, -1.2564,  8.0590, -1.0108, -1.3774,\n","         -0.6013, -1.2036, -1.1660],\n","        [-1.2407, -0.8806, -0.9718, -1.2712, -1.1840,  8.0640, -1.0614, -1.3650,\n","         -0.6339, -1.1384, -1.0996]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0019202064722776413\n","During training one batch, the logits of the output is: tensor([[-1.2686, -0.8299, -0.9545, -1.2791, -1.3450,  8.0728, -1.0257, -1.3241,\n","         -0.5849, -1.3221, -1.1030],\n","        [-0.9660,  7.2439, -0.8039, -0.6712, -1.7312, -0.6343, -0.5995,  0.3412,\n","         -0.5871, -1.1442, -1.3890],\n","        [-1.2243, -0.7779, -0.9561, -1.2410, -1.2874,  8.0947, -1.0204, -1.2838,\n","         -0.6773, -1.3929, -1.1800]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0017159780254587531\n","During training one batch, the logits of the output is: tensor([[-1.1652, -0.7142, -0.9432, -1.2576, -1.2687,  8.0816, -1.0063, -1.2258,\n","         -0.8411, -1.3884, -1.1766],\n","        [-0.6819, -0.7224, -0.2746, -0.6813, -0.7016, -0.6870, -0.0390, -0.8517,\n","         -0.7665,  8.1690, -1.0854],\n","        [-1.1793, -0.7289, -0.5413, -0.6752, -0.9466, -0.5049, -0.3662, -0.5793,\n","         -0.6070, -1.2744,  7.5684]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.002569827251136303\n","During training one batch, the logits of the output is: tensor([[-1.1595, -0.8183, -0.5997, -0.6512, -0.8706, -0.3686, -0.4618, -0.5550,\n","         -0.6237, -1.3137,  7.5473],\n","        [-1.1061, -0.7598, -0.6493, -0.6957, -0.9983, -0.4660, -0.4193, -0.5256,\n","         -0.4983, -1.3267,  7.5576],\n","        [-1.1110, -0.7504, -0.5156, -0.7196, -0.9852, -0.5561, -0.4416, -0.4410,\n","         -0.6721, -1.2633,  7.5701]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0064, device='cuda:0')\n","Loss.item: 0.006409039255231619\n","During training one batch, the logits of the output is: tensor([[-1.2354, -0.4522, -0.9038, -1.4236, -1.3901,  8.0098, -0.9617, -1.5643,\n","         -0.6084, -1.3142, -1.0179],\n","        [-0.1732, -0.8145, -0.7609, -0.8231, -1.1975, -0.1119, -0.1755, -0.6142,\n","          6.4397, -0.9640, -0.5525],\n","        [-0.2349, -0.7250, -0.9301, -0.8767, -1.0068, -0.3509, -0.2181, -0.5136,\n","          6.4377, -1.0044, -0.3748]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0095, device='cuda:0')\n","Loss.item: 0.009515210054814816\n","During training one batch, the logits of the output is: tensor([[-0.3255, -0.7225, -0.6987, -0.8417, -1.3586, -0.2150, -0.0113, -0.6446,\n","          6.4759, -0.8512, -0.5618],\n","        [-0.3235, -0.7356, -0.7113, -0.8472, -1.3602, -0.2258, -0.0106, -0.6451,\n","          6.4837, -0.7962, -0.5668],\n","        [-0.2890, -0.6912, -0.6020, -0.8861, -1.2582, -0.5270,  0.0428, -0.5289,\n","          6.2445, -0.5022, -0.8457]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018575392896309495\n","During training one batch, the logits of the output is: tensor([[-1.4071e+00, -7.9006e-01, -1.0325e+00, -1.2346e+00, -1.2143e+00,\n","          8.1224e+00, -9.6243e-01, -1.3400e+00, -8.1097e-01, -1.2538e+00,\n","         -1.0204e+00],\n","        [-1.2141e+00, -6.8326e-01, -9.8279e-01, -1.2392e+00, -1.4190e+00,\n","          8.0811e+00, -9.7105e-01, -1.4277e+00, -5.9111e-01, -1.4500e+00,\n","         -1.0677e+00],\n","        [-1.1212e+00,  7.2679e+00, -8.2348e-01, -5.1096e-01, -1.7042e+00,\n","         -6.7091e-01, -2.5694e-01,  4.6552e-03, -6.5084e-01, -9.2750e-01,\n","         -1.2897e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014702953631058335\n","During training one batch, the logits of the output is: tensor([[-1.1549, -0.6466, -0.9307, -1.3218, -1.3304,  8.0165, -0.7153, -1.4469,\n","         -0.7122, -1.2537, -1.3130],\n","        [-0.3022, -0.2073, -1.7168, -1.2133, -0.6241, -1.2169, -1.4004,  7.8909,\n","         -0.7538, -1.0620, -0.9882],\n","        [-0.4354, -0.2425, -1.7738, -1.2539, -0.7926, -1.0929, -1.2601,  7.8576,\n","         -0.7061, -1.2834, -0.6659]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.002412472851574421\n","During training one batch, the logits of the output is: tensor([[-0.2019, -0.2732, -1.5124, -1.2566, -0.8791, -1.2057, -1.3612,  7.9144,\n","         -0.9150, -1.0474, -0.9086],\n","        [-0.9772, -1.0038, -0.1766, -0.5105,  7.5502, -0.9682, -0.4415, -0.5363,\n","         -0.8738, -0.5837, -0.4463],\n","        [-0.9905, -1.0116, -0.1452, -0.5360,  7.5403, -0.9137, -0.4307, -0.6029,\n","         -0.7852, -0.6325, -0.4698]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.2164, device='cuda:0')\n","Loss.item: 0.21643047034740448\n","During training one batch, the logits of the output is: tensor([[-0.9724, -0.9994, -0.1584, -0.5286,  7.5373, -0.8697, -0.4306, -0.6435,\n","         -0.8021, -0.6147, -0.5013],\n","        [ 2.6180,  0.2013, -0.5992, -0.7058, -0.1960, -1.1997, -1.3119, -0.3837,\n","          0.3530, -0.1286,  1.7469],\n","        [-0.8693, -0.8309,  7.4167, -0.8684, -0.1907, -0.0154, -0.8892, -1.7834,\n","         -0.9637, -0.6691, -0.8679]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0016143792308866978\n","During training one batch, the logits of the output is: tensor([[-0.6017, -0.7126, -0.2281, -0.6765, -0.7081, -0.7379, -0.0741, -0.9942,\n","         -0.6226,  8.1424, -1.0958],\n","        [-0.4575, -0.7958, -0.2489, -0.6583, -0.6586, -0.8613, -0.1305, -0.9077,\n","         -0.6317,  8.1244, -1.0891],\n","        [-0.2621, -0.3423, -1.5426, -1.3315, -1.0413, -1.1210, -1.0457,  7.8365,\n","         -0.7093, -1.0704, -1.1817]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.0028080008924007416\n","During training one batch, the logits of the output is: tensor([[-0.7894, -0.7624,  7.4359, -0.9172, -0.2645, -0.0475, -0.9179, -1.6967,\n","         -1.0587, -0.7825, -0.8250],\n","        [-0.8764, -0.7190,  7.4660, -0.9133, -0.3358, -0.1382, -0.8558, -1.7400,\n","         -0.9287, -0.7078, -0.8727],\n","        [-0.9495, -1.1166, -0.2683, -0.6053,  7.5522, -1.0306, -0.4849, -0.2082,\n","         -0.8757, -0.5718, -0.5371]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.0206, device='cuda:0')\n","Loss.item: 1.0205657482147217\n","During training one batch, the logits of the output is: tensor([[-0.9258,  1.1693, -0.2695, -0.7365,  1.1672, -0.7766, -0.9036, -0.1665,\n","         -0.4760,  0.0575,  2.5859],\n","        [ 7.7200, -1.3616, -0.5049, -0.4136, -0.5579, -1.1661, -0.7893, -0.5862,\n","         -0.5986, -0.9939, -1.1768]], device='cuda:0')\n","Validation loss: 0.0420190173938996\n","F1 Score (Weighted): 0.9924781865705603\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b1b58a6327b47efb589608cf7645a72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=798.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r\n","Epoch 5\n","\rTraining loss: 0.048088318735558334\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.002357172081246972\n","During training one batch, the logits of the output is: tensor([[-0.9193, -1.1662, -0.2090, -0.7000,  7.6527, -1.0946, -0.5439, -0.1330,\n","         -0.9061, -0.4784, -0.5357],\n","        [-0.7113, -0.8246, -0.3583, -0.9164, -0.6891, -0.6408, -0.1011, -0.8243,\n","         -0.4418,  8.2758, -1.0999],\n","        [-1.2134, -0.9247,  7.4367, -1.1273, -0.2718,  0.5534, -0.8972, -1.7185,\n","         -1.1419, -0.8066, -0.7819]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.003148164600133896\n","During training one batch, the logits of the output is: tensor([[-1.2322, -0.9149,  7.4358, -1.1322, -0.2670,  0.5547, -0.9042, -1.7077,\n","         -1.1475, -0.8058, -0.7812],\n","        [-0.4013, -0.2715, -0.6599,  7.6403, -0.3560, -0.9039, -0.0276, -0.6661,\n","         -0.6195, -0.5579, -0.1319],\n","        [-0.4921, -0.2286, -0.6790,  7.6253, -0.4006, -0.8419, -0.0172, -0.6373,\n","         -0.6098, -0.5243, -0.1610]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.002432886278256774\n","During training one batch, the logits of the output is: tensor([[-0.4697, -0.2337, -0.6508,  7.6241, -0.3935, -0.8443, -0.0290, -0.6939,\n","         -0.5834, -0.5682, -0.1243],\n","        [ 7.8141, -1.3198, -0.4414, -0.7295, -0.3520, -1.2171, -0.8156, -0.5771,\n","         -0.7332, -0.8814, -1.1740],\n","        [-1.0850, -0.8818, -0.7071, -0.6224, -0.7701, -0.7736, -0.3869, -0.3699,\n","         -0.5340, -1.3302,  7.6853]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0025120300706475973\n","During training one batch, the logits of the output is: tensor([[-1.2068, -0.7760, -0.7480, -0.5461, -0.6096, -0.6079, -0.3738, -0.5119,\n","         -0.8085, -1.2990,  7.6191],\n","        [-0.4754, -0.1611, -0.6071,  7.5917, -0.4511, -0.7786, -0.0613, -0.4478,\n","         -0.7927, -0.6166, -0.2229],\n","        [ 7.8192, -1.2963, -0.2098, -0.4856, -0.5073, -1.1630, -0.8588, -0.7691,\n","         -0.8065, -0.8576, -1.3194]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.9894, device='cuda:0')\n","Loss.item: 1.9894332885742188\n","During training one batch, the logits of the output is: tensor([[-0.5347, -0.5196, -1.5065, -1.1309, -0.8234, -1.0739, -1.3343,  8.0383,\n","         -0.8535, -0.9072, -0.8942],\n","        [-0.9359, -1.1654, -0.2075, -0.5448,  7.6799, -1.0392, -0.4551, -0.3533,\n","         -0.9019, -0.5377, -0.5497],\n","        [-1.6221, -0.7383,  6.5014, -1.2902,  0.5472,  0.7049, -1.1574, -1.2425,\n","         -1.6876, -0.9404,  0.0786]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0046, device='cuda:0')\n","Loss.item: 0.00457985932007432\n","During training one batch, the logits of the output is: tensor([[-0.8013, -1.6182,  1.7781, -1.0501,  6.8621, -0.7438, -0.5962, -1.0264,\n","         -0.5425, -0.7602, -0.8306],\n","        [-0.9726, -0.9789, -0.6122, -0.4920, -0.8838, -0.7699, -0.4034, -0.4682,\n","         -0.5392, -1.3254,  7.7070],\n","        [-0.8090, -0.1198, -1.5138, -1.0874, -0.9085, -1.1603, -1.1688,  8.0239,\n","         -0.9023, -1.0598, -0.8808]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0087, device='cuda:0')\n","Loss.item: 0.008699971251189709\n","During training one batch, the logits of the output is: tensor([[-0.2243, -0.5616, -1.5785, -1.0733, -0.5713, -1.4714, -1.3306,  7.9484,\n","         -1.0174, -0.6296, -0.7090],\n","        [-0.2736, -0.2687, -0.6855, -0.6458, -1.3398, -0.4471, -0.2468, -1.0492,\n","          6.2307, -0.6580, -0.2441],\n","        [-0.4974, -0.7500, -0.9819, -1.0552, -1.4594,  1.1745, -0.1209, -0.9778,\n","          6.3451, -0.8490, -0.6629]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.003317425260320306\n","During training one batch, the logits of the output is: tensor([[-0.2775, -0.7792, -0.9504, -0.8515, -1.3252,  0.0154, -0.1496, -0.7570,\n","          6.6002, -0.7781, -0.4978],\n","        [-0.6675, -0.4227, -1.4664, -1.1263, -0.9130, -1.1558, -1.0960,  8.0052,\n","         -0.9061, -1.0837, -0.7596],\n","        [-1.3857, -0.9954, -0.8758, -1.2779, -1.1571,  8.2336, -1.0229, -1.1979,\n","         -0.9275, -1.2499, -0.9729]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.0013477058382704854\n","During training one batch, the logits of the output is: tensor([[-0.8023, -0.4680, -1.4855, -1.1836, -0.9072, -0.9151, -1.2100,  7.9972,\n","         -0.8716, -0.9476, -0.9062],\n","        [-0.7780, -0.2676, -1.7069, -1.0102, -0.8658, -1.3076, -1.1644,  8.0222,\n","         -0.7428, -0.9019, -0.7333],\n","        [-0.2825, -0.3530, -1.6802, -1.1129, -0.9490, -1.4013, -1.1456,  8.0231,\n","         -0.6669, -0.9622, -1.0139]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0076, device='cuda:0')\n","Loss.item: 0.007570577319711447\n","During training one batch, the logits of the output is: tensor([[-1.0639, -0.7969, -0.5606, -0.6639, -0.9645, -0.6423, -0.4350, -0.4412,\n","         -0.7736, -1.2967,  7.6791],\n","        [-0.5386, -0.5008, -0.9187, -0.2531, -0.3871, -0.8848,  5.8292, -1.3357,\n","         -0.3470, -0.1025, -0.5917],\n","        [-1.2781,  7.3312, -0.9330, -0.4953, -1.3692, -0.9022, -0.6126,  0.3385,\n","         -0.8551, -0.9415, -0.9004]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.0010303696617484093\n","During training one batch, the logits of the output is: tensor([[-0.5783, -0.3247, -1.5487, -1.2767, -0.7420, -1.1827, -1.3360,  8.0311,\n","         -0.9238, -0.7387, -0.9890],\n","        [-1.4028, -0.9198, -0.6970, -1.2683, -1.1306,  8.2438, -1.0251, -1.2996,\n","         -1.0094, -1.3540, -1.0089],\n","        [-1.2858, -0.9778, -0.8519, -1.2612, -1.1870,  8.2363, -0.9917, -1.2872,\n","         -0.8079, -1.5004, -1.0223]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0021650760900229216\n","During training one batch, the logits of the output is: tensor([[-1.5258, -0.9464, -0.8385, -1.2743, -1.0842,  8.2122, -1.0084, -1.1862,\n","         -1.0244, -1.5275, -0.7517],\n","        [-1.2276, -0.8488, -0.8040, -0.6742, -0.8444, -0.2753, -0.4260, -0.5060,\n","         -0.6182, -1.2998,  7.6226],\n","        [-0.4173, -0.2429, -0.6302,  7.6288, -0.3804, -0.8976, -0.0404, -0.4804,\n","         -0.7287, -0.6256, -0.1472]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.0024362883996218443\n","During training one batch, the logits of the output is: tensor([[-0.4229, -0.8696,  7.5760, -0.6567, -0.4181, -0.3316, -0.7274, -1.8218,\n","         -0.8360, -0.5847, -1.1841],\n","        [-0.6825, -0.7371,  7.5722, -0.7872, -0.3847, -0.4227, -0.7376, -1.6528,\n","         -0.7538, -0.7389, -1.0992],\n","        [-0.7890, -1.0378, -0.5449, -0.6016, -0.9135, -0.5913, -0.5260, -0.4599,\n","         -0.6619, -1.3820,  7.6306]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0016565431142225862\n","During training one batch, the logits of the output is: tensor([[-0.6266, -0.5122, -1.5193, -1.2280, -0.8591, -1.1526, -1.3000,  8.0270,\n","         -0.7779, -0.6288, -1.0327],\n","        [-0.5151, -0.5354, -1.5213, -1.1938, -0.7163, -1.2506, -1.3887,  8.0537,\n","         -0.7818, -0.6722, -0.9642],\n","        [-0.6828, -0.7953,  7.5897, -0.7900, -0.3353, -0.5059, -0.7651, -1.6037,\n","         -0.7878, -0.7141, -1.0588]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0039, device='cuda:0')\n","Loss.item: 0.003922984004020691\n","During training one batch, the logits of the output is: tensor([[-0.2149, -0.9011, -0.7155, -0.7629, -1.4502, -0.2863, -0.1566, -0.6811,\n","          6.6497, -0.9169, -0.2459],\n","        [-0.4816, -0.3961, -1.5358, -1.1681, -0.9222, -1.1305, -1.3268,  8.0295,\n","         -0.7365, -0.9329, -1.0843],\n","        [-0.3101, -0.3113, -0.6216,  7.6408, -0.2740, -0.8757, -0.1001, -0.7253,\n","         -0.6617, -0.5552, -0.1779]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022805174812674522\n","During training one batch, the logits of the output is: tensor([[-0.2423, -0.3366, -0.6583,  7.6401, -0.2646, -0.8951, -0.1151, -0.6679,\n","         -0.6800, -0.5874, -0.1817],\n","        [ 7.8207, -1.3310, -0.3813, -0.4679, -0.7026, -1.1950, -0.7383, -0.7886,\n","         -0.5281, -0.8645, -1.1635],\n","        [ 7.8392, -1.3792, -0.3918, -0.4133, -0.4321, -1.3974, -0.8763, -0.3956,\n","         -0.7280, -0.8581, -1.3188]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.0013459996553137898\n","During training one batch, the logits of the output is: tensor([[-0.6915, -0.7567, -0.4252, -0.8843, -0.7740, -0.7555, -0.0771, -0.7431,\n","         -0.3906,  8.2871, -1.2273],\n","        [-0.7245, -0.7750, -0.3650, -0.7461, -0.6874, -0.7172, -0.0448, -0.5582,\n","         -0.7664,  8.3028, -1.1813],\n","        [-0.7113, -0.7638, -0.2872, -0.7510, -0.6962, -0.6753, -0.0345, -0.6999,\n","         -0.7539,  8.2923, -1.2066]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0070, device='cuda:0')\n","Loss.item: 0.007021391298621893\n","During training one batch, the logits of the output is: tensor([[-0.7241, -0.5013, -1.6453, -1.1718, -0.8545, -1.0327, -1.2589,  8.0029,\n","         -0.8423, -0.9086, -0.7562],\n","        [-0.6265, -0.4475, -0.9005, -0.2063, -0.5080, -0.6629,  5.7972, -1.4997,\n","         -0.0528, -0.1975, -0.8066],\n","        [ 7.8329, -1.3313, -0.3383, -0.5429, -0.5529, -1.3609, -0.7346, -0.5945,\n","         -0.6261, -0.8647, -1.3460]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0021106067579239607\n","During training one batch, the logits of the output is: tensor([[-0.4221, -0.2943, -0.5315,  7.6305, -0.2414, -0.8525, -0.0848, -0.6812,\n","         -0.8205, -0.5003, -0.1525],\n","        [-0.6306, -0.2720, -1.5473, -1.0830, -0.8245, -1.1998, -1.2613,  8.0586,\n","         -0.8008, -0.9478, -1.0163],\n","        [ 7.8142, -1.3117, -0.3222, -0.4933, -0.7045, -1.3635, -0.6550, -0.5884,\n","         -0.5506, -0.8963, -1.3966]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.0030594717245548964\n","During training one batch, the logits of the output is: tensor([[-1.0512e-01, -3.5418e-01, -5.3861e-01,  7.6260e+00, -2.5375e-01,\n","         -9.0385e-01, -1.3957e-01, -7.0070e-01, -8.3597e-01, -5.8348e-01,\n","         -1.8374e-01],\n","        [ 7.3400e-02, -3.8430e-01, -5.9072e-01,  7.5980e+00, -2.3368e-01,\n","         -9.3745e-01, -2.0609e-01, -6.6498e-01, -8.5309e-01, -6.5584e-01,\n","         -1.5975e-01],\n","        [-9.3274e-01, -1.1812e+00, -2.1393e-03, -4.2460e-01,  7.6216e+00,\n","         -9.5094e-01, -4.1974e-01, -4.7517e-01, -1.0452e+00, -6.6480e-01,\n","         -5.4388e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018262626836076379\n","During training one batch, the logits of the output is: tensor([[-7.0784e-01, -8.4482e-01, -3.7244e-01, -7.6289e-01, -6.1237e-01,\n","         -8.4069e-01, -9.6241e-02, -4.7764e-01, -6.6272e-01,  8.3142e+00,\n","         -1.1540e+00],\n","        [-1.3990e+00, -6.9426e-01, -1.2240e+00, -1.4339e+00, -1.4488e+00,\n","          8.0480e+00, -5.6341e-01, -1.5597e+00, -2.6796e-01, -1.3901e+00,\n","         -1.0961e+00],\n","        [-1.1162e+00,  7.3794e+00, -8.9863e-01, -5.4835e-01, -1.6171e+00,\n","         -6.0798e-01, -4.4767e-01, -7.4151e-03, -8.0979e-01, -9.5716e-01,\n","         -1.2585e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018825255101546645\n","During training one batch, the logits of the output is: tensor([[-0.9874, -0.9140, -0.4999, -0.5667, -0.8288, -0.8690, -0.4050, -0.5423,\n","         -0.6418, -1.2446,  7.6822],\n","        [-0.9347, -1.1677, -0.2442, -0.5309,  7.6687, -0.9941, -0.4991, -0.2437,\n","         -0.9836, -0.6048, -0.5252],\n","        [-1.2787, -0.9752, -1.0669, -1.2891, -1.2348,  8.2036, -0.8843, -1.2602,\n","         -0.7060, -1.3116, -1.1820]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0066, device='cuda:0')\n","Loss.item: 0.006605637725442648\n","During training one batch, the logits of the output is: tensor([[-0.5826, -0.5193, -0.7992, -0.0427, -0.3773, -0.7785,  5.8431, -1.4759,\n","         -0.3859, -0.0929, -0.7527],\n","        [-0.6661, -0.3821, -1.5869, -1.2025, -0.9887, -1.0599, -1.2555,  8.0446,\n","         -0.7318, -1.0083, -0.7918],\n","        [-0.7007, -0.3779, -1.5822, -1.1685, -0.9653, -1.0254, -1.2369,  8.0359,\n","         -0.6860, -0.9452, -0.9668]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0014, device='cuda:0')\n","Loss.item: 0.0013567536370828748\n","During training one batch, the logits of the output is: tensor([[-0.5990, -0.4942, -1.3975, -1.1024, -0.7847, -1.2322, -1.2205,  7.9884,\n","         -0.9210, -1.0627, -0.6857],\n","        [-0.7212, -0.3189, -1.5350, -1.0844, -0.9005, -1.2403, -1.0883,  7.9965,\n","         -0.8480, -1.0611, -0.7540],\n","        [-0.6743, -0.4108, -1.4597, -1.1419, -0.8095, -1.2223, -1.2337,  8.0082,\n","         -0.9033, -0.8424, -0.8001]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0014, device='cuda:0')\n","Loss.item: 0.001384967123158276\n","During training one batch, the logits of the output is: tensor([[-0.7608, -0.3489, -1.5568, -1.1409, -0.7742, -1.2209, -1.2119,  8.0168,\n","         -0.8306, -0.8163, -0.8116],\n","        [-0.3445, -0.5684, -1.4989, -1.0406, -0.5576, -1.2542, -1.2344,  7.9995,\n","         -0.7941, -1.1329, -0.9632],\n","        [-0.0645, -0.6738, -1.4833, -1.0724, -0.6555, -1.2645, -1.3061,  7.9972,\n","         -0.8682, -1.0695, -1.0185]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.00211463775485754\n","During training one batch, the logits of the output is: tensor([[ 0.1253, -0.7189, -1.5203, -1.1382, -0.6073, -1.2661, -1.3239,  7.9496,\n","         -0.6930, -1.2117, -1.1592],\n","        [-0.6241, -0.7424,  7.5813, -0.7202, -0.3638, -0.4326, -0.7680, -1.7754,\n","         -0.8786, -0.6708, -1.0590],\n","        [-0.5958, -0.7844,  7.5891, -0.7960, -0.3444, -0.4610, -0.7612, -1.7535,\n","         -0.7844, -0.7260, -1.0635]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.0019599308725446463\n","During training one batch, the logits of the output is: tensor([[-0.6533, -0.8138, -0.3693, -0.8741, -0.5983, -0.7195, -0.0993, -0.8096,\n","         -0.4250,  8.2959, -1.2206],\n","        [-1.0488, -0.7630, -0.5268, -0.6817, -1.0059, -0.6689, -0.4881, -0.4907,\n","         -0.6307, -1.3007,  7.6643],\n","        [-1.0326, -0.8080, -0.5310, -0.7017, -1.0285, -0.6695, -0.4562, -0.4764,\n","         -0.6025, -1.2906,  7.6616]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0023010552395135164\n","During training one batch, the logits of the output is: tensor([[-1.1377,  7.3895, -0.8420, -0.5004, -1.4979, -0.8657, -0.4148,  0.0693,\n","         -0.7302, -1.0132, -1.1345],\n","        [ 7.8447, -1.4104, -0.4242, -0.3896, -0.4831, -1.2162, -0.8536, -0.5395,\n","         -0.6972, -0.8596, -1.3189],\n","        [ 7.7401, -1.3316, -0.0561, -0.8162, -0.3933, -1.0620, -0.9248, -0.6798,\n","         -0.8989, -0.8480, -1.2136]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.001859047100879252\n","During training one batch, the logits of the output is: tensor([[ 7.7903, -1.3436, -0.1027, -0.6478, -0.4746, -1.0627, -0.8751, -0.7839,\n","         -0.7303, -0.9324, -1.2946],\n","        [ 7.8500, -1.3267, -0.3247, -0.3558, -0.5238, -1.2928, -0.8353, -0.6898,\n","         -0.7452, -0.9735, -1.1835],\n","        [ 7.8348, -1.3554, -0.2650, -0.6308, -0.4333, -1.2080, -0.8866, -0.5515,\n","         -0.8678, -0.8642, -1.1906]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.0019720804411917925\n","During training one batch, the logits of the output is: tensor([[-1.3671, -0.9622, -1.1592, -1.3489, -1.2724,  8.1999, -1.0242, -1.1983,\n","         -0.5581, -1.3586, -1.0122],\n","        [-0.9575, -1.1393, -0.2244, -0.6309,  7.6715, -1.0112, -0.4764, -0.1527,\n","         -0.8844, -0.6987, -0.5699],\n","        [-0.9546, -1.1275, -0.2354, -0.6463,  7.6652, -0.9961, -0.4778, -0.1442,\n","         -0.8656, -0.7366, -0.5536]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0016665784642100334\n","During training one batch, the logits of the output is: tensor([[-0.3226, -0.6247, -1.5172, -1.1763, -0.7874, -1.1751, -1.2886,  8.0194,\n","         -0.8073, -0.8974, -1.0168],\n","        [-1.5453, -1.0509, -0.3488, -1.4812, -1.3991,  8.0483, -1.1576, -1.6460,\n","         -0.3850, -1.4907, -0.5590],\n","        [-0.9673, -1.0678, -0.1942, -0.5631,  7.6692, -1.0058, -0.4469, -0.3949,\n","         -0.8792, -0.6181, -0.5669]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.0023934810888022184\n","During training one batch, the logits of the output is: tensor([[-0.6292, -0.4553, -1.4921, -1.1236, -0.8112, -1.1061, -1.2003,  8.0303,\n","         -0.8446, -1.0452, -0.9361],\n","        [-0.4237, -0.6311, -1.4769, -1.1069, -0.7855, -1.1714, -1.1721,  8.0170,\n","         -0.7469, -1.1251, -0.9683],\n","        [-1.2110,  7.2822, -1.0888, -0.7384, -1.6479, -0.7778, -0.6428,  1.2236,\n","         -0.9241, -1.1955, -1.2927]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.0019636491779237986\n","During training one batch, the logits of the output is: tensor([[-1.5348,  7.1846, -1.1078, -0.4887, -1.1844, -0.3200, -0.4023,  0.3490,\n","         -1.2935, -0.9667, -0.8491],\n","        [-1.2599, -1.0310, -0.8533, -1.2168, -1.1379,  8.1910, -1.0711, -1.2868,\n","         -0.7791, -1.4093, -1.0736],\n","        [-1.2952, -0.9981, -0.8307, -1.2428, -1.1515,  8.2081, -1.0596, -1.2690,\n","         -0.8362, -1.3942, -1.0701]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0015711048617959023\n","During training one batch, the logits of the output is: tensor([[-1.2987, -1.0808, -0.8103, -1.1931, -1.0655,  8.1907, -1.0692, -1.2892,\n","         -0.9019, -1.3162, -1.0372],\n","        [-1.4814, -1.0879, -0.8694, -1.1545, -1.0555,  8.2177, -1.0606, -1.2616,\n","         -0.9784, -1.2022, -0.9020],\n","        [-0.0948, -0.9021,  7.4684, -0.7862, -0.4687, -0.7214, -0.7025, -1.6883,\n","         -0.7665, -0.3571, -1.1304]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.002784429118037224\n","During training one batch, the logits of the output is: tensor([[-0.5148, -0.8437,  7.5409, -0.8943, -0.2561, -0.7515, -0.8082, -1.7300,\n","         -0.5963, -0.7503, -0.8253],\n","        [ 0.0858, -1.1880,  7.3607, -0.9476, -0.4165, -0.8610, -0.7948, -1.5452,\n","         -0.8525, -0.1386, -0.8540],\n","        [-0.6686, -0.8642,  7.5564, -0.9967, -0.3499, -0.0480, -0.8652, -1.6445,\n","         -1.0561, -0.7953, -0.9163]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.00252805114723742\n","During training one batch, the logits of the output is: tensor([[-7.0481e-01, -9.3690e-01,  7.5687e+00, -1.0296e+00, -2.9013e-01,\n","         -6.1380e-03, -8.7976e-01, -1.7202e+00, -1.0331e+00, -6.7050e-01,\n","         -9.6146e-01],\n","        [-7.9027e-01, -9.1587e-01,  7.5617e+00, -1.0048e+00, -2.5635e-01,\n","         -9.0792e-03, -8.9307e-01, -1.6615e+00, -1.0852e+00, -6.7448e-01,\n","         -9.3771e-01],\n","        [-7.6971e-01, -9.8891e-01,  7.5417e+00, -1.0166e+00, -2.9706e-01,\n","          8.6805e-02, -8.7212e-01, -1.6275e+00, -1.0570e+00, -6.5586e-01,\n","         -9.8099e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0021906124893575907\n","During training one batch, the logits of the output is: tensor([[-1.0214, -0.9309, -0.6477, -0.5769, -0.9008, -0.7120, -0.3919, -0.3847,\n","         -0.5683, -1.3540,  7.7198],\n","        [-1.0681, -0.9210, -0.6349, -0.5327, -0.8471, -0.7055, -0.3667, -0.4855,\n","         -0.5703, -1.3419,  7.7186],\n","        [-1.0371, -0.9596, -0.6338, -0.4863, -0.8238, -0.7615, -0.3906, -0.4573,\n","         -0.5647, -1.2816,  7.7126]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.0035480286460369825\n","During training one batch, the logits of the output is: tensor([[-1.0342, -0.9619, -0.6202, -0.5142, -0.8467, -0.7235, -0.3555, -0.5149,\n","         -0.5196, -1.3124,  7.7187],\n","        [-1.1716,  7.3016, -1.1302, -0.8052, -1.4918, -1.0660, -0.7448,  1.3279,\n","         -0.8563, -1.1580, -1.0938],\n","        [-1.1436,  7.3630, -0.9121, -0.7215, -1.5766, -1.0019, -0.7407,  0.9521,\n","         -0.9175, -1.0803, -1.0616]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014880235539749265\n","During training one batch, the logits of the output is: tensor([[-0.6667, -0.4027, -1.4924, -1.1180, -0.8156, -1.0269, -1.2471,  8.0265,\n","         -0.8738, -1.1587, -0.9087],\n","        [-0.4612, -0.5515, -1.4516, -1.1474, -0.7035, -1.3052, -1.3515,  8.0335,\n","         -0.8758, -0.6222, -0.9838],\n","        [ 7.8156, -1.3636, -0.4484, -0.6266, -0.5299, -1.1743, -0.8550, -0.4984,\n","         -0.6161, -0.9323, -1.2991]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0009, device='cuda:0')\n","Loss.item: 0.0008724299841560423\n","During training one batch, the logits of the output is: tensor([[-1.5360, -0.8093, -0.8553, -1.2393, -1.2103,  8.2511, -1.0774, -1.2729,\n","         -0.9585, -1.2501, -0.9398],\n","        [-1.4868, -0.8003, -0.8666, -1.2532, -1.2492,  8.2545, -1.0636, -1.3039,\n","         -0.8555, -1.2966, -1.0021],\n","        [-1.4802, -0.8676, -0.9206, -1.2171, -1.1923,  8.2518, -1.0575, -1.3098,\n","         -0.8493, -1.2719, -0.9890]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.0028897554147988558\n","During training one batch, the logits of the output is: tensor([[-2.1598,  0.9535, -1.2606, -1.2848, -1.1339,  7.4206, -0.3783, -1.5821,\n","         -1.1226, -1.3866, -0.8845],\n","        [-2.1534,  1.1096, -1.0126, -1.2403, -1.2328,  7.5287, -0.9833, -1.1424,\n","         -1.4116, -1.5291, -0.9914],\n","        [-0.5802, -0.8370,  7.5982, -0.7618, -0.3678, -0.4651, -0.7158, -1.6549,\n","         -0.8640, -0.5881, -1.1255]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018388201715424657\n","During training one batch, the logits of the output is: tensor([[ 7.8002, -1.4491, -0.5297, -0.5558, -0.5914, -1.1078, -0.7826, -0.6816,\n","         -0.5760, -0.9661, -1.0625],\n","        [ 7.8163, -1.4453, -0.5090, -0.5846, -0.5000, -1.1215, -0.7705, -0.6059,\n","         -0.7431, -0.9355, -1.1064],\n","        [ 7.8120, -1.4333, -0.5005, -0.5121, -0.5476, -1.1534, -0.7649, -0.6345,\n","         -0.7092, -0.9399, -1.1321]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022866709623485804\n","During training one batch, the logits of the output is: tensor([[ 7.8060, -1.3820, -0.4664, -0.5019, -0.5388, -1.1708, -0.7336, -0.7748,\n","         -0.6224, -0.8851, -1.1967],\n","        [ 7.8380, -1.3443, -0.4305, -0.5613, -0.4996, -1.1997, -0.8389, -0.5958,\n","         -0.7046, -0.8277, -1.2747],\n","        [-1.1455,  7.3890, -0.7702, -0.5548, -1.5144, -0.9592, -0.6295,  0.4187,\n","         -0.7803, -0.9335, -1.1593]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.002596338279545307\n","During training one batch, the logits of the output is: tensor([[-1.1258,  7.3776, -0.7175, -0.5363, -1.5132, -0.8857, -0.6254,  0.2946,\n","         -0.8020, -0.9057, -1.2103],\n","        [ 7.8406, -1.3561, -0.4305, -0.5131, -0.4873, -1.2326, -0.8637, -0.5561,\n","         -0.7049, -0.8331, -1.2899],\n","        [-0.7323, -1.1700, -0.0499, -0.7317,  7.5722, -0.9960, -0.5991, -0.1106,\n","         -0.9982, -0.7977, -0.4614]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0021177567541599274\n","During training one batch, the logits of the output is: tensor([[-0.8812, -1.1226, -0.1061, -0.6221,  7.6538, -0.9403, -0.5140, -0.4135,\n","         -0.9385, -0.6563, -0.4749],\n","        [-1.1256, -0.9037, -0.5813, -0.5403, -0.6864, -0.9560, -0.3151, -0.5737,\n","         -0.2925, -1.4100,  7.6351],\n","        [-0.7023, -0.7996, -0.4180, -0.9466, -0.5097, -0.7732, -0.1089, -0.7507,\n","         -0.3588,  8.2748, -1.1829]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.00133433286100626\n","During training one batch, the logits of the output is: tensor([[-0.7428, -0.7957, -0.2888, -0.8662, -0.5332, -0.7958, -0.0885, -0.7885,\n","         -0.5116,  8.3065, -1.1121],\n","        [-0.6960, -0.7698, -0.3011, -0.8477, -0.6511, -0.7186, -0.1045, -0.8262,\n","         -0.4536,  8.3051, -1.2511],\n","        [-0.7039, -0.8262, -0.2626, -0.8269, -0.5356, -0.8054, -0.0810, -0.7383,\n","         -0.5898,  8.3174, -1.1490]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0022204711567610502\n","During training one batch, the logits of the output is: tensor([[-0.7158, -0.8111, -0.2993, -0.7989, -0.5349, -0.8216, -0.0777, -0.7264,\n","         -0.6018,  8.3139, -1.1026],\n","        [-0.4191, -0.2920, -0.5694,  7.6390, -0.3234, -0.8077, -0.0258, -0.6218,\n","         -0.8110, -0.5202, -0.1536],\n","        [-1.0047, -0.9609, -0.6467, -0.5498, -0.9261, -0.6685, -0.3802, -0.5301,\n","         -0.5369, -1.2760,  7.7142]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.0035602860152721405\n","During training one batch, the logits of the output is: tensor([[-1.6271,  0.5221, -1.3704, -1.0265, -1.1664, -0.6403,  0.3422,  0.1107,\n","         -0.1519, -1.8367,  6.6968],\n","        [-1.4336, -0.8399, -0.9979, -1.3256, -1.1526,  8.2283, -0.9897, -1.2296,\n","         -0.7592, -1.4881, -0.9292],\n","        [-0.7527, -0.7158, -0.3405, -0.7252, -0.6455, -0.7539, -0.0489, -0.6833,\n","         -0.7537,  8.3113, -1.1419]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.0035439729690551758\n","During training one batch, the logits of the output is: tensor([[-0.8089, -0.7652, -0.3993, -0.8337, -0.7098, -0.6822, -0.1302, -0.2645,\n","         -0.7906,  8.2534, -1.1556],\n","        [-1.0087, -0.9643, -0.5907, -0.5172, -0.8327, -0.7758, -0.3802, -0.5418,\n","         -0.5341, -1.2924,  7.7127],\n","        [-0.4832, -0.8166, -0.8819, -0.6541, -1.0973, -0.4115, -0.0247, -0.7437,\n","          6.6699, -0.8842, -0.3518]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.001048272824846208\n","During training one batch, the logits of the output is: tensor([[-0.8308, -0.4243, -1.5679, -1.1989, -0.9207, -1.0285, -1.2658,  8.0084,\n","         -0.8691, -0.9066, -0.5870],\n","        [-1.3243, -1.0317, -0.7593, -1.1933, -1.1482,  8.2232, -1.0249, -1.3776,\n","         -0.9366, -1.2373, -1.0162],\n","        [-1.3400, -1.0210, -1.0109, -1.2342, -1.1753,  8.2086, -1.0440, -1.2300,\n","         -0.7738, -1.3836, -0.9431]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0011, device='cuda:0')\n","Loss.item: 0.0010791494278237224\n","During training one batch, the logits of the output is: tensor([[-1.3246, -1.0578, -1.0619, -1.2255, -1.1452,  8.1955, -1.0150, -1.2188,\n","         -0.7455, -1.3907, -0.9266],\n","        [-1.3648, -1.0445, -0.9342, -1.2305, -1.0658,  8.2173, -1.0132, -1.2257,\n","         -0.9313, -1.4604, -0.7716],\n","        [-0.6097, -0.8336, -0.3396, -0.6719, -0.6209, -0.9405, -0.1384, -0.7210,\n","         -0.5800,  8.2551, -1.0465]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0033, device='cuda:0')\n","Loss.item: 0.003279987955465913\n","During training one batch, the logits of the output is: tensor([[-0.7139, -0.7937, -0.3185, -0.6955, -0.5755, -0.9531, -0.1319, -0.5880,\n","         -0.6696,  8.2857, -1.0247],\n","        [-0.3530, -0.7235, -0.9289, -0.7576, -1.2553, -0.1062, -0.0428, -0.7508,\n","          6.6157, -0.8846, -0.4922],\n","        [-1.5219, -1.0472, -0.9221, -1.1808, -1.0667,  8.2391, -1.0864, -1.2508,\n","         -0.9614, -1.2586, -0.8216]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.0041387053206563\n","During training one batch, the logits of the output is: tensor([[-0.3932, -0.6639, -1.0736, -0.8024, -1.2011, -0.1042, -0.0754, -0.6968,\n","          6.6350, -1.0290, -0.3233],\n","        [ 7.8216, -1.3366, -0.4273, -0.4391, -0.5928, -1.3049, -0.6695, -0.6438,\n","         -0.5216, -0.8585, -1.4524],\n","        [ 7.6103, -1.4841, -0.7157,  0.8903, -0.5472, -1.5887, -0.8731, -0.0659,\n","         -0.7396, -1.3411, -1.4195]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0059, device='cuda:0')\n","Loss.item: 0.005879318807274103\n","During training one batch, the logits of the output is: tensor([[ 7.8140, -1.3495, -0.4705, -0.4129, -0.5793, -1.3017, -0.6530, -0.6427,\n","         -0.4925, -0.8523, -1.4779],\n","        [-0.9485, -1.1845, -0.2126, -0.5079,  7.6743, -1.0255, -0.3887, -0.4625,\n","         -0.8319, -0.5835, -0.5003],\n","        [-1.2358,  6.8645, -1.2871, -0.8088, -1.6189, -0.8984, -0.7915,  2.2856,\n","         -1.0346, -1.3990, -1.3010]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0035, device='cuda:0')\n","Loss.item: 0.0034563811495900154\n","During training one batch, the logits of the output is: tensor([[-1.9221,  0.4315, -1.3423, -1.3700, -1.6009,  7.7158, -0.6697, -1.5514,\n","         -0.0720, -1.5966, -1.1974],\n","        [-1.1013,  7.2979, -0.9626, -0.7268, -1.6856, -0.9245, -0.7330,  1.0752,\n","         -0.9708, -1.1610, -1.1166],\n","        [-1.1417,  7.3099, -1.0606, -0.7411, -1.5514, -1.0276, -0.6352,  1.0731,\n","         -0.9509, -1.1471, -1.0904]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0009, device='cuda:0')\n","Loss.item: 0.0008841419476084411\n","During training one batch, the logits of the output is: tensor([[-1.3248, -0.9064, -0.9126, -1.1126, -1.1907,  8.2383, -0.9759, -1.3319,\n","         -1.0160, -1.2913, -0.9966],\n","        [-1.3075, -1.0447, -0.9402, -1.1259, -1.1918,  8.2316, -0.9649, -1.3999,\n","         -0.8450, -1.3629, -0.9045],\n","        [-1.4445, -0.8577, -1.1732, -1.2367, -1.2363,  8.2381, -1.0106, -1.2449,\n","         -0.8457, -1.3249, -0.8963]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.0019882346969097853\n","During training one batch, the logits of the output is: tensor([[-0.0404, -0.3927, -0.6900,  7.6045, -0.3322, -0.9961, -0.0263, -0.7013,\n","         -0.5641, -0.5955, -0.2258],\n","        [-0.7470, -0.4156, -1.6090, -1.2001, -0.7483, -1.0830, -1.3583,  8.0282,\n","         -0.7844, -0.5754, -1.0533],\n","        [-0.7988, -0.4072, -1.6309, -1.2346, -0.7697, -1.0603, -1.3885,  8.0191,\n","         -0.7580, -0.4863, -1.0278]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0078, device='cuda:0')\n","Loss.item: 0.007789203897118568\n","During training one batch, the logits of the output is: tensor([[-1.1283,  0.7538, -0.5982, -0.7740, -0.4132, -0.7736, -0.8312,  0.5913,\n","         -1.1845, -1.3655,  5.8897],\n","        [-1.2216,  0.2079, -1.6400, -1.0529, -0.8970, -0.9956, -1.3791,  7.8102,\n","         -0.8609, -1.1571, -0.4432],\n","        [-0.5850, -0.7503, -0.3109, -0.7263, -0.5860, -0.8463, -0.0664, -0.7293,\n","         -0.7094,  8.3238, -1.2306]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0165, device='cuda:0')\n","Loss.item: 0.016539109870791435\n","During training one batch, the logits of the output is: tensor([[-0.6422, -0.7406, -0.4150, -0.7629, -0.6669, -0.7664, -0.0864, -0.4688,\n","         -0.7745,  8.3032, -1.2620],\n","        [-0.6617, -0.7659, -0.3903, -0.6969, -0.6589, -0.7778, -0.0664, -0.6060,\n","         -0.7779,  8.2978, -1.1400],\n","        [-1.3286,  6.4067, -1.5881, -0.7960, -1.4549, -1.0898, -0.8296,  3.2705,\n","         -1.1199, -1.4298, -1.2013]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0390, device='cuda:0')\n","Loss.item: 0.038952115923166275\n","During training one batch, the logits of the output is: tensor([[-1.3747,  5.9174, -1.5545, -0.5542, -1.3831, -1.1068, -0.7158,  3.6454,\n","         -1.3517, -1.5489, -1.1493],\n","        [-1.1770,  7.3446, -1.0246, -0.6710, -1.5663, -0.3842, -0.7359,  0.6159,\n","         -0.8551, -1.2430, -1.4025],\n","        [-1.2396,  7.0690, -1.1561, -0.7932, -1.5342, -0.8191, -0.8487,  1.8985,\n","         -1.1610, -1.2838, -1.2221]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0029, device='cuda:0')\n","Loss.item: 0.002919830149039626\n","During training one batch, the logits of the output is: tensor([[-0.9816, -1.1009, -0.2224, -0.6204,  7.6748, -1.0064, -0.4846, -0.2763,\n","         -0.9034, -0.6019, -0.5614],\n","        [-1.3945,  7.3266, -1.0618, -0.7710, -1.6440, -0.2595, -0.7120,  0.8291,\n","         -0.9760, -1.2047, -1.2732],\n","        [-0.9536, -1.1193, -0.1631, -0.4303,  7.6724, -1.0531, -0.4430, -0.4559,\n","         -0.9790, -0.5970, -0.4337]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0076, device='cuda:0')\n","Loss.item: 0.0075910878367722034\n","During training one batch, the logits of the output is: tensor([[-0.9928, -1.1042, -0.2460, -0.4047,  7.6565, -1.0323, -0.4472, -0.2920,\n","         -0.9808, -0.7227, -0.4623],\n","        [-0.3288, -0.8374, -0.7355, -0.7501, -1.2463, -0.4512, -0.0781, -0.6892,\n","          6.6882, -0.8841, -0.3939],\n","        [-1.4163,  6.7772, -1.1169, -0.7215, -1.2967, -0.5755, -0.9182,  2.1577,\n","         -1.4271, -1.3700, -1.0273]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014825206017121673\n","During training one batch, the logits of the output is: tensor([[-0.7236, -0.7539, -0.2602, -0.8087, -0.6725, -0.6395, -0.1205, -0.5579,\n","         -0.7643,  8.3016, -1.2808],\n","        [-0.6898, -0.7895, -0.5645, -1.0498, -0.7512, -0.6134, -0.2843,  0.3690,\n","         -0.6246,  8.1049, -1.5983],\n","        [-0.7097, -0.7782, -0.2788, -0.8521, -0.6411, -0.6742, -0.1354, -0.4301,\n","         -0.7362,  8.3025, -1.3180]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017784197116270661\n","During training one batch, the logits of the output is: tensor([[-0.7115, -0.7853, -0.2918, -0.8595, -0.6379, -0.6824, -0.1425, -0.3947,\n","         -0.7285,  8.3006, -1.3174],\n","        [-0.7429, -0.7300, -0.4126, -0.9814, -0.7436, -0.4714, -0.1808, -0.0887,\n","         -0.7351,  8.1790, -1.5382],\n","        [-0.7885, -0.9649,  7.5907, -0.8964, -0.2620, -0.1707, -0.7743, -1.7811,\n","         -0.9620, -0.5668, -0.9520]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018581771291792393\n","During training one batch, the logits of the output is: tensor([[-1.3716, -0.8619, -0.9917, -1.2930, -1.1402,  8.2195, -1.1298, -1.1022,\n","         -0.9407, -1.3553, -0.9993],\n","        [-0.8714, -0.2430, -1.7365, -1.2214, -0.5430, -1.2173, -1.4429,  7.9838,\n","         -0.6954, -0.2431, -1.1860],\n","        [-0.4895, -0.2230, -0.6430,  7.6266, -0.3490, -0.8623, -0.0194, -0.5489,\n","         -0.7473, -0.5045, -0.1526]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0011, device='cuda:0')\n","Loss.item: 0.0010777509305626154\n","During training one batch, the logits of the output is: tensor([[-0.8096, -0.7209, -0.3015, -0.7020, -0.3121, -0.8825, -0.1016, -0.5301,\n","         -0.8718,  8.2487, -1.0927],\n","        [-1.3123, -0.8663, -0.7691, -1.2843, -1.2400,  8.2380, -1.0209, -1.3597,\n","         -0.8541, -1.5080, -1.0635],\n","        [-1.2278, -1.0513, -0.8585, -1.2298, -1.1661,  8.2251, -1.0427, -1.3722,\n","         -0.7584, -1.3925, -0.9893]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0009, device='cuda:0')\n","Loss.item: 0.0008910102187655866\n","During training one batch, the logits of the output is: tensor([[-1.2132, -0.9461, -0.7875, -1.2421, -1.2196,  8.2205, -1.0457, -1.4443,\n","         -0.7987, -1.3818, -1.0386],\n","        [-1.3869, -0.9091, -0.8807, -1.2224, -1.1561,  8.2339, -1.0811, -1.2071,\n","         -0.9444, -1.3622, -1.0116],\n","        [-1.4495, -0.9174, -0.9673, -1.2675, -1.1186,  8.2307, -1.0654, -1.1189,\n","         -0.9602, -1.4149, -0.9005]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0014, device='cuda:0')\n","Loss.item: 0.0014387527480721474\n","During training one batch, the logits of the output is: tensor([[-1.4772, -0.8939, -0.8775, -1.2174, -1.0818,  8.2188, -1.1229, -1.1397,\n","         -1.0437, -1.3581, -0.9154],\n","        [-0.9175, -1.0720, -0.1506, -0.4643,  7.6635, -0.9933, -0.4543, -0.6057,\n","         -0.9626, -0.5214, -0.4435],\n","        [-1.4861, -0.8123, -0.9674, -1.2234, -1.1750,  8.2362, -1.0357, -1.1989,\n","         -0.9577, -1.3278, -0.9761]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0025214592460542917\n","During training one batch, the logits of the output is: tensor([[-1.2796,  7.3600, -0.7626, -0.4547, -1.3441, -0.6223, -0.5537, -0.0246,\n","         -0.9378, -0.8813, -1.0603],\n","        [-1.2177,  7.3337, -0.7650, -0.3755, -1.3826, -0.7923, -0.4405, -0.1788,\n","         -0.8391, -0.8367, -1.0550],\n","        [-0.5538, -0.4726, -1.4070, -1.1812, -0.8709, -1.0755, -1.2313,  8.0141,\n","         -0.9367, -0.9795, -0.9675]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.001956640975549817\n","During training one batch, the logits of the output is: tensor([[-0.6525, -0.4100, -1.4960, -1.1425, -0.7901, -1.1473, -1.2062,  8.0371,\n","         -0.9098, -0.9044, -0.9267],\n","        [-0.9232, -0.1430, -1.5312, -1.0737, -0.8271, -1.0215, -1.2682,  8.0117,\n","         -0.8688, -1.0536, -0.9609],\n","        [-0.4105, -0.2921, -0.6770,  7.6157, -0.3038, -0.8264,  0.0775, -0.6052,\n","         -0.6882, -0.5964, -0.2188]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024902757722884417\n","During training one batch, the logits of the output is: tensor([[-1.3264, -0.8425, -0.9516, -1.2551, -1.2501,  8.2392, -0.9974, -1.2878,\n","         -0.8081, -1.3893, -1.0903],\n","        [-1.3306, -0.9772, -0.9432, -1.2650, -1.2408,  8.2258, -1.0045, -1.3191,\n","         -0.7515, -1.2904, -1.0567],\n","        [-1.1110,  7.2072, -1.1138, -0.7996, -1.5875, -0.8094, -0.9002,  1.5200,\n","         -0.8969, -1.2470, -1.2618]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017851678421720862\n","During training one batch, the logits of the output is: tensor([[-1.3913, -0.9723, -0.8913, -1.1889, -1.1754,  8.2451, -1.0884, -1.2960,\n","         -0.9203, -1.2514, -0.9345],\n","        [-0.8067, -0.4227, -1.6807, -1.3380, -0.8036, -1.1149, -1.3823,  7.9387,\n","         -0.7104, -0.1762, -1.0866],\n","        [-0.6296, -0.6167,  7.3921, -0.8283, -0.0363, -0.7379, -0.8730, -1.3343,\n","         -1.1332, -0.8103, -0.8601]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.00227969978004694\n","During training one batch, the logits of the output is: tensor([[ 7.8293, -1.4393, -0.4029, -0.4879, -0.4275, -1.2797, -0.8983, -0.4321,\n","         -0.6604, -1.0292, -1.2912],\n","        [-0.4039, -0.3231, -0.5385,  7.6301, -0.2045, -0.8711, -0.0643, -0.7989,\n","         -0.6510, -0.4993, -0.1989],\n","        [ 7.8308, -1.3616, -0.4442, -0.5652, -0.5230, -1.1204, -0.8565, -0.5539,\n","         -0.6087, -0.9923, -1.2907]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.002161390148103237\n","During training one batch, the logits of the output is: tensor([[-1.3938, -0.7658, -0.9518, -1.2752, -1.2494,  8.2361, -1.0389, -1.2627,\n","         -0.7524, -1.4403, -1.0945],\n","        [-1.2794,  7.3177, -0.8095, -0.6692, -1.5017, -0.3289, -0.4601,  0.2926,\n","         -0.9637, -1.2330, -1.3221],\n","        [ 7.6978, -1.4611, -0.5322, -0.7714, -0.5877, -1.3302, -1.0344,  0.0578,\n","         -0.8546, -1.2757, -0.3694]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002111714566126466\n","During training one batch, the logits of the output is: tensor([[-0.3818, -0.8470,  7.5825, -0.6340, -0.3679, -0.5996, -0.7339, -1.7654,\n","         -0.7326, -0.6419, -1.1229],\n","        [-0.5444, -0.8146,  7.5995, -0.5997, -0.3599, -0.5393, -0.7411, -1.7768,\n","         -0.8447, -0.5949, -1.0515],\n","        [-1.1691, -0.9164, -1.0224, -1.5004, -1.1467,  7.8495, -0.4682, -1.7238,\n","         -0.5763, -1.2618, -0.8055]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0010, device='cuda:0')\n","Loss.item: 0.0010491822613403201\n","During training one batch, the logits of the output is: tensor([[-1.4619, -0.9873, -0.9294, -1.2197, -1.1633,  8.2359, -1.0386, -1.4007,\n","         -0.7574, -1.2139, -0.9401],\n","        [-1.4432, -1.0891, -0.9094, -1.1629, -1.1048,  8.2237, -1.0568, -1.2743,\n","         -0.9010, -1.2199, -0.9210],\n","        [-0.5109, -0.8208, -0.4852, -0.7659, -0.6459, -0.9701, -0.1317, -0.3760,\n","         -0.5601,  8.2915, -1.3089]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0011, device='cuda:0')\n","Loss.item: 0.0010907049290835857\n","During training one batch, the logits of the output is: tensor([[-0.4226, -0.8537, -0.5166, -0.8254, -0.6423, -1.0123, -0.1257, -0.3266,\n","         -0.4938,  8.2841, -1.3428],\n","        [-1.2120, -0.9740, -0.9548, -1.3060, -1.2339,  8.1875, -1.0382, -1.3803,\n","         -0.5895, -1.2438, -1.0960],\n","        [-1.2833, -1.0148, -0.9645, -1.2930, -1.1588,  8.1914, -1.0894, -1.3711,\n","         -0.6224, -1.1687, -1.0264]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0020, device='cuda:0')\n","Loss.item: 0.0020023733377456665\n","During training one batch, the logits of the output is: tensor([[-1.3103, -0.9885, -0.9421, -1.3221, -1.3438,  8.1737, -1.0650, -1.3547,\n","         -0.4851, -1.3824, -0.9807],\n","        [-1.0246,  7.2780, -1.0202, -0.7701, -1.7518, -0.6468, -0.7087,  0.9816,\n","         -0.7603, -1.2923, -1.4190],\n","        [-1.2555, -0.9129, -0.9432, -1.2554, -1.2575,  8.2248, -1.0468, -1.2971,\n","         -0.6784, -1.4341, -1.1055]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014795762253925204\n","During training one batch, the logits of the output is: tensor([[-1.2082, -0.8587, -0.9222, -1.2654, -1.2240,  8.2187, -1.0349, -1.2412,\n","         -0.8649, -1.4304, -1.0815],\n","        [-0.7029, -0.7526, -0.2847, -0.6997, -0.6926, -0.6893, -0.0525, -0.8313,\n","         -0.7712,  8.3012, -1.1203],\n","        [-1.1587, -0.8318, -0.5352, -0.6314, -0.8945, -0.6240, -0.3866, -0.5376,\n","         -0.6518, -1.2445,  7.7034]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.0022108335979282856\n","During training one batch, the logits of the output is: tensor([[-1.1277, -0.8936, -0.5852, -0.6126, -0.8252, -0.5589, -0.4529, -0.5021,\n","         -0.6667, -1.2797,  7.6905],\n","        [-1.0933, -0.8510, -0.6392, -0.6445, -0.9381, -0.5958, -0.4384, -0.4806,\n","         -0.5563, -1.2949,  7.6965],\n","        [-1.0941, -0.8539, -0.5273, -0.6591, -0.9199, -0.6811, -0.4553, -0.4008,\n","         -0.6975, -1.2435,  7.7012]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0055, device='cuda:0')\n","Loss.item: 0.0055122035555541515\n","During training one batch, the logits of the output is: tensor([[-1.2955, -0.6776, -0.8470, -1.3957, -1.3145,  8.1628, -1.0224, -1.5927,\n","         -0.6645, -1.3537, -0.8230],\n","        [-0.2572, -0.8977, -0.7770, -0.8421, -1.1280, -0.1591, -0.2198, -0.5683,\n","          6.5678, -0.9668, -0.5601],\n","        [-0.3205, -0.8423, -0.9590, -0.8958, -0.8951, -0.4127, -0.2701, -0.4653,\n","          6.5558, -0.9945, -0.3715]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0080, device='cuda:0')\n","Loss.item: 0.008015326224267483\n","During training one batch, the logits of the output is: tensor([[-0.4133, -0.7833, -0.7144, -0.8568, -1.2909, -0.3018, -0.0533, -0.5812,\n","          6.6108, -0.8444, -0.5889],\n","        [-0.4117, -0.8027, -0.7278, -0.8623, -1.2867, -0.3086, -0.0520, -0.5851,\n","          6.6142, -0.7800, -0.5992],\n","        [-0.4354, -0.7984, -0.6103, -0.8838, -1.0842, -0.6020, -0.0074, -0.4967,\n","          6.4004, -0.5043, -0.8435]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0017343590734526515\n","During training one batch, the logits of the output is: tensor([[-1.4575, -0.9347, -1.0170, -1.2350, -1.1671,  8.2512, -0.9956, -1.3284,\n","         -0.8561, -1.2921, -0.9083],\n","        [-1.2161, -0.8479, -0.9837, -1.2567, -1.4011,  8.2019, -1.0018, -1.4348,\n","         -0.5696, -1.4974, -0.9674],\n","        [-1.1873,  7.3554, -1.0080, -0.6030, -1.7843, -0.6747, -0.2560,  0.4532,\n","         -0.7497, -1.0264, -1.3453]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.0013338638236746192\n","During training one batch, the logits of the output is: tensor([[-1.2022, -0.8051, -0.9100, -1.3181, -1.2499,  8.1690, -0.7608, -1.4601,\n","         -0.7742, -1.2929, -1.1957],\n","        [-0.4852, -0.4525, -1.7639, -1.2437, -0.3009, -1.2208, -1.4184,  7.9576,\n","         -0.7225, -0.9464, -0.9513],\n","        [-0.6079, -0.5294, -1.8169, -1.2769, -0.4853, -1.1227, -1.2739,  7.9122,\n","         -0.6751, -1.2035, -0.4997]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002106752246618271\n","During training one batch, the logits of the output is: tensor([[-0.4194, -0.4704, -1.5397, -1.2567, -0.7844, -1.1717, -1.3544,  8.0305,\n","         -0.8859, -0.9393, -0.8364],\n","        [-0.9623, -1.0580, -0.1647, -0.5028,  7.6682, -0.9793, -0.4477, -0.5348,\n","         -0.9606, -0.5540, -0.4716],\n","        [-0.9782, -1.0657, -0.1325, -0.5332,  7.6665, -0.9276, -0.4362, -0.6004,\n","         -0.8742, -0.6048, -0.4907]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1974, device='cuda:0')\n","Loss.item: 0.1974024772644043\n","During training one batch, the logits of the output is: tensor([[-0.9584, -1.0486, -0.1467, -0.5215,  7.6593, -0.8940, -0.4393, -0.6288,\n","         -0.9113, -0.5819, -0.5141],\n","        [ 3.0599, -0.4704, -0.4884, -0.9107, -0.2921, -1.7548, -1.3089, -0.2783,\n","          1.3553, -0.5680,  2.1976],\n","        [-0.9145, -0.9528,  7.5476, -0.8880, -0.1257, -0.1055, -0.8895, -1.7253,\n","         -0.9531, -0.6582, -0.8787]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0014, device='cuda:0')\n","Loss.item: 0.0013649677857756615\n","During training one batch, the logits of the output is: tensor([[-0.6333, -0.7451, -0.2380, -0.6949, -0.6918, -0.7466, -0.0859, -0.9712,\n","         -0.6333,  8.2806, -1.1135],\n","        [-0.5075, -0.8202, -0.2588, -0.6842, -0.6504, -0.8587, -0.1409, -0.8776,\n","         -0.6287,  8.2701, -1.1154],\n","        [-0.4165, -0.5167, -1.5535, -1.3150, -0.8996, -1.1271, -1.0994,  7.9964,\n","         -0.7632, -0.9377, -1.1448]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0024, device='cuda:0')\n","Loss.item: 0.0024457236286252737\n","During training one batch, the logits of the output is: tensor([[-0.8376, -0.8753,  7.5601, -0.9305, -0.2343, -0.1169, -0.9059, -1.6274,\n","         -1.0376, -0.7832, -0.8564],\n","        [-0.9175, -0.8278,  7.5841, -0.9361, -0.3207, -0.2036, -0.8378, -1.6650,\n","         -0.8991, -0.7020, -0.9129],\n","        [-0.9451, -1.1626, -0.2487, -0.5783,  7.6744, -1.0328, -0.4774, -0.2676,\n","         -0.9271, -0.5389, -0.5568]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.7460, device='cuda:0')\n","Loss.item: 1.7460448741912842\n","During training one batch, the logits of the output is: tensor([[-1.3199,  0.5443,  1.5102, -1.2862,  3.8091, -0.7341, -1.2180, -0.2987,\n","         -1.3988, -1.0406,  0.9133],\n","        [ 7.8341, -1.4484, -0.5654, -0.4313, -0.5337, -1.2026, -0.8152, -0.4654,\n","         -0.5266, -1.0531, -1.2219]], device='cuda:0')\n","Validation loss: 0.04732873884953161\n","F1 Score (Weighted): 0.9924846128927762\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SiBR5q164309"},"source":["Load the fine-tuned model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjQvEdCxDU4c","executionInfo":{"status":"ok","timestamp":1626813076373,"user_tz":240,"elapsed":1028,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"08c33069-2bc8-41ed-f16f-a439fc39f68b"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model.to(device)\n","model.load_state_dict(torch.load('second_finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"bD4L7Eie4_b4"},"source":["Take a look in some data in the validation set"]},{"cell_type":"code","metadata":{"id":"vCMZEo7pHN9z"},"source":["dataiter = iter(dataloader_validation)\n","\n","texts, _, labels = dataiter.next()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_XjUbb9HOF6","executionInfo":{"status":"ok","timestamp":1626813088117,"user_tz":240,"elapsed":156,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"0eb2c7ec-d3b3-42a2-c344-13e3a941ed7b"},"source":["labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4, 9, 2])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"ym85VK6jHOMP"},"source":["output_test = model(texts.to(device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXoqmLyVJCJM","executionInfo":{"status":"ok","timestamp":1626813105883,"user_tz":240,"elapsed":184,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"e37c04df-5317-420b-dae4-be497e37d519"},"source":["output_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput([('logits',\n","                           tensor([[-0.9503, -1.1377, -0.2512, -0.7353,  7.6463, -1.0559, -0.5708, -0.0470,\n","                                    -0.9613, -0.5182, -0.5091],\n","                                   [-0.6774, -0.7376, -0.5423, -1.0054, -0.7230, -0.6022, -0.1937, -0.6914,\n","                                    -0.4369,  8.2002, -0.9837],\n","                                   [-1.1941, -0.8498,  7.3005, -1.2992, -0.3343,  0.9047, -1.0476, -1.7439,\n","                                    -1.2216, -0.8222, -0.6837]], device='cuda:0', grad_fn=<AddmmBackward>))])"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"4DS1Bp9BH9rB"},"source":["_, predicted = torch.max(output_test[0], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlW2Z9kJH90w","executionInfo":{"status":"ok","timestamp":1626813112371,"user_tz":240,"elapsed":7,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"fd4616c2-b8ff-41b8-92de-a770e36c52c0"},"source":["predicted"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4, 9, 2], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"WOhQCyOM5U-_"},"source":["The predicted labels are the same with the original labels"]},{"cell_type":"markdown","metadata":{"id":"M7plsT2V5QRK"},"source":["Evaluate the test set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHIlg1bKDjlX","executionInfo":{"status":"ok","timestamp":1626813939381,"user_tz":240,"elapsed":4855,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"77a470fa-161a-4de7-dcd9-6de368f1f840"},"source":["val_loss, predictions, true_vals = evaluate(dataloader_test)\n","val_f1 = f1_score_func(predictions, true_vals)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["During training one batch, the loss of the output is: tensor(0.0028, device='cuda:0')\n","Loss.item: 0.002779792994260788\n","During training one batch, the logits of the output is: tensor([[-0.9532, -1.1742, -0.2287, -0.7632,  7.6411, -1.0676, -0.5676,  0.0294,\n","         -0.8962, -0.6260, -0.5330],\n","        [-0.9438, -1.1612, -0.2293, -0.7412,  7.6482, -1.0868, -0.5661, -0.0167,\n","         -0.8913, -0.5956, -0.5216],\n","        [-0.4132, -0.2427, -0.6596,  7.6271, -0.4221, -0.8534, -0.0496, -0.6764,\n","         -0.5776, -0.5732, -0.1509]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0023189960047602654\n","During training one batch, the logits of the output is: tensor([[-5.1296e-01, -2.3827e-01, -6.7461e-01,  7.6346e+00, -3.6074e-01,\n","         -8.5894e-01, -1.9362e-03, -6.2375e-01, -6.1870e-01, -4.9537e-01,\n","         -1.7561e-01],\n","        [-1.3333e-01, -5.2672e-01, -1.6201e+00, -1.1057e+00, -1.1939e+00,\n","         -1.2865e+00, -1.1629e+00,  7.9456e+00, -8.2389e-01, -1.2204e+00,\n","         -5.3162e-01],\n","        [-1.2048e+00, -8.8165e-01, -7.5696e-01, -6.6445e-01, -8.2519e-01,\n","         -3.4548e-01, -4.3190e-01, -5.0007e-01, -6.1004e-01, -1.3376e+00,\n","          7.6587e+00]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014847774291411042\n","During training one batch, the logits of the output is: tensor([[-0.4765, -0.6171, -1.6023, -1.0798, -0.9323, -1.2469, -1.2541,  7.8657,\n","         -0.9535, -1.1170, -0.0898],\n","        [ 7.8301, -1.2604, -0.3133, -0.5682, -0.5837, -1.1422, -0.8057, -0.7062,\n","         -0.7365, -0.8440, -1.3211],\n","        [-1.3084, -0.9739, -1.1952, -1.3718, -1.3550,  8.1316, -1.0650, -1.1473,\n","         -0.4637, -1.3878, -1.0000]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.0038109803572297096\n","During training one batch, the logits of the output is: tensor([[-1.1056, -0.8320, -0.5744, -0.6628, -0.9486, -0.5278, -0.3808, -0.6105,\n","         -0.5928, -1.3560,  7.6851],\n","        [-0.3859, -0.7993, -0.9912, -0.8631, -1.3979,  0.1172, -0.1286, -0.7254,\n","          6.6021, -0.7925, -0.3282],\n","        [-0.5769, -0.4193, -1.4696, -1.1064, -0.9163, -1.1786, -1.1116,  8.0135,\n","         -0.9189, -1.1396, -0.7941]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0036, device='cuda:0')\n","Loss.item: 0.003566846949979663\n","During training one batch, the logits of the output is: tensor([[-0.7688, -0.2724, -1.5311, -1.0360, -0.9673, -1.1714, -1.0824,  8.0195,\n","         -0.8559, -1.0337, -0.8795],\n","        [-0.5968, -0.8759,  7.5863, -0.9748, -0.2729, -0.3442, -0.8373, -1.6235,\n","         -0.9226, -0.7056, -1.0652],\n","        [-0.4322, -0.8651, -0.6537, -0.8248, -1.0479, -0.5317, -0.1029, -0.5956,\n","          6.6507, -1.0361, -0.4118]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0014, device='cuda:0')\n","Loss.item: 0.0013660775730386376\n","During training one batch, the logits of the output is: tensor([[-0.5560, -0.5632, -1.5534, -1.1951, -0.8649, -0.9866, -1.1437,  8.0161,\n","         -0.9056, -1.1109, -0.8979],\n","        [-0.9070, -0.1641, -1.7596, -1.0432, -0.9398, -1.3158, -1.1035,  7.9864,\n","         -0.7636, -0.9840, -0.5217],\n","        [-0.6625, -0.6837, -1.4925, -1.1373, -0.7291, -1.0644, -1.1302,  7.9633,\n","         -0.9283, -1.0334, -0.6878]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.6300, device='cuda:0')\n","Loss.item: 0.6299532055854797\n","During training one batch, the logits of the output is: tensor([[-0.6734, -0.1627, -0.9413, -0.1194, -0.2703, -1.1319,  5.8208, -1.1140,\n","         -0.3701, -0.2846, -0.7225],\n","        [ 4.6488, -1.7911, -1.3253, -1.8316, -0.9283, -1.9372, -0.6053,  3.0282,\n","          0.1690,  1.7457, -1.3308],\n","        [-0.9540, -1.2643, -0.1334, -0.6860,  7.5738, -0.8996, -0.4167, -0.8331,\n","         -0.5139, -0.3171, -0.4496]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.1770, device='cuda:0')\n","Loss.item: 0.17702864110469818\n","During training one batch, the logits of the output is: tensor([[-1.8923,  4.3866, -1.5825, -1.2036, -1.9470,  4.7949, -0.1685, -1.2664,\n","         -0.3390, -1.4380, -1.7529],\n","        [-0.4547, -0.2003, -0.5781,  7.6081, -0.4213, -0.7934, -0.0312, -0.6322,\n","         -0.6471, -0.6123, -0.2307],\n","        [-0.2935, -0.2147, -0.5675,  7.6166, -0.4341, -0.8570, -0.1279, -0.6099,\n","         -0.6562, -0.6417, -0.2372]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.002205147175118327\n","During training one batch, the logits of the output is: tensor([[-0.3869, -0.6635, -1.6163, -1.1276, -0.9651, -1.2484, -1.2682,  7.7463,\n","         -1.0227, -1.1697,  0.2234],\n","        [-1.0519, -0.9847, -0.6154, -0.5133, -0.8591, -0.6738, -0.3887, -0.5944,\n","         -0.4832, -1.2616,  7.7099],\n","        [-0.6803, -0.8127,  7.5746, -0.7712, -0.3533, -0.4010, -0.7372, -1.6780,\n","         -0.7414, -0.7361, -1.1000]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0041, device='cuda:0')\n","Loss.item: 0.004122402518987656\n","During training one batch, the logits of the output is: tensor([[-0.4615, -0.3428, -1.5294, -1.2145, -0.9298, -1.1146, -1.3076,  8.0132,\n","         -0.9099, -0.9668, -0.9451],\n","        [-0.4366, -0.8861, -0.7069, -0.7975, -1.4029, -0.0099, -0.0974, -0.6385,\n","          6.5937, -0.9828, -0.3147],\n","        [-0.3558, -0.3243, -0.6158,  7.6289, -0.3270, -0.8239, -0.0992, -0.6725,\n","         -0.7209, -0.4848, -0.1427]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.001333499327301979\n","During training one batch, the logits of the output is: tensor([[-0.6656, -0.7547, -0.3777, -0.8038, -0.7217, -0.7830, -0.0754, -0.6728,\n","         -0.5559,  8.3192, -1.2432],\n","        [-0.7427, -0.7377, -0.2994, -0.7152, -0.6911, -0.6406, -0.0342, -0.7016,\n","         -0.8215,  8.2920, -1.1596],\n","        [-0.6958, -0.7584, -0.3776, -0.7805, -0.7383, -0.6843, -0.0568, -0.5727,\n","         -0.7061,  8.3009, -1.2468]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0049, device='cuda:0')\n","Loss.item: 0.004940731916576624\n","During training one batch, the logits of the output is: tensor([[-7.0590e-01, -4.2736e-01, -1.4591e+00, -1.1132e+00, -9.0273e-01,\n","         -1.0167e+00, -1.3423e+00,  8.0205e+00, -7.6545e-01, -9.9191e-01,\n","         -9.4735e-01],\n","        [-5.2450e-01, -7.5836e-01, -8.6603e-01, -8.1447e-01, -1.1964e+00,\n","         -3.6654e-01, -7.0273e-03, -6.6982e-01,  6.7096e+00, -9.1316e-01,\n","         -3.0191e-01],\n","        [-5.0025e-01, -7.5938e-01, -8.8842e-01, -8.4930e-01, -1.2054e+00,\n","         -3.6514e-01, -1.5383e-02, -6.4232e-01,  6.7064e+00, -9.1358e-01,\n","         -3.1842e-01]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0038, device='cuda:0')\n","Loss.item: 0.003793540643528104\n","During training one batch, the logits of the output is: tensor([[-0.4778, -0.8071, -0.8338, -0.8111, -1.1872, -0.3954, -0.0112, -0.6086,\n","          6.6939, -0.8841, -0.4157],\n","        [ 0.0283, -0.3700, -0.5783,  7.6091, -0.2673, -0.9047, -0.1795, -0.6924,\n","         -0.8437, -0.6222, -0.1922],\n","        [-0.7141, -0.3762, -1.5793, -1.0636, -0.7982, -1.1878, -1.2726,  8.0432,\n","         -0.8213, -0.8213, -0.9067]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0017842833185568452\n","During training one batch, the logits of the output is: tensor([[ 7.8092, -1.3334, -0.3131, -0.5807, -0.7031, -1.3320, -0.6253, -0.5992,\n","         -0.4898, -0.8773, -1.4255],\n","        [-1.3682, -0.8961, -1.1551, -1.2349, -1.2467,  8.2270, -1.0325, -1.3677,\n","         -0.6965, -1.2871, -0.9095],\n","        [-0.9541, -1.1992, -0.0631, -0.4552,  7.6642, -1.0360, -0.4324, -0.4917,\n","         -0.9699, -0.5205, -0.4669]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0073, device='cuda:0')\n","Loss.item: 0.007251751143485308\n","During training one batch, the logits of the output is: tensor([[ 7.8242, -1.3868, -0.2990, -0.5676, -0.4086, -1.2285, -0.8854, -0.5554,\n","         -0.8483, -0.8946, -1.2061],\n","        [-0.5201, -0.6305, -0.7172, -0.0541, -0.4291, -0.7778,  5.8358, -1.6533,\n","         -0.3111, -0.0881, -0.6380],\n","        [-0.6626, -0.7621,  7.5930, -0.7117, -0.2831, -0.4313, -0.7807, -1.7914,\n","         -0.8429, -0.6667, -1.0718]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0022768450435250998\n","During training one batch, the logits of the output is: tensor([[-1.1908,  7.4241, -0.9237, -0.5731, -1.5249, -0.8178, -0.4925,  0.2564,\n","         -0.8467, -0.9616, -1.0751],\n","        [ 7.8260, -1.4115, -0.4374, -0.4077, -0.5037, -1.1417, -0.8124, -0.6525,\n","         -0.6100, -0.8362, -1.3587],\n","        [ 7.7616, -1.3800, -0.0948, -0.7210, -0.4224, -1.0530, -0.8950, -0.6997,\n","         -0.7915, -0.9000, -1.2705]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0017, device='cuda:0')\n","Loss.item: 0.0016697216778993607\n","During training one batch, the logits of the output is: tensor([[ 7.8042, -1.3396, -0.2185, -0.7510, -0.4774, -1.0558, -0.8415, -0.7505,\n","         -0.7161, -0.8746, -1.2676],\n","        [ 7.8262, -1.4281, -0.4383, -0.4422, -0.4664, -1.1293, -0.8312, -0.6428,\n","         -0.6435, -0.8224, -1.3499],\n","        [-0.7071, -0.2658, -1.4955, -1.0891, -0.8361, -1.1160, -1.2829,  8.0565,\n","         -0.8709, -0.9509, -1.0110]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0026490231975913048\n","During training one batch, the logits of the output is: tensor([[-0.9510, -1.1288, -0.1528, -0.7226,  7.6618, -0.9623, -0.4890, -0.2709,\n","         -0.9227, -0.6223, -0.5071],\n","        [-1.2220,  7.3153, -1.1382, -0.8593, -1.3874, -0.9760, -0.7584,  1.2739,\n","         -0.9027, -1.1874, -1.0324],\n","        [-1.2532, -0.9044, -0.9698, -1.2735, -1.2230,  8.1783, -1.0610, -1.3241,\n","         -0.6730, -1.4034, -1.1143]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.0018386313458904624\n","During training one batch, the logits of the output is: tensor([[-1.2565, -1.0913, -0.8077, -1.2156, -1.0949,  8.1967, -1.0456, -1.3397,\n","         -0.8037, -1.3324, -1.0654],\n","        [-1.1063,  7.3428, -0.9688, -0.7042, -1.6623, -0.9307, -0.6475,  0.8522,\n","         -0.9670, -1.1014, -1.0444],\n","        [-1.4621, -1.0574, -0.9044, -1.1917, -1.1227,  8.2264, -1.0221, -1.2517,\n","         -0.9189, -1.2281, -0.9519]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0025, device='cuda:0')\n","Loss.item: 0.0024505204055458307\n","During training one batch, the logits of the output is: tensor([[-0.6011, -0.8931,  7.5677, -0.8683, -0.4391, -0.5808, -0.7114, -1.8059,\n","         -0.6710, -0.5415, -0.8315],\n","        [-0.5465, -0.9206,  7.5802, -1.0249, -0.3481, -0.2039, -0.8384, -1.7155,\n","         -0.9676, -0.7400, -0.9043],\n","        [-0.8028, -0.8857,  7.5677, -0.9808, -0.2816, -0.0579, -0.8599, -1.6794,\n","         -1.0769, -0.6563, -0.9205]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0037, device='cuda:0')\n","Loss.item: 0.003699341556057334\n","During training one batch, the logits of the output is: tensor([[-1.0467, -0.9556, -0.6050, -0.5132, -0.8388, -0.8109, -0.3740, -0.4518,\n","         -0.5504, -1.2826,  7.7127],\n","        [-1.1864,  7.2870, -1.1460, -0.8145, -1.4902, -1.0623, -0.7339,  1.3733,\n","         -0.8671, -1.1750, -1.0872],\n","        [-1.1448,  7.3450, -1.0135, -0.7838, -1.5020, -1.0601, -0.7618,  1.0873,\n","         -0.8738, -1.0880, -1.0497]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0031, device='cuda:0')\n","Loss.item: 0.0031353484373539686\n","During training one batch, the logits of the output is: tensor([[-1.1407,  7.3805, -0.9747, -0.6754, -1.6072, -0.5674, -0.7281,  0.4281,\n","         -0.8983, -1.0228, -1.1656],\n","        [-1.1132,  7.2476, -1.1655, -0.8060, -1.6325, -0.8500, -0.7402,  1.3224,\n","         -0.8211, -1.2310, -1.2782],\n","        [-0.5721, -0.5693, -1.4345, -1.1299, -0.7726, -1.0614, -1.2198,  8.0090,\n","         -0.9046, -1.0729, -0.8655]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0012256461195647717\n","During training one batch, the logits of the output is: tensor([[ 7.7969, -1.3877, -0.4184, -0.5982, -0.4944, -1.1675, -0.8990, -0.5038,\n","         -0.6616, -0.9342, -1.2793],\n","        [-1.4847, -0.7985, -0.9992, -1.3428, -1.3842,  8.2086, -1.0112, -1.3846,\n","         -0.5568, -1.3896, -0.9346],\n","        [-1.4782, -0.8182, -0.9113, -1.2182, -1.2295,  8.2505, -1.0542, -1.3253,\n","         -0.8114, -1.2981, -1.0069]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0015365952858701348\n","During training one batch, the logits of the output is: tensor([[-1.3336, -1.0170, -0.8692, -1.2042, -1.1875,  8.2124, -1.0411, -1.4109,\n","         -0.7654, -1.3124, -0.9873],\n","        [-0.7770, -0.5948, -0.4131, -0.8694, -0.6938, -0.6318, -0.1058, -0.5094,\n","         -0.6574,  8.2917, -1.3952],\n","        [-0.6098, -0.8579,  7.6107, -0.7320, -0.2669, -0.6473, -0.7279, -1.6816,\n","         -0.7852, -0.6119, -1.0547]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0020608233753591776\n","During training one batch, the logits of the output is: tensor([[-0.9577, -1.0399, -0.1814, -0.4868,  7.6668, -0.9568, -0.4544, -0.5507,\n","         -0.9452, -0.5782, -0.4879],\n","        [ 7.8078, -1.4278, -0.4768, -0.5335, -0.4853, -1.1599, -0.7804, -0.6650,\n","         -0.7627, -0.9025, -1.1170],\n","        [ 7.8190, -1.3806, -0.5271, -0.5226, -0.5891, -1.1401, -0.6967, -0.7019,\n","         -0.5510, -0.8894, -1.2674]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.002267397940158844\n","During training one batch, the logits of the output is: tensor([[ 7.8563, -1.4274, -0.4221, -0.4689, -0.3572, -1.2626, -0.8543, -0.5235,\n","         -0.7399, -0.9005, -1.3082],\n","        [-0.9604, -1.1606, -0.1787, -0.4551,  7.6685, -1.0610, -0.4153, -0.4206,\n","         -0.9034, -0.5571, -0.5474],\n","        [-0.9837, -1.1439, -0.2277, -0.4931,  7.6752, -1.0420, -0.4149, -0.3836,\n","         -0.8721, -0.5922, -0.5424]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0023, device='cuda:0')\n","Loss.item: 0.0023455906193703413\n","During training one batch, the logits of the output is: tensor([[-1.1210, -0.9140, -0.5241, -0.5428, -0.7551, -0.7831, -0.3595, -0.7082,\n","         -0.3208, -1.3540,  7.6655],\n","        [-1.1334, -0.9249, -0.4925, -0.5416, -0.7482, -0.7767, -0.3630, -0.7241,\n","         -0.3257, -1.3512,  7.6578],\n","        [-1.1017, -0.9303, -0.6150, -0.5150, -0.7108, -0.8551, -0.3509, -0.5527,\n","         -0.3774, -1.4156,  7.6664]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0015, device='cuda:0')\n","Loss.item: 0.0014704493805766106\n","During training one batch, the logits of the output is: tensor([[-0.7448, -0.7012, -0.2837, -0.7228, -0.6869, -0.6618, -0.0508, -0.7939,\n","         -0.7722,  8.3034, -1.1730],\n","        [-1.0299, -0.9591, -0.6055, -0.5150, -0.9028, -0.6830, -0.3870, -0.5533,\n","         -0.5382, -1.2845,  7.7131],\n","        [-1.3168, -0.8836, -0.8347, -1.2569, -1.1933,  8.2448, -0.9654, -1.3647,\n","         -0.8595, -1.3566, -1.0728]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0016019181348383427\n","During training one batch, the logits of the output is: tensor([[-0.6789, -0.7368, -0.4196, -0.8048, -0.6534, -0.8586, -0.1651, -0.4958,\n","         -0.6021,  8.2968, -1.1871],\n","        [-1.0649, -0.9424, -0.6222, -0.5456, -0.8561, -0.6896, -0.3838, -0.5075,\n","         -0.5754, -1.2905,  7.7167],\n","        [-0.5229, -0.4260, -1.5691, -1.1060, -0.8967, -1.1548, -1.1727,  8.0374,\n","         -0.7706, -1.0360, -0.9780]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.002112272661179304\n","During training one batch, the logits of the output is: tensor([[-1.4426, -0.8410, -0.9095, -1.2637, -1.2034,  8.2414, -1.0650, -1.2156,\n","         -0.8618, -1.3640, -1.0204],\n","        [-1.0899,  7.2749, -1.0355, -0.7213, -1.6131, -1.0975, -0.6451,  1.2040,\n","         -0.9399, -1.1936, -1.0989],\n","        [-1.3855, -0.9610, -0.8690, -1.1406, -1.1548,  8.2296, -0.9850, -1.2866,\n","         -0.8691, -1.4512, -0.9450]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.5016, device='cuda:0')\n","Loss.item: 0.501589834690094\n","During training one batch, the logits of the output is: tensor([[-1.4440, -0.9344, -0.9545, -1.1032, -1.1301,  8.2388, -0.9645, -1.2665,\n","         -1.0430, -1.3743, -0.8654],\n","        [-1.9214,  4.6009, -1.9597, -0.7956, -0.8749, -0.8472, -0.9322,  5.1617,\n","         -1.0495, -1.7734, -0.9947],\n","        [-1.7777,  5.1875, -1.8539, -0.8015, -1.1010, -0.9020, -0.9330,  4.6807,\n","         -1.1225, -1.7188, -1.0284]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0040, device='cuda:0')\n","Loss.item: 0.004042822867631912\n","During training one batch, the logits of the output is: tensor([[-0.3564, -0.7710, -0.8348, -0.8362, -1.3507, -0.1728, -0.0132, -0.7193,\n","          6.6701, -0.9511, -0.3730],\n","        [-0.9603, -1.0708, -0.2243, -0.4699,  7.6793, -1.0169, -0.4280, -0.5022,\n","         -0.8835, -0.5781, -0.4759],\n","        [-0.9557, -1.0707, -0.2355, -0.5143,  7.6769, -1.0268, -0.4318, -0.4869,\n","         -0.8912, -0.5478, -0.4767]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0026, device='cuda:0')\n","Loss.item: 0.0025753534864634275\n","During training one batch, the logits of the output is: tensor([[-0.7709, -0.7575, -0.3146, -0.8268, -0.6475, -0.6582, -0.1087, -0.4594,\n","         -0.7651,  8.3027, -1.2496],\n","        [-0.4788, -0.2388, -0.6418,  7.6298, -0.3218, -0.8944,  0.0380, -0.5184,\n","         -0.7611, -0.5211, -0.1728],\n","        [-0.4917, -0.2255, -0.6273,  7.6280, -0.3422, -0.8561, -0.0369, -0.5719,\n","         -0.7313, -0.4984, -0.1621]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0011, device='cuda:0')\n","Loss.item: 0.0010526359546929598\n","During training one batch, the logits of the output is: tensor([[-0.6480, -0.7972, -0.2949, -0.6718, -0.5731, -0.8267, -0.0827, -0.6189,\n","         -0.8346,  8.2953, -1.0999],\n","        [-1.2114, -1.0348, -0.8176, -1.2590, -1.1695,  8.2124, -1.0675, -1.3657,\n","         -0.7532, -1.4296, -1.0149],\n","        [-1.4075, -0.8993, -0.8965, -1.2428, -1.1661,  8.2305, -1.0876, -1.2050,\n","         -0.8859, -1.3931, -1.0015]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0022, device='cuda:0')\n","Loss.item: 0.002156500006094575\n","During training one batch, the logits of the output is: tensor([[-1.3203, -0.9131, -0.9566, -1.2800, -1.1484,  8.2276, -1.1049, -1.1990,\n","         -0.8790, -1.4528, -0.9903],\n","        [-0.8484, -1.0643, -0.5769, -0.4473, -0.7218, -0.5785, -0.4863, -0.7044,\n","         -0.5587, -1.3296,  7.6201],\n","        [-1.1504,  7.3548, -1.0574, -0.6781, -1.6035, -0.6096, -0.2949,  0.0854,\n","         -0.6240, -0.9469, -1.3199]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0021292506717145443\n","During training one batch, the logits of the output is: tensor([[-0.5837, -0.3763, -1.4991, -1.1392, -0.8128, -1.1751, -1.1858,  8.0417,\n","         -0.9193, -1.0073, -0.9180],\n","        [-0.9042,  0.0730, -1.5525, -1.1051, -0.9365, -1.0300, -1.2200,  7.9971,\n","         -0.8288, -1.1552, -1.0681],\n","        [-1.2127,  7.3640, -0.9388, -0.6272, -1.5305, -0.8549, -0.8700,  0.9094,\n","         -1.0119, -1.1091, -0.9447]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0012319082161411643\n","During training one batch, the logits of the output is: tensor([[-1.4431, -1.0184, -0.9239, -1.1829, -1.1321,  8.2489, -1.0704, -1.2627,\n","         -0.9567, -1.2707, -0.8482],\n","        [-0.6523, -0.4472, -1.5870, -1.1638, -0.7350, -1.0857, -1.3131,  8.0392,\n","         -0.7649, -0.7897, -1.0449],\n","        [-0.5749, -0.3991, -1.7039, -1.1931, -0.6570, -1.1934, -1.3944,  7.9107,\n","         -0.7698, -0.6584, -0.9610]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.3983, device='cuda:0')\n","Loss.item: 0.3982534408569336\n","During training one batch, the logits of the output is: tensor([[-0.4752, -0.3723, -1.6944, -1.2141, -0.4941, -1.2041, -1.5388,  7.8409,\n","         -0.8008, -0.5407, -1.0875],\n","        [-1.3468,  0.1627,  2.3204, -1.4589,  2.7558,  0.9908, -1.5643, -0.3093,\n","         -1.3993, -1.8388,  0.6106],\n","        [-0.6144, -0.6155,  7.3984, -0.8287, -0.0639, -0.7300, -0.8643, -1.3464,\n","         -1.1208, -0.8177, -0.8722]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0016, device='cuda:0')\n","Loss.item: 0.0016231253976002336\n","During training one batch, the logits of the output is: tensor([[ 7.8306, -1.4319, -0.4376, -0.3846, -0.4549, -1.3426, -0.9352, -0.2454,\n","         -0.6611, -1.1303, -1.3182],\n","        [-0.5473, -0.8161, -0.4154, -0.8533, -0.6784, -0.9466, -0.1988, -0.2577,\n","         -0.5057,  8.2847, -1.3769],\n","        [ 0.0635, -0.9369, -0.2990, -0.8938, -0.7439, -0.8540, -0.2211, -0.8805,\n","         -0.3126,  8.1613, -1.6364]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0012, device='cuda:0')\n","Loss.item: 0.0011998176341876388\n","During training one batch, the logits of the output is: tensor([[-0.6744, -0.7058, -0.3037, -0.7207, -0.6726, -0.7571, -0.1273, -0.7387,\n","         -0.6980,  8.2969, -1.1725],\n","        [-1.2726, -0.6974, -1.0859, -1.2496, -1.4052,  8.2004, -0.9620, -1.3155,\n","         -0.7282, -1.2757, -1.1887],\n","        [-0.6323, -0.7171, -0.3626, -0.7156, -0.6777, -0.7777, -0.1154, -0.7871,\n","         -0.6350,  8.2947, -1.1689]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0009, device='cuda:0')\n","Loss.item: 0.0008991885115392506\n","During training one batch, the logits of the output is: tensor([[-1.4118, -1.0489, -0.9234, -1.1831, -1.1189,  8.2280, -1.0580, -1.2673,\n","         -0.9303, -1.2297, -0.9547],\n","        [-1.4033, -1.0664, -0.9143, -1.2037, -1.1021,  8.2261, -1.0715, -1.2556,\n","         -0.9628, -1.2222, -0.9133],\n","        [-1.3215, -0.9974, -0.9104, -1.3019, -1.1720,  8.2138, -1.0118, -1.3743,\n","         -0.6372, -1.2793, -1.0360]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0009, device='cuda:0')\n","Loss.item: 0.0008923203567974269\n","During training one batch, the logits of the output is: tensor([[-1.3549, -0.9840, -0.9274, -1.3027, -1.1616,  8.2214, -1.0152, -1.3595,\n","         -0.6763, -1.2507, -1.0020],\n","        [-1.3317, -0.8865, -0.8883, -1.2427, -1.2440,  8.2280, -1.0527, -1.2531,\n","         -0.7583, -1.4222, -1.1173],\n","        [-1.4214, -0.8754, -0.7646, -1.2790, -1.1556,  8.2487, -0.9845, -1.2790,\n","         -1.0278, -1.3127, -1.0511]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0013, device='cuda:0')\n","Loss.item: 0.001329814433120191\n","During training one batch, the logits of the output is: tensor([[-1.4054, -0.9008, -0.8998, -1.2366, -1.1907,  8.2177, -1.0746, -1.2022,\n","         -0.8706, -1.4344, -0.9991],\n","        [-1.4451, -0.9035, -0.8795, -1.2384, -1.1619,  8.2297, -1.0939, -1.1944,\n","         -0.9051, -1.3987, -0.9807],\n","        [-1.0875, -0.8576, -0.5282, -0.6531, -0.9249, -0.6545, -0.4701, -0.4100,\n","         -0.7079, -1.2337,  7.6953]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0021, device='cuda:0')\n","Loss.item: 0.0021472268272191286\n","During training one batch, the logits of the output is: tensor([[-1.0760, -0.8875, -0.5244, -0.6074, -0.8357, -0.7927, -0.3878, -0.4028,\n","         -0.6817, -1.2949,  7.7107],\n","        [-1.3947, -0.6356, -0.8556, -1.3357, -1.2166,  8.1705, -1.0415, -1.2702,\n","         -0.8498, -1.2515, -1.1360],\n","        [-1.1752,  7.3433, -0.9539, -0.5231, -1.7724, -0.5721, -0.1958,  0.0863,\n","         -0.6899, -0.9335, -1.3963]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0018, device='cuda:0')\n","Loss.item: 0.001770616858266294\n","During training one batch, the logits of the output is: tensor([[-0.8582, -0.5055, -1.8453, -1.3467, -0.6789, -1.1185, -1.2336,  7.8964,\n","         -0.6750, -0.7859, -0.3553],\n","        [-0.6063, -0.7658, -0.3810, -0.7670, -0.6291, -0.9051, -0.0873, -0.8508,\n","         -0.4185,  8.2923, -1.2104],\n","        [-0.7128, -0.9168,  7.5790, -1.0450, -0.2944, -0.2653, -0.8387, -1.5565,\n","         -0.9682, -0.6211, -1.0175]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(1.4043, device='cuda:0')\n","Loss.item: 1.4042762517929077\n","During training one batch, the logits of the output is: tensor([[-0.7796,  1.6592, -1.3990, -1.7237, -0.3153, -0.2131, -1.2898, -0.3164,\n","          2.2883, -0.2340,  2.4448],\n","        [-0.4863, -0.8296, -0.2786, -0.6542, -0.6324, -0.8812, -0.1226, -0.8769,\n","         -0.6470,  8.2440, -1.0894],\n","        [-0.9476, -0.7928,  7.5720, -0.9552, -0.2562, -0.2649, -0.8760, -1.5903,\n","         -0.9054, -0.7718, -0.8604]], device='cuda:0')\n","During training one batch, the loss of the output is: tensor(0.0019, device='cuda:0')\n","Loss.item: 0.0018909150967374444\n","During training one batch, the logits of the output is: tensor([[-0.9581, -1.1632, -0.2216, -0.5336,  7.6793, -1.0312, -0.4510, -0.3398,\n","         -0.9135, -0.5427, -0.5346],\n","        [-0.6451, -0.7230, -0.3727, -0.8440, -0.7624, -0.7813, -0.1022, -0.9231,\n","         -0.3180,  8.2650, -1.2381],\n","        [ 7.8470, -1.4402, -0.5375, -0.4438, -0.4594, -1.2540, -0.8662, -0.3564,\n","         -0.6829, -1.0174, -1.2478]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mzBzJT7_5ifz"},"source":["Our fune-tuned model has 0.9716165220328032 f1 score on the testset"]},{"cell_type":"code","metadata":{"id":"H9TZzkaGTCFC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626813951815,"user_tz":240,"elapsed":152,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"1e132de8-66d8-4c91-9eea-f3f9ad56d80f"},"source":["val_f1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9716165220328032"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"lyMG8aqSTCOV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ElLM-vLTBR0"},"source":["Test the model with some random texts on the internet"]},{"cell_type":"markdown","metadata":{"id":"CebXAC2k6MbX"},"source":["Test moderna description from their website"]},{"cell_type":"code","metadata":{"id":"C0p9cQ4Y44Q2"},"source":["moderna = \"\"\"The Ann Arbor Area Transportation Authority (TheRide) operates the public transit system for the greater Ann Arbor-Ypsilanti area. TheRide provides reliable, safe, affordable transportation services. We offer a variety of service types including: Fixed route buses Paratransit services (A-Ride) Commuter services (ExpressRide, Park & Ride, Vanpools) Demand-responsive services (FlexRide, NightRide, HolidayRide) Event services (FootballRide)  Airport services (AirRide)\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FstHQ9uN02i7","executionInfo":{"status":"ok","timestamp":1626720321706,"user_tz":240,"elapsed":136,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"c11c70ae-ebc6-4068-af15-89c75be0d401"},"source":["moderna_test = [moderna]\n","moderna_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['The Ann Arbor Area Transportation Authority (TheRide) operates the public transit system for the greater Ann Arbor-Ypsilanti area. TheRide provides reliable, safe, affordable transportation services. We offer a variety of service types including: Fixed route buses Paratransit services (A-Ride) Commuter services (ExpressRide, Park & Ride, Vanpools) Demand-responsive services (FlexRide, NightRide, HolidayRide) Event services (FootballRide)  Airport services (AirRide)']"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"4NfW0Htv6k25"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n","                                          do_lower_case=True)\n","                                          \n","encoded_data_test = tokenizer.batch_encode_plus(\n","    moderna_test, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    padding=True, \n","    max_length=384, \n","    return_tensors='pt'\n",")\n","\n","input_ids_moderna = encoded_data_test['input_ids']\n","attention_masks_moderna = encoded_data_test['attention_mask']\n","import numpy as np\n","labels_test = torch.tensor(np.array([4]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MurM6mFm6l4M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626720331292,"user_tz":240,"elapsed":104,"user":{"displayName":"Ssuying Chen","photoUrl":"","userId":"06006498631132285266"}},"outputId":"690b7d71-8c3e-43df-dc40-71605a23ca31"},"source":["output_moderna = model(input_ids_moderna.to(device))\n","_, predicted = torch.max(output_moderna[0], 1)\n","predicted"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([5], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"b8MZrsVJR0-x"},"source":[""],"execution_count":null,"outputs":[]}]}