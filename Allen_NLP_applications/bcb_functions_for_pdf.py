# -*- coding: utf-8 -*-
"""bcb_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/103e-2ZDApA7dJ1mapWrg611zjIr_JiUz

# Useful functions for current/future project members

Import these functions to save time and avoid re-writing code!
"""

"""## Data collection functions

### PDF text scraper
"""

from bs4 import BeautifulSoup
from datetime import datetime
import filetype
import io
import multiprocessing as mp
import numpy as np
import os
import pandas as pd
from pdf2image import convert_from_bytes, convert_from_path
import PIL
import PyPDF4
import pytesseract
import re
import requests
import subprocess
from tqdm import notebook #tqdm
from tqdm import tqdm_notebook
import warnings

warnings.filterwarnings("ignore")

# NLP libraries
from striprtf.striprtf import rtf_to_text
from allennlp.predictors.predictor import Predictor
import nltk
nltk.download('punkt')

class PDF:
  '''
  Description:
    A class representing one instance of a PDF, and the functions needed to extract raw text from it
    without downloading (if on web)

  Attributes:
    pdf_location (str): the URL or file path where the PDF can be found.
    pdf_source (str or None): 'web', 'downloaded', or None (default None - determines source automatically) 
    pdf_content (bytes): byte-level representation of PDF if pdf_location is URL (default None - generated automatically)
    pdf4_output (None): PyPDF4 representation of PDF, used solely to count pages (default None - generated automatically)
    numPages (int or None): number of pages in PDF (default None - generated automatically)
    cpu_count (int or None): number of CPUs available (default None - generated automatically)
    batch_size (int): number of pages included in each batch if PDF has more pages than max_pages. Default 100.
    max_pages (int): max_pages: the maximum number of pages to accept. Anything over this number is flagged as impossible for current available hardware. Default 500.

  Methods:
    pdf_images(first_page = None, last_page = None):
      Converts pages from PDF into BMP images to be read by ocr().
    pdf_ocr_single_image(pdf_image):
      Uses optical character recognize to extract raw text from a single image of a page.
    pdf_ocr_multiprocessor(pdf_images):
      Takes output of pdf_images(), parallelizes pdf_ocr_single_image() using the available number of cores, and returns full raw text of PDF images.
    ocr():
      Calls all functions above to convert PDF to correct format, parallelize OCR, and return full raw text.
    ocr_large_batch():
      Called if numPages > batch_size; reads pages in batches to conserve memory.
  '''

  def __init__(self, pdf_location, pdf_source = None, pdf_content = None, pdf4_output = None, numPages = None,
               cpu_count = None, batch_size = 100, max_pages = 500):
    '''
    Description:
      Initializes PDF object.

    Parameters:
      pdf_images(first_page = None, last_page = None):
        Converts pages from PDF into BMP images to be read by PyTesseract.
      pdf_ocr_single_image(pdf_image):
        Uses optical character recognize to extract raw text from a single image of a page.
      pdf_ocr_multiprocessor(pdf_images):
        Takes output of pdf_images(), parallelizes pdf_ocr_single_image() using the available number of cores, and returns full raw text of PDF images.
      ocr():
        Calls all functions above to convert PDF to correct format, parallelize OCR, and return full raw text.
      ocr_large_batch():
        Called if numPages > batch_size; reads pages in batches to conserve memory.

    Returns:
      None
    '''
    self.pdf_location = pdf_location
    self.pdf_source = pdf_source
    self.pdf_content = pdf_content
    self.pdf4_output = pdf4_output
    self.numPages = numPages
    self.cpu_count = cpu_count
    self.batch_size = batch_size
    self.max_pages = max_pages

    if self.pdf_location.startswith('http://') or self.pdf_location.startswith('https://'):
      self.pdf_source = 'web'
      self.pdf_content = requests.get(pdf_location).content
      self.pdf4_output = PyPDF4.PdfFileReader(io.BytesIO(self.pdf_content))
    else:
      self.pdf_source = 'downloaded'
      self.pdf_content = self.pdf_location
      self.pdf4_output = PyPDF4.PdfFileReader(self.pdf_content)

    self.numPages = self.pdf4_output.numPages

    self.cpu_count = subprocess.check_output("cat /proc/cpuinfo | grep -ci 'cpu cores'", shell = True).decode('utf-8')
    #  for later multiprocessing task
    self.cpu_count = int(self.cpu_count[0])
    
  def pdf_images(self, first_page = None, last_page = None):
    '''
    Description:
      Converts pages from PDF into BMP images to be read by PyTesseract.
      If pdf_source == 'web', converts from byte-level representation of PDF.
      If pdf_source == 'downloaded', converts from downloaded PDF.

    Parameters:
      first_page (int or None): the first page number to convert to an image in the PDF. Default None - detected automatically.
      last_page (int or None): the last page number to convert to an image in the PDF. Default None - detected automatically.

    Returns:
      list: BMP images in specified page range

    '''
    if self.pdf_source == 'web':
      return convert_from_bytes(self.pdf_content, first_page = first_page, last_page = last_page,)
    elif self.pdf_source == 'downloaded':
      return convert_from_path(self.pdf_content, first_page = first_page, last_page = last_page)


  def pdf_ocr_single_image(self, pdf_image):
    '''
    Description:
        Uses PyTesseract OCR to read single page of PDF.
        
    Parameters:
        pdf_image: a PIL representation (BMP image) of a PDF page.
        
    Returns:
        string: text of page
    '''
    # extra space is useful later when joining text from multiple pages into one doc
    return f' {pytesseract.image_to_string(pdf_image)}'

  def pdf_ocr_multiprocessor(self, pdf_images):
    '''
    Description:
        Takes output of pdf_iamges() and extracts text using pdf_ocr_single_image() (parallelized).
        
    Parameters:
        pdf_images: a list of BMP images from pdf_images()
    
    Returns:
        string: text of the original PDF (without extra whitespace).
    '''
    raw_string = ''

    print(f'Reading from: {self.pdf_location}...')

    if self.cpu_count > 1:

      os.environ['OMP_THREAD_LIMIT'] = '1' # for allocating each process to a single CPU
      
      # referenced https://stackoverflow.com/questions/41920124/multiprocessing-use-tqdm-to-display-a-progress-bar for mp progress bar 

      pooler = mp.Pool(processes = self.cpu_count)
      results = list(notebook.tqdm(pooler.imap(self.pdf_ocr_single_image, pdf_images), total = len(pdf_images)))
      for r in results:
          raw_string += f' {r}'
      pooler.close()
      pooler.join()
      
      os.environ['OMP_THREAD_LIMIT'] = str(self.cpu_count)

    else:
      for image in tqdm_notebook(pdf_images, total = len(pdf_images)):
        raw_string += f' {self.pdf_ocr_single_image(image)}'

    output = ' '.join([word for word in raw_string.split() if len(word) < 30]) # removes extra spaces and extra-long lines that pytesseract read as words...inexplicably

    return output


  def ocr(self):
    '''
    Description:
        Takes PDF from pdf_location and extracts text using pdf_ocr_multiprocessor().
        If numPages > batch_size, calls ocr_large_batch() to read in batches.
        If numPages > max_pages, does not read PDF.
        
    Parameters:
        None.
        
    Returns:
        string: all text from PDF (if numPages < batch_size) or note to scrape separately (if numPages > batch_size)
    '''

    # removing maximum number of pixels allowed for PIL so function doesn't crash with larger files
    PIL.Image.MAX_IMAGE_PIXELS = None

    if self.numPages > self.max_pages:
      output = f'###SCRAPE SEPARATELY: this document is {self.numPages} pages and may crash this program.'
      print(output)
      return output

    if self.numPages > self.batch_size:
        return self.ocr_large_batch()
    
    pdf_images = self.pdf_images()

    output = self.pdf_ocr_multiprocessor(pdf_images)

    return output


  def ocr_large_batch(self):
    '''
    Description:
        Takes PDF from pdf_location, creates smaller batches, and extracts text using OCR (parallelized).
        
    Parameters:
        None.
        
    Returns:
        string: all text from PDF.
    '''

    hundreds = int(self.numPages/self.batch_size) # rounds down
    remainder = self.numPages%self.batch_size
    
    print(f'Page count: {self.numPages}. Reading in batches of {self.batch_size} to save memory.')
    
    batch_floor = 0
    
    output_strs = []
    
    for i in range(batch_floor, hundreds + 1):
        
      batch_ceil = (self.batch_size * (i + 1)) - 1
                      
      if i == hundreds:
          batch_ceil = batch_floor + remainder
            
      print(f'\tScraping pages {batch_floor} to {batch_ceil}...')
        
      pdf_images = self.pdf_images(first_page = batch_floor, last_page = batch_ceil)

      batch_floor += self.batch_size
        
      batch_output = self.pdf_ocr_multiprocessor(pdf_images)

      output_strs.append(batch_output)

    output = ' '.join(output_strs)

    return output


def read_from_folder(folder_path, write_to_files = True, output_folder = None, batch_size = 100, max_pages = 500):
  '''
  Description:
    Extracts raw text from each file in a folder of PDFs.
    If write_to_files == True, saves extracted text as plain text file in output_folder.

  Parameters:
    folder_path (str): path to folder that holds PDFs (only)
    write_to_files (bool): If True, writes to saves extracted text as plain text file in output_folder. If False, does not write to output_folder. Default True.
    output_folder (str or None): path to folder 
  
  Returns:
    list: string representations of text from each PDF in folder
  '''
  # referenced https://askubuntu.com/questions/811210/how-can-i-make-ls-only-display-files for "find" syntax

  folder_files = [file for file in subprocess.check_output(
      f"find ./{folder_path} -maxdepth 1 -not -type d", shell = True
      ).decode('utf-8'
      ).replace('//', '/'
      ).split('\n'
      ) if len(file) > 0
    ]

  if write_to_files == True:
    if output_folder is None:
      output_folder = f'ocr_output' # .replace('//', '/')
    try:
      subprocess.run(f'mkdir {output_folder}', shell = True)
    except FileExistsError:
      subprocess.run(f'rmdir -r {output_folder}', shell = True)
      subprocess.run(f'mkdir {output_folder}', shell = True)
    print(f'Saving results to {output_folder}...')

    try:
      current_txt_files = [file for file in subprocess.check_output(
        f'ls -p {output_folder}/*', shell = True
        ).decode('utf-8'
        ).replace('//', '/'
        ).split('\n'
        ) if len(file) > 0
        ]
    except:
      current_txt_files = []

  file_contents = []
  for file in folder_files:
    if write_to_files == True:
      txt_file_path = f"{output_folder}/{file.split('/')[-1].split('.pdf')[0]}.txt".replace('//', '/')
      if txt_file_path in current_txt_files:
        source = open(txt_file_path, 'r')
        print(f'Loading {txt_file_path} from memory...')
        file_content = source.read()
      else:
        txt_file_path = f"{output_folder}/{file.split('/')[-1].split('.pdf')[0]}.txt"
        file_content = PDF(file, batch_size = batch_size, max_pages = max_pages).ocr()
        target = open(txt_file_path, 'w')
        target.write(file_content)
    else:
      file_content = PDF(file, batch_size = batch_size, max_pages = max_pages).ocr()
    
    file_contents.append(file_content)

  return file_contents

# # read_from_folder('data_collection/test_data/')

# test_url_list = ['https://whc.unesco.org/document/155117', 'https://whc.unesco.org/document/127112']

def read_from_urls(url_list, write_to_files = True, output_folder = None, batch_size = 100, max_pages = 500):

    '''
    Description:
      Extracts raw text from each PDF in list of URLs to PDFs.
      If write_to_files == True, saves extracted text as plain text file in output_folder.

    Parameters:
      url_list (str): list of URLs for PDFs (only)
      write_to_files (bool): If True, writes to saves extracted text as plain text file in output_folder. If False, does not write to output_folder. Default True.
      output_folder (str or None): path to folder 
    
    Returns:
      list: string representations of text from each PDF in url_list
    '''

    file_contents = []
    
    if write_to_files == True:
        if output_folder is None:
            output_folder = f'ocr_output'
        try:
            subprocess.run(f'mkdir {output_folder}', shell = True)
        except FileExistsError:
            subprocess.run(f'rmdir -r {output_folder}', shell = True)
            subprocess.run(f'mkdir {output_folder}', shell = True)
        
        try:
            current_txt_files = [file for file in subprocess.check_output(
            f'ls -p {output_folder}/*', shell = True
            ).decode('utf-8'
            ).replace('//', '/'
            ).split('\n'
            ) if len(file) > 0
          ]
        except: # exception here means the folder is empty.
            current_txt_files = []
        
        print(f'Saving results to {output_folder}...')

    for url in url_list:
        if write_to_files == True:
            new_file_name = url.replace('/', '_')
            new_file_name = new_file_name.replace(':', '-')  # to make URL non-webpage-ish
            txt_file_path = f"{output_folder}/{new_file_name}".replace('//', '/')
            if txt_file_path in current_txt_files:
                source = open(txt_file_path, 'r')
                print(f'Loading {txt_file_path} from memory...')
                file_content = source.read()
            else:
                txt_file_path = f"{output_folder}/{new_file_name}".replace('//', '/') # to make URL non-webpage-ish
                file_content = PDF(url, batch_size = batch_size, max_pages = max_pages).ocr()
                target = open(txt_file_path, 'w')
                target.write(file_content)
        else:
            file_content = PDF(url, batch_size = batch_size, max_pages = max_pages).ocr()

        file_contents.append(file_content)

    return file_contents

# # read_from_urls(test_url_list)

def rtf_to_text_from_folder(folder_path):
    '''
    transfer rtf files into text
    
    parameter: str, path to folders containing rtf files
    
    return: list, rtf file paths and text
    '''
    
    
    for root, dirs, files in os.walk(folder_path): 
        files = [f for f in files if not f[0] == '.'] # ignore hidden files
#         full_text = []
#         full_file_path = []
        output_list = []
        for file in files:
            f = open(os.path.join(root, file), "r")
            # skip files that are not rtf format
            if file[-3:] == "rtf":
                text = rtf_to_text(f.read()).replace("\n", "")
                path = os.path.join(root, file)
                output_list.append((text, path))
#                 full_file_path.append(os.path.join(root, file))  
#                 full_text.append(rtf_to_text(f.read()).replace("\n", ""))
                
            
    return output_list


def extract_sub_folder_path(folder_path):
    '''
    extract the subfolder paths which contains rtf files
    and skip the first layer of the result because the first layer
    leads to all the subfolders
    
    parameter: str, folder path
    
    return: list, subfolder paths
    
    '''
    sub_folder_path = []
    for (dirpath, dirnames, filenames) in os.walk(folder_path):
        if ".ipynb_checkpoints" not in dirpath:
            sub_folder_path.append(dirpath)
            print((dirpath, dirnames, filenames))
    
    # skip the first sub folder path because it returns all sub folders
    # instead of the files in the subfolders
    return sub_folder_path

def extract_file_names(folder_path):
    '''
    extract the file names in the folder path
    
    parameter: str, folder path
    
    return: list, file names
    
    '''
    
    for (dirpath, dirnames, filenames) in os.walk(folder_path):
        if ".ipynb_checkpoints" not in dirpath:
            return filenames


# """## NLP functions"""

# Predictor is used to generate allen nlp tags

class Semantic_Role_Labeling:
    def __init__(self):
        self.predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz")

    def generate_spo_triples(self,text):
        '''
        generate SPO triples
        ===
        parameter:string
        ===
        return: SPO triples
        '''
        preds = self.predictor.predict(text)
        return preds

# https://stackoverflow.com/questions/42012152/unstack-a-pandas-column-containing-lists-into-multiple-rows
    @staticmethod
    def generate_spo_csv(df):
        '''
        If there is one verb in a sentence, the code will grab info out from that verb.
        If the sentence has more than one verb, the code below will also grab those info and stack them for that sentence.
        If there is no verb, the empty list in the df row will be replaced by "No verb","No description", "No tag". 
        
        parameter: pandas dataframe containing spo triples
        ===
        return: new df
        '''
        data = []
        for row in df.itertuples():
            if len(row[2]) >= 1:
                lst = row[2]
                for col2 in lst:
                    data.append([row[0], row[1], col2['verb'], col2['description'], col2['tags']])
            else:
                data.append([row[0], row[1], "No verb", "No description", "No tag"])
        new_df = pd.DataFrame(data =data, columns=['sentence index', 'sentence', 'verb', 'description', 'tags'])
        return new_df

    @staticmethod
    def extract_spo_tags(text, arg_tag_lst):
        '''
        restore the tags of the text in the arg tag list order
        if there is no correspondant tag value, leave it as None

        parameter: a string generated by the spo generator function, a list of tuples
        ===
        return: a list of tags in arg tag list order
        '''
        desc_value = []
        for tag, intro in arg_tag_lst:
            args = re.findall(f"\[{tag}\:.*?\]", text)
            if args:
                args_list = []
                for arg in args:
                    arg = arg.replace("[", "")
                    arg = arg.replace("]", "")
                    args_list.append(arg.split(":")[1].strip())
                desc_value.append(args_list)
            else:
                desc_value.append(None)

        return desc_value

    def export_csv(self,doc_lst, file_name_lst):
        '''
        an aggregated function which takes a bunch of pdf context and export csv files 
        containing spo tags
        
        parameter: a list of context transformed from pdf urls with the read_from_url function above
        
        return: csv files in local directory
        '''
        # all documented arg tags found in https://www.aclweb.org/anthology/J05-1004.pdf
        #Arg2-Arg4 are not consistent, causes a problem for labeling in academy
        self.arg_intro = [
                 ("ARG0", "agent(the one who causes an event or change of state in another participant)"),
                 ("ARG1", "patient(someone or something undergoes changes of state/causally affected by another participant)"),
                 ("ARG2", "usually benefactive, instrument, attribute or end state"),
                 ("ARG3", "start point, benefactive, instrument or attribute"),
                 ("ARG4", "usually end point"),
                 ("ARGM-ADV", "general purpose"),
                 ("ARGM-CAU", "cause"),
                 ("ARGM-DIR", "direction"),
                 ("ARGM-DIS", "discourse connectives"),
                 ("ARGM-EXT", "extent"),
                 ("ARGM-LOC", "location"),
                 ("ARGM-MNR", "manner"),
                 ("ARGM-MOD", "modal verb"),
                 ("ARGM-NEG", "negation marker"),
                 ("ARGM-PRP", "purpose"),
                 ("ARGM-TMP", "time"),
                 ("ARGM-PRD", "secondary predication"),
                 ("C-ARG1", "discontinuous argument of arg1"),
                 ("R-ARG1", "referent or pronouns of arg1"),
                 ("R-ARG2",	"referent or pronouns of arg2"),
                 ("V", "verb/predicate")]

        for doc_num in range(len(doc_lst)):
            doc_sentences = nltk.sent_tokenize(doc_lst[doc_num])
            doc_spo = {sentence: self.generate_spo_triples(sentence)['verbs'] for sentence in doc_sentences}

            # Create dataframe
            doc_df = pd.DataFrame({"sentence":doc_spo.keys(), "verbs":doc_spo.values()})
            doc_df.head()

            # re-organize the dataframe
            new_df = self.generate_spo_csv(doc_df)

            # create a new column names temp to store the tags in the order of arg tag list
            # we will drop the temp column at last
            new_df["temp"] = new_df["description"].apply(self.extract_spo_tags, arg_tag_lst = self.arg_intro)

            #create a second df to expand column temp
            second_df = pd.DataFrame(new_df['temp'].values.tolist(), columns=pd.MultiIndex.from_tuples(self.arg_intro))

            # get rid of the parenthesis around x if x exist and there is only one word in x
            second_df = second_df.applymap(lambda x:x[0] if x != None and len(x)==1 else x)

            # also add a second level of column for new_df so that it can merge with the second_df
            new_df.columns = pd.MultiIndex.from_tuples([(col, "N/A") for col in new_df.columns])

            #merge two dataframes
            final_df = pd.concat([new_df.drop(columns=["temp"]), second_df], axis=1)
            
            # transform it to csv and download it locally in a directory
            # remove ".pdf" in the file name if it is a pdf file
            if  file_name_lst[doc_num][-4:] == ".pdf":
                file_name_lst[doc_num] = file_name_lst[doc_num][:-4]
            
            # remove "https://" in the url name 
            if file_name_lst[doc_num][:8] == 'https://':
                file_name_lst[doc_num] = file_name_lst[doc_num][8:]
                file_name_lst[doc_num] = file_name_lst[doc_num].replace("/", "_")
                
            outname = file_name_lst[doc_num]+"_spo.csv"
            outdir = './spo_csv_outputs'
            
            if not os.path.exists(outdir):
                os.mkdir(outdir)

            fullname = os.path.join(outdir, outname)  
            print(fullname)
            final_df.to_csv(fullname)

class Named_Entity_Recognition:
    def __init__(self):
        self.predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/ner-model-2020.02.10.tar.gz")
        
    def generate_ner(self,text):
        '''
        using Allen NLP NER model to generate tags
        
        parameter: text
        
        return: tags and tokens
        '''
        ner_tags = self.predictor.predict(text)
        return (ner_tags["tags"], ner_tags['words'])
    
    @staticmethod
    def extract_ner_tags(text):
        '''
        extract only the tag part from the generate_ner output
        
        parameter: string, generate_ner output
        
        return: ner tags
        '''
        if text != "N/A":
            return text[0]
    
    @staticmethod
    def extract_ner_tokens(text):
        '''
        extract only the tokens part from the generate_ner output
        
        parameter: string, generate_ner output
        
        return: ner tokens
        '''
        if text != "N/A":
            return text[1]

    @staticmethod
    def indices(mylist, value):
        '''
        search for the indices of value in a list
        
        parameter: list, a list element
        
        return: list of indicecs
        '''
        return [i for i,x in enumerate(mylist) if x==value]

    @staticmethod
    def extract_entities(tokens, tags):
        '''
        extract a tuple of tokens and tags corresponding to the tag dictionary keys

        parameter:
                a list of tokens
                a list of tags

        return:
                a list (dictionary.values)
        '''
        tag_dict = {
                    "PER": [],
                    "NORP": [],
                    "FAC": [],
                    "ORG": [],
                    "GPE": [],
                    "LOC": [],
                    "PRODUCT": [],
                    "EVENT": [],
                    "WORK OF ART": [],
                    "LAW": [], 
                    "LANGUAGE": [],
                    "DATE": [],
                    "TIME": [],
                    "PERCENT": [],
                    "MONEY": [],
                    "QUANTITY": [],
                    "ORDINAL": [],
                    "CARDINAL": [],
                    'MISC': []
                    }
        if tags is not None:
            left = 0
            #BIOUL stands for Beginning, Inside, Outside, Unit and Last
            # https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition
            while left < len(tags):
                if tags[left][0] == "U":
                    tag_dict[tags[left][2:]].append(tokens[left])
                if tags[left][0] == "B":
                    right = left +1
                    while right < len(tags):
                        if tags[right][0] == "L":
                            tag_dict[tags[left][2:]].append(tokens[left:right+1])
                            break
                        right += 1
                    left = right
                left += 1            

            return tag_dict.values()
        else:
            return []

    def export_csv_for_url(self, doc_lst):
        '''
        an aggregated function which takes a bunch of pdf context and export csv files 
        containing ner tags to the specified folder
        
        parameter: a list of context transformed from pdf urls with the read_from_url function above
        
        return: csv files in local directory
        '''
        tag_intro = [
            ("PER", "People, including fictional"),
            ("NORP", "Nationalities or religious or political groups"),
            ("FAC", "Buildings, airports, highways, bridges"),
            ("ORG", "Companies, agencies, institutions"),
            ("GPE", "Countries, cities, states"),
            ("LOC", "Non-GPE locations, mountain ranges, bodies of water"),
            ("PRODUCT", "Vehicles, weapons, foods, etc. (Not services)"),
            ("EVENT", "Named hurricanes, battles, wars, sports events"),
            ("WORK OF ART", "Titles of books, songs"),
            ("LAW", "Named documents made into laws"), 
            ("LANGUAGE", "Any named language"),
            ("DATE", "Absolute or relative dates or periods"),
            ("TIME", "Times smaller than a day"),
            ("PERCENT", "Percentage (including %)"),
            ("MONEY", "Monetary values, including unit"),
            ("QUANTITY", "Measurements, as of weight or distance"),
            ("ORDINAL", "first, second"),
            ("CARDINAL", "Numerals that do not fall under another type"),
            ("MISC", "Miscellaneous")]
            
        tag_dict = {
            "PER": [],
            "NORP": [],
            "FAC": [],
            "ORG": [],
            "GPE": [],
            "LOC": [],
            "PRODUCT": [],
            "EVENT": [],
            "WORK OF ART": [],
            "LAW": [], 
            "LANGUAGE": [],
            "DATE": [],
            "TIME": [],
            "PERCENT": [],
            "MONEY": [],
            "QUANTITY": [],
            "ORDINAL": [],
            "CARDINAL": [],
            'MISC': []}
        
        for doc_num in range(len(doc_lst)):
            doc_sentences = nltk.sent_tokenize(doc_lst[doc_num])
            predicted_ner = {}
            for sentence in doc_sentences:
                try:
                    predicted_ner[sentence] = self.generate_ner(sentence)
                except:
                    predicted_ner[sentence] = "N/A"
            
            # Create dataframe
            first_df = pd.DataFrame({"sentence":predicted_ner.keys(), "ner":predicted_ner.values()})
            first_df.head()

            # re-organize the dataframe
            first_df['tags'] = first_df['ner'].apply(lambda x:self.extract_ner_tags(x))
            first_df['tokens'] = first_df['ner'].apply(lambda x:self.extract_ner_tokens(x))
            first_df = first_df.drop(columns=["ner"])
            
            second_df = pd.DataFrame(columns = tag_dict.keys())

            # create a new column named temp to store the tags in the order of tag_dict 
            first_df["temp"] = first_df.apply(lambda x:self.extract_entities(x.tokens, x.tags), axis=1)

            # #create a second df to expand column:temp
            second_df = pd.DataFrame(first_df['temp'].values.tolist(), columns=pd.MultiIndex.from_tuples(tag_intro))

            # # get rid of the parenthesis around x if x exist and there is only one word in x
            second_df = second_df.applymap(lambda x:x[0] if x != None and len(x)==1 else x)
            second_df = second_df.applymap(lambda x:None if x != None and len(x)==0 else x)

            # # also add a second level of column for first_df so that it can merge with the second_df
            first_df.columns = pd.MultiIndex.from_tuples([(col, "N/A") for col in first_df.columns])

            # #merge two df
            final_df = pd.concat([first_df.drop(columns=["temp"]), second_df], axis=1)

            # transform it to csv and download it locally in a directory
            outname = f"PDF_to_NER_{doc_num+1}"
            outdir = './ner_csv_outputs'
            
            if not os.path.exists(outdir):
                os.mkdir(outdir)

            fullname = os.path.join(outdir, outname)  
            final_df.to_csv(fullname)
    
    def export_csv_for_folder(self, doc_lst, file_path):
        '''
        an aggregated function which takes a bunch of pdf context and export csv files 
        containing ner tags to the specified file path
        
        parameter: a list of context transformed from pdf urls with the rtf_to_text_from_folder function above
                   a path to a folder containing rtf files
        
        return: csv files in local directory
        '''
        tag_intro = [
            ("PER", "People, including fictional"),
            ("NORP", "Nationalities or religious or political groups"),
            ("FAC", "Buildings, airports, highways, bridges"),
            ("ORG", "Companies, agencies, institutions"),
            ("GPE", "Countries, cities, states"),
            ("LOC", "Non-GPE locations, mountain ranges, bodies of water"),
            ("PRODUCT", "Vehicles, weapons, foods, etc. (Not services)"),
            ("EVENT", "Named hurricanes, battles, wars, sports events"),
            ("WORK OF ART", "Titles of books, songs"),
            ("LAW", "Named documents made into laws"), 
            ("LANGUAGE", "Any named language"),
            ("DATE", "Absolute or relative dates or periods"),
            ("TIME", "Times smaller than a day"),
            ("PERCENT", "Percentage (including %)"),
            ("MONEY", "Monetary values, including unit"),
            ("QUANTITY", "Measurements, as of weight or distance"),
            ("ORDINAL", "first, second"),
            ("CARDINAL", "Numerals that do not fall under another type"),
            ("MISC", "Miscellaneous")]
            
        tag_dict = {
            "PER": [],
            "NORP": [],
            "FAC": [],
            "ORG": [],
            "GPE": [],
            "LOC": [],
            "PRODUCT": [],
            "EVENT": [],
            "WORK OF ART": [],
            "LAW": [], 
            "LANGUAGE": [],
            "DATE": [],
            "TIME": [],
            "PERCENT": [],
            "MONEY": [],
            "QUANTITY": [],
            "ORDINAL": [],
            "CARDINAL": [],
            'MISC': []}
        
        for doc_num in range(len(doc_lst)):
            doc_sentences = nltk.sent_tokenize(doc_lst[doc_num])
            predicted_ner = {}
            for sentence in doc_sentences:
                try:
                    predicted_ner[sentence] = self.generate_ner(sentence)
                except:
                    predicted_ner[sentence] = "N/A"
            
            # Create dataframe
            first_df = pd.DataFrame({"sentence":predicted_ner.keys(), "ner":predicted_ner.values()})
            first_df.head()

            # re-organize the dataframe
            first_df['tags'] = first_df['ner'].apply(lambda x:self.extract_ner_tags(x))
            first_df['tokens'] = first_df['ner'].apply(lambda x:self.extract_ner_tokens(x))
            first_df = first_df.drop(columns=["ner"])
            
            second_df = pd.DataFrame(columns = tag_dict.keys())

            # create a new column named temp to store the tags in the order of tag_dict 
            first_df["temp"] = first_df.apply(lambda x:self.extract_entities(x.tokens, x.tags), axis=1)

            # #create a second df to expand column:temp
            second_df = pd.DataFrame(first_df['temp'].values.tolist(), columns=pd.MultiIndex.from_tuples(tag_intro))

            # # get rid of the parenthesis around x if x exist and there is only one word in x
            second_df = second_df.applymap(lambda x:x[0] if x != None and len(x)==1 else x)
            second_df = second_df.applymap(lambda x:None if x != None and len(x)==0 else x)

            # # also add a second level of column for first_df so that it can merge with the second_df
            first_df.columns = pd.MultiIndex.from_tuples([(col, "N/A") for col in first_df.columns])

            # #merge two df
            final_df = pd.concat([first_df.drop(columns=["temp"]), second_df], axis=1)

            # transform it to csv and download it locally in a directory
            outname = file_path[doc_num][:-4] + "_ner.csv"
            final_df.to_csv(outname)
            



# """## Big aggregated functions"""

# function that takes a single pdf, read the text, tokenizes it, and returns a report of all NLP info on it

# function that takes an iterable of pdfs and does everything for all of them